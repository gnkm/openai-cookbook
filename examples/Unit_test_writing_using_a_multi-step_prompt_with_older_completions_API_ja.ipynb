{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# マルチステッププロンプトを使用した単体テストの作成（旧API使用）\n",
    "\n",
    "単体テストの作成などの複雑なタスクは、マルチステッププロンプトの恩恵を受けることができます。単一のプロンプトとは対照的に、マルチステッププロンプトはGPT-3からテキストを生成し、そのテキストを後続のプロンプトに戻します。これは、GPT-3に答える前に推論を説明させたい場合や、実行前に計画をブレインストーミングさせたい場合に役立ちます。\n",
    "\n",
    "このノートブックでは、以下のステップを使用してPythonで単体テストを作成するために3ステップのプロンプトを使用します：\n",
    "\n",
    "1. Python関数が与えられた場合、まずGPT-3にその関数が何をしているかを説明するよう促します。\n",
    "2. 次に、GPT-3にその関数の単体テストのセットを計画するよう促します。\n",
    "    - 計画が短すぎる場合は、GPT-3に単体テストのより多くのアイデアで詳しく説明するよう求めます。\n",
    "3. 最後に、GPT-3に単体テストを書くよう促します。\n",
    "\n",
    "このコード例は、連鎖したマルチステッププロンプトのいくつかのオプションの装飾を示しています：\n",
    "\n",
    "- 条件分岐（例：最初の計画が短すぎる場合のみ詳細化を求める）\n",
    "- 異なるステップでの異なるモデル（例：テキスト計画ステップには`gpt-3.5-turbo-instruct`、コード作成ステップには`gpt-4`）\n",
    "- 出力が不満足な場合に関数を再実行するチェック（例：出力コードがPythonの`ast`モジュールで解析できない場合）\n",
    "- ストリーミング出力により、完全に生成される前に出力の読み取りを開始できる（長いマルチステップ出力に有用）\n",
    "\n",
    "完全な3ステップのプロンプトは以下のようになります（単体テストフレームワークとして`pytest`、関数として`is_palindrome`を例として使用）：\n",
    "\n",
    "    # pytestで優れた単体テストを書く方法\n",
    "\n",
    "    この専門家向けの上級チュートリアルでは、Python 3.9と`pytest`を使用して、以下の関数の動作を検証する単体テストのスイートを作成します。\n",
    "    ```python\n",
    "    def is_palindrome(s):\n",
    "        return s == s[::-1]\n",
    "    ```\n",
    "\n",
    "    単体テストを書く前に、関数の各要素が正確に何をしているか、そして作者の意図が何であったかを確認しましょう。\n",
    "    - まず、{ステップ1で生成}\n",
    "        \n",
    "    優れた単体テストスイートは以下を目指すべきです：\n",
    "    - 可能な限り幅広い入力に対する関数の動作をテストする\n",
    "    - 作者が予見していない可能性のあるエッジケースをテストする\n",
    "    - `pytest`の機能を活用してテストを書きやすく保守しやすくする\n",
    "    - クリーンなコードと説明的な名前で読みやすく理解しやすくする\n",
    "    - 決定論的であり、テストが常に同じ方法で成功または失敗する\n",
    "\n",
    "    `pytest`には単体テストを書きやすく保守しやすくする多くの便利な機能があります。これらを使用して上記の関数の単体テストを作成します。\n",
    "\n",
    "    この特定の関数については、単体テストで以下の多様なシナリオを処理したいと思います（各シナリオの下に、サブ箇条書きとしていくつかの例を含めます）：\n",
    "    -{ステップ2で生成}\n",
    "\n",
    "    [オプションで追加]上記のシナリオに加えて、稀または予期しないエッジケースのテストを忘れないようにしたいと思います（各エッジケースの下に、サブ箇条書きとしていくつかの例を含めます）：\n",
    "    -{ステップ2Bで生成}\n",
    "\n",
    "    個別のテストに入る前に、まず単体テストの完全なスイートを結束した全体として見てみましょう。各行が何をするかを説明する有用なコメントを追加しました。\n",
    "    ```python\n",
    "    import pytest  # 単体テストに使用\n",
    "\n",
    "    def is_palindrome(s):\n",
    "        return s == s[::-1]\n",
    "\n",
    "    #以下では、各テストケースは@pytest.mark.parametrizeデコレータに渡されるタプルで表現されます\n",
    "    {ステップ3で生成}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # used for detecting whether generated Python code is valid\n",
    "import openai\n",
    "\n",
    "# example of a function that uses a multi-step prompt to write unit tests\n",
    "def unit_test_from_function(\n",
    "    function_to_test: str,  # Python function to test, as a string\n",
    "    unit_test_package: str = \"pytest\",  # unit testing package; use the name as it appears in the import statement\n",
    "    approx_min_cases_to_cover: int = 7,  # minimum number of test case categories to cover (approximate)\n",
    "    print_text: bool = False,  # optionally prints text; helpful for understanding the function & debugging\n",
    "    text_model: str = \"gpt-3.5-turbo-instruct\",  # model used to generate text plans in steps 1, 2, and 2b\n",
    "    code_model: str = \"gpt-3.5-turbo-instruct\",  # if you don't have access to code models, you can use text models here instead\n",
    "    max_tokens: int = 1000,  # can set this high, as generations should be stopped earlier by stop sequences\n",
    "    temperature: float = 0.4,  # temperature = 0 can sometimes get stuck in repetitive loops, so we use 0.4\n",
    "    reruns_if_fail: int = 1,  # if the output code cannot be parsed, this will re-run the function up to N times\n",
    ") -> str:\n",
    "    \"\"\"Outputs a unit test for a given Python function, using a 3-step GPT-3 prompt.\"\"\"\n",
    "\n",
    "    # Step 1: Generate an explanation of the function\n",
    "\n",
    "    # create a markdown-formatted prompt that asks GPT-3 to complete an explanation of the function, formatted as a bullet list\n",
    "    prompt_to_explain_the_function = f\"\"\"# How to write great unit tests with {unit_test_package}\n",
    "\n",
    "In this advanced tutorial for experts, we'll use Python 3.9 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.\n",
    "```python\n",
    "{function_to_test}\n",
    "```\n",
    "\n",
    "Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
    "- First,\"\"\"\n",
    "    if print_text:\n",
    "        text_color_prefix = \"\\033[30m\"  # black; if you read against a dark background \\033[97m is white\n",
    "        print(text_color_prefix + prompt_to_explain_the_function, end=\"\")  # end='' prevents a newline from being printed\n",
    "\n",
    "    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
    "    explanation_response = openai.Completion.create(\n",
    "        model=text_model,\n",
    "        prompt=prompt_to_explain_the_function,\n",
    "        stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "    explanation_completion = \"\"\n",
    "    if print_text:\n",
    "        completion_color_prefix = \"\\033[92m\"  # green\n",
    "        print(completion_color_prefix, end=\"\")\n",
    "    for event in explanation_response:\n",
    "        event_text = event[\"choices\"][0][\"text\"]\n",
    "        explanation_completion += event_text\n",
    "        if print_text:\n",
    "            print(event_text, end=\"\")\n",
    "\n",
    "    # Step 2: Generate a plan to write a unit test\n",
    "\n",
    "    # create a markdown-formatted prompt that asks GPT-3 to complete a plan for writing unit tests, formatted as a bullet list\n",
    "    prompt_to_explain_a_plan = f\"\"\"\n",
    "    \n",
    "A good unit test suite should aim to:\n",
    "- Test the function's behavior for a wide range of possible inputs\n",
    "- Test edge cases that the author may not have foreseen\n",
    "- Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n",
    "- Be easy to read and understand, with clean code and descriptive names\n",
    "- Be deterministic, so that the tests always pass or fail in the same way\n",
    "\n",
    "`{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
    "\n",
    "For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
    "-\"\"\"\n",
    "    if print_text:\n",
    "        print(text_color_prefix + prompt_to_explain_a_plan, end=\"\")\n",
    "\n",
    "    # append this planning prompt to the results from step 1\n",
    "    prior_text = prompt_to_explain_the_function + explanation_completion\n",
    "    full_plan_prompt = prior_text + prompt_to_explain_a_plan\n",
    "\n",
    "    # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
    "    plan_response = openai.Completion.create(\n",
    "        model=text_model,\n",
    "        prompt=full_plan_prompt,\n",
    "        stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "    plan_completion = \"\"\n",
    "    if print_text:\n",
    "        print(completion_color_prefix, end=\"\")\n",
    "    for event in plan_response:\n",
    "        event_text = event[\"choices\"][0][\"text\"]\n",
    "        plan_completion += event_text\n",
    "        if print_text:\n",
    "            print(event_text, end=\"\")\n",
    "\n",
    "    # Step 2b: If the plan is short, ask GPT-3 to elaborate further\n",
    "    # this counts top-level bullets (e.g., categories), but not sub-bullets (e.g., test cases)\n",
    "    elaboration_needed = plan_completion.count(\"\\n-\") +1 < approx_min_cases_to_cover  # adds 1 because the first bullet is not counted\n",
    "    if elaboration_needed:\n",
    "        prompt_to_elaborate_on_the_plan = f\"\"\"\n",
    "\n",
    "In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n",
    "-\"\"\"\n",
    "        if print_text:\n",
    "            print(text_color_prefix + prompt_to_elaborate_on_the_plan, end=\"\")\n",
    "\n",
    "        # append this elaboration prompt to the results from step 2\n",
    "        prior_text = full_plan_prompt + plan_completion\n",
    "        full_elaboration_prompt = prior_text + prompt_to_elaborate_on_the_plan\n",
    "\n",
    "        # send the prompt to the API, using \\n\\n as a stop sequence to stop at the end of the bullet list\n",
    "        elaboration_response = openai.Completion.create(\n",
    "            model=text_model,\n",
    "            prompt=full_elaboration_prompt,\n",
    "            stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            stream=True,\n",
    "        )\n",
    "        elaboration_completion = \"\"\n",
    "        if print_text:\n",
    "            print(completion_color_prefix, end=\"\")\n",
    "        for event in elaboration_response:\n",
    "            event_text = event[\"choices\"][0][\"text\"]\n",
    "            elaboration_completion += event_text\n",
    "            if print_text:\n",
    "                print(event_text, end=\"\")\n",
    "\n",
    "    # Step 3: Generate the unit test\n",
    "\n",
    "    # create a markdown-formatted prompt that asks GPT-3 to complete a unit test\n",
    "    starter_comment = \"\"\n",
    "    if unit_test_package == \"pytest\":\n",
    "        starter_comment = \"Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n",
    "    prompt_to_generate_the_unit_test = f\"\"\"\n",
    "\n",
    "Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
    "```python\n",
    "import {unit_test_package}  # used for our unit tests\n",
    "\n",
    "{function_to_test}\n",
    "\n",
    "#{starter_comment}\"\"\"\n",
    "    if print_text:\n",
    "        print(text_color_prefix + prompt_to_generate_the_unit_test, end=\"\")\n",
    "\n",
    "    # append this unit test prompt to the results from step 3\n",
    "    if elaboration_needed:\n",
    "        prior_text = full_elaboration_prompt + elaboration_completion\n",
    "    else:\n",
    "        prior_text = full_plan_prompt + plan_completion\n",
    "    full_unit_test_prompt = prior_text + prompt_to_generate_the_unit_test\n",
    "\n",
    "    # send the prompt to the API, using ``` as a stop sequence to stop at the end of the code block\n",
    "    unit_test_response = openai.Completion.create(\n",
    "        model=code_model,\n",
    "        prompt=full_unit_test_prompt,\n",
    "        stop=\"```\",\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stream=True\n",
    "    )\n",
    "    unit_test_completion = \"\"\n",
    "    if print_text:\n",
    "        print(completion_color_prefix, end=\"\")\n",
    "    for event in unit_test_response:\n",
    "        event_text = event[\"choices\"][0][\"text\"]\n",
    "        unit_test_completion += event_text\n",
    "        if print_text:\n",
    "            print(event_text, end=\"\")\n",
    "\n",
    "    # check the output for errors\n",
    "    code_start_index = prompt_to_generate_the_unit_test.find(\"```python\\n\") + len(\"```python\\n\")\n",
    "    code_output = prompt_to_generate_the_unit_test[code_start_index:] + unit_test_completion\n",
    "    try:\n",
    "        ast.parse(code_output)\n",
    "    except SyntaxError as e:\n",
    "        print(f\"Syntax error in generated code: {e}\")\n",
    "        if reruns_if_fail > 0:\n",
    "            print(\"Rerunning...\")\n",
    "            return unit_test_from_function(\n",
    "                function_to_test=function_to_test,\n",
    "                unit_test_package=unit_test_package,\n",
    "                approx_min_cases_to_cover=approx_min_cases_to_cover,\n",
    "                print_text=print_text,\n",
    "                text_model=text_model,\n",
    "                code_model=code_model,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                reruns_if_fail=reruns_if_fail-1,  # decrement rerun counter when calling again\n",
    "            )\n",
    "\n",
    "    # return the unit test as a string\n",
    "    return unit_test_completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m# How to write great unit tests with pytest\n",
      "\n",
      "In this advanced tutorial for experts, we'll use Python 3.9 and `pytest` to write a suite of unit tests to verify the behavior of the following function.\n",
      "```python\n",
      "def is_palindrome(s):\n",
      "    return s == s[::-1]\n",
      "```\n",
      "\n",
      "Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
      "- First,\u001b[92m we have a function definition. This is where we give the function a name, `is_palindrome`, and specify the arguments that the function accepts. In this case, the function accepts a single string argument, `s`.\n",
      "- Next, we have a return statement. This is where we specify the value that the function returns. In this case, the function returns `s == s[::-1]`.\n",
      "- Finally, we have a function call. This is where we actually call the function with a specific set of arguments. In this case, we're calling the function with the string `\"racecar\"`.\u001b[30m\n",
      "    \n",
      "A good unit test suite should aim to:\n",
      "- Test the function's behavior for a wide range of possible inputs\n",
      "- Test edge cases that the author may not have foreseen\n",
      "- Take advantage of the features of `pytest` to make the tests easy to write and maintain\n",
      "- Be easy to read and understand, with clean code and descriptive names\n",
      "- Be deterministic, so that the tests always pass or fail in the same way\n",
      "\n",
      "`pytest` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
      "\n",
      "For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
      "-\u001b[92m The input is a palindrome\n",
      "    - `\"racecar\"`\n",
      "    - `\"madam\"`\n",
      "    - `\"anna\"`\n",
      "- The input is not a palindrome\n",
      "    - `\"python\"`\n",
      "    - `\"test\"`\n",
      "    - `\"1234\"`\n",
      "- The input is an empty string\n",
      "    - `\"\"`\n",
      "- The input is `None`\n",
      "- The input is not a string\n",
      "    - `1`\n",
      "    - `1.0`\n",
      "    - `True`\n",
      "    - `False`\n",
      "    - `[]`\n",
      "    - `{}`\u001b[30m\n",
      "\n",
      "In addition to the scenarios above, we'll also want to make sure we don't forget to test rare or unexpected edge cases (and under each edge case, we include a few examples as sub-bullets):\n",
      "-\u001b[92m The input is a palindrome with spaces\n",
      "    - `\"race car\"`\n",
      "    - `\" madam \"`\n",
      "    - `\" anna \"`\n",
      "- The input is not a palindrome with spaces\n",
      "    - `\" python \"`\n",
      "    - `\" test \"`\n",
      "    - `\" 1234 \"`\n",
      "- The input is a palindrome with punctuation\n",
      "    - `\"racecar!\"`\n",
      "    - `\"Madam, I'm Adam.\"`\n",
      "    - `\"Anna's\"`\n",
      "- The input is not a palindrome with punctuation\n",
      "    - `\"python!\"`\n",
      "    - `\"test.\"`\n",
      "    - `\"1234!\"`\n",
      "- The input is a palindrome with mixed case\n",
      "    - `\"Racecar\"`\n",
      "    - `\"Madam\"`\n",
      "    - `\"Anna\"`\n",
      "- The input is not a palindrome with mixed case\n",
      "    - `\"Python\"`\n",
      "    - `\"Test\"`\n",
      "    - `\"1234\"`\u001b[30m\n",
      "\n",
      "Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
      "```python\n",
      "import pytest  # used for our unit tests\n",
      "\n",
      "def is_palindrome(s):\n",
      "    return s == s[::-1]\n",
      "\n",
      "#Below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\u001b[92m.\n",
      "#The first element of the tuple is a name for the test case, and the second element is a list of arguments for the test case.\n",
      "#The @pytest.mark.parametrize decorator will generate a separate test function for each test case.\n",
      "#The generated test function will be named test_is_palindrome_<name> where <name> is the name of the test case.\n",
      "#The generated test function will be given the arguments specified in the list of arguments for the test case.\n",
      "#The generated test function will be given the fixture specified in the decorator, in this case the function itself.\n",
      "#The generated test function will call the function with the arguments and assert that the result is equal to the expected value.\n",
      "@pytest.mark.parametrize(\n",
      "    \"name,args,expected\",\n",
      "    [\n",
      "        # Test the function's behavior for a wide range of possible inputs\n",
      "        (\"palindrome\", [\"racecar\"], True),\n",
      "        (\"palindrome\", [\"madam\"], True),\n",
      "        (\"palindrome\", [\"anna\"], True),\n",
      "        (\"non-palindrome\", [\"python\"], False),\n",
      "        (\"non-palindrome\", [\"test\"], False),\n",
      "        (\"non-palindrome\", [\"1234\"], False),\n",
      "        (\"empty string\", [\"\"], True),\n",
      "        (\"None\", [None], False),\n",
      "        (\"non-string\", [1], False),\n",
      "        (\"non-string\", [1.0], False),\n",
      "        (\"non-string\", [True], False),\n",
      "        (\"non-string\", [False], False),\n",
      "        (\"non-string\", [[]], False),\n",
      "        (\"non-string\", [{}], False),\n",
      "        # Test edge cases that the author may not have foreseen\n",
      "        (\"palindrome with spaces\", [\"race car\"], True),\n",
      "        (\"palindrome with spaces\", [\" madam \"], True),\n",
      "        (\"palindrome with spaces\", [\" anna \"], True),\n",
      "        (\"non-palindrome with spaces\", [\" python \"], False),\n",
      "        (\"non-palindrome with spaces\", [\" test \"], False),\n",
      "        (\"non-palindrome with spaces\", [\" 1234 \"], False),\n",
      "        (\"palindrome with punctuation\", [\"racecar!\"], True),\n",
      "        (\"palindrome with punctuation\", [\"Madam, I'm Adam.\"], True),\n",
      "        (\"palindrome with punctuation\", [\"Anna's\"], True),\n",
      "        (\"non-palindrome with punctuation\", [\"python!\"], False),\n",
      "        (\"non-palindrome with punctuation\", [\"test.\"], False),\n",
      "        (\"non-palindrome with punctuation\", [\"1234!\"], False),\n",
      "        (\"palindrome with mixed case\", [\"Racecar\"], True),\n",
      "        (\"palindrome with mixed case\", [\"Madam\"], True),\n",
      "        (\"palindrome with mixed case\", [\"Anna\"], True),\n",
      "        (\"non-palindrome with mixed case\", [\"Python\"], False),\n",
      "        (\"non-palindrome with mixed case\", [\"Test\"], False),\n",
      "        (\"non-palindrome with mixed case\", [\"1234\"], False),\n",
      "    ],\n",
      ")\n",
      "def test_is_palindrome(is_palindrome, args, expected):\n",
      "    assert is_palindrome(*args) == expected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\n#The first element of the tuple is a name for the test case, and the second element is a list of arguments for the test case.\\n#The @pytest.mark.parametrize decorator will generate a separate test function for each test case.\\n#The generated test function will be named test_is_palindrome_<name> where <name> is the name of the test case.\\n#The generated test function will be given the arguments specified in the list of arguments for the test case.\\n#The generated test function will be given the fixture specified in the decorator, in this case the function itself.\\n#The generated test function will call the function with the arguments and assert that the result is equal to the expected value.\\n@pytest.mark.parametrize(\\n    \"name,args,expected\",\\n    [\\n        # Test the function\\'s behavior for a wide range of possible inputs\\n        (\"palindrome\", [\"racecar\"], True),\\n        (\"palindrome\", [\"madam\"], True),\\n        (\"palindrome\", [\"anna\"], True),\\n        (\"non-palindrome\", [\"python\"], False),\\n        (\"non-palindrome\", [\"test\"], False),\\n        (\"non-palindrome\", [\"1234\"], False),\\n        (\"empty string\", [\"\"], True),\\n        (\"None\", [None], False),\\n        (\"non-string\", [1], False),\\n        (\"non-string\", [1.0], False),\\n        (\"non-string\", [True], False),\\n        (\"non-string\", [False], False),\\n        (\"non-string\", [[]], False),\\n        (\"non-string\", [{}], False),\\n        # Test edge cases that the author may not have foreseen\\n        (\"palindrome with spaces\", [\"race car\"], True),\\n        (\"palindrome with spaces\", [\" madam \"], True),\\n        (\"palindrome with spaces\", [\" anna \"], True),\\n        (\"non-palindrome with spaces\", [\" python \"], False),\\n        (\"non-palindrome with spaces\", [\" test \"], False),\\n        (\"non-palindrome with spaces\", [\" 1234 \"], False),\\n        (\"palindrome with punctuation\", [\"racecar!\"], True),\\n        (\"palindrome with punctuation\", [\"Madam, I\\'m Adam.\"], True),\\n        (\"palindrome with punctuation\", [\"Anna\\'s\"], True),\\n        (\"non-palindrome with punctuation\", [\"python!\"], False),\\n        (\"non-palindrome with punctuation\", [\"test.\"], False),\\n        (\"non-palindrome with punctuation\", [\"1234!\"], False),\\n        (\"palindrome with mixed case\", [\"Racecar\"], True),\\n        (\"palindrome with mixed case\", [\"Madam\"], True),\\n        (\"palindrome with mixed case\", [\"Anna\"], True),\\n        (\"non-palindrome with mixed case\", [\"Python\"], False),\\n        (\"non-palindrome with mixed case\", [\"Test\"], False),\\n        (\"non-palindrome with mixed case\", [\"1234\"], False),\\n    ],\\n)\\ndef test_is_palindrome(is_palindrome, args, expected):\\n    assert is_palindrome(*args) == expected\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_function = \"\"\"def is_palindrome(s):\n",
    "    return s == s[::-1]\"\"\"\n",
    "\n",
    "unit_test_from_function(example_function, print_text=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('openai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
