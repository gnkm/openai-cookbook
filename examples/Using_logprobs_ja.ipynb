{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åˆ†é¡ã¨Q&Aè©•ä¾¡ã§ã®logprobsã®ä½¿ç”¨\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Chat Completions APIã®`logprobs`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä½¿ç”¨æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚`logprobs`ãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ã‚‹å ´åˆã€APIã¯å„å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®å¯¾æ•°ç¢ºç‡ã¨ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã§ã®æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®é™å®šæ•°ã¨ãã®å¯¾æ•°ç¢ºç‡ã‚’è¿”ã—ã¾ã™ã€‚é–¢é€£ã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n",
    "* `logprobs`: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®å¯¾æ•°ç¢ºç‡ã‚’è¿”ã™ã‹ã©ã†ã‹ã€‚trueã®å ´åˆã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§è¿”ã•ã‚Œã‚‹å„å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®å¯¾æ•°ç¢ºç‡ã‚’è¿”ã—ã¾ã™ã€‚\n",
    "* `top_logprobs`: å„ãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®ã§è¿”ã™æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®æ•°ã‚’æŒ‡å®šã™ã‚‹0ã‹ã‚‰5ã®é–“ã®æ•´æ•°ã§ã€ãã‚Œãã‚Œã«é–¢é€£ã™ã‚‹å¯¾æ•°ç¢ºç‡ãŒä»˜ãã¾ã™ã€‚ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€`logprobs`ã‚’trueã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®å¯¾æ•°ç¢ºç‡ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã®å„ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã§ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºã—ã¾ã™ã€‚ç°¡å˜ã«è¨€ã†ã¨ã€logprobã¯`log(p)`ã§ã€`p` = ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«åŸºã¥ã„ã¦ç‰¹å®šã®ä½ç½®ã§ãƒˆãƒ¼ã‚¯ãƒ³ãŒç™ºç”Ÿã™ã‚‹ç¢ºç‡ã§ã™ã€‚`logprobs`ã«é–¢ã™ã‚‹é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼š\n",
    "* ã‚ˆã‚Šé«˜ã„å¯¾æ•°ç¢ºç‡ã¯ã€ãã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®å¯èƒ½æ€§ãŒã‚ˆã‚Šé«˜ã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã«å¯¾ã™ã‚‹ä¿¡é ¼åº¦ã‚’æ¸¬å®šã—ãŸã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒæ¤œè¨ã—ãŸä»£æ›¿çš„ãªå¿œç­”ã‚’æ¢ç´¢ã—ãŸã‚Šã§ãã¾ã™ã€‚\n",
    "* Logprobã¯ä»»æ„ã®è² ã®æ•°ã¾ãŸã¯`0.0`ã«ãªã‚Šã¾ã™ã€‚`0.0`ã¯100%ã®ç¢ºç‡ã«å¯¾å¿œã—ã¾ã™ã€‚\n",
    "* Logprobsã«ã‚ˆã‚Šã€å€‹ã€…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®logprobsã®åˆè¨ˆã¨ã—ã¦ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®çµåˆç¢ºç‡ã‚’è¨ˆç®—ã§ãã¾ã™ã€‚ã“ã‚Œã¯ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«æœ‰ç”¨ã§ã™ã€‚ã‚‚ã†ä¸€ã¤ã®ä¸€èˆ¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€æœ€è‰¯ã®ç”Ÿæˆã‚’é¸æŠã™ã‚‹ãŸã‚ã«æ–‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®å¹³å‡logprobã‚’å–ã‚‹ã“ã¨ã§ã™ã€‚\n",
    "* ç•°ãªã‚‹å€™è£œãƒˆãƒ¼ã‚¯ãƒ³ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸ`logprobs`ã‚’èª¿ã¹ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å¦¥å½“ã¾ãŸã¯ä¸å¦¥å½“ã¨è€ƒãˆãŸã‹ã‚’ç†è§£ã§ãã¾ã™ã€‚\n",
    "\n",
    "`logprobs`ã«ã¯å¹…åºƒã„ä½¿ç”¨ä¾‹ãŒã‚ã‚Šã¾ã™ãŒã€ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã®ç”¨é€”ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ï¼š\n",
    "\n",
    "1. åˆ†é¡ã‚¿ã‚¹ã‚¯\n",
    "\n",
    "* å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯å¤šãã®åˆ†é¡ã‚¿ã‚¹ã‚¯ã«å„ªã‚Œã¦ã„ã¾ã™ãŒã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã«å¯¾ã™ã‚‹ä¿¡é ¼åº¦ã‚’æ­£ç¢ºã«æ¸¬å®šã™ã‚‹ã“ã¨ã¯å›°é›£ãªå ´åˆãŒã‚ã‚Šã¾ã™ã€‚`logprobs`ã¯å„ã‚¯ãƒ©ã‚¹äºˆæ¸¬ã«é–¢é€£ã™ã‚‹ç¢ºç‡ã‚’æä¾›ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç‹¬è‡ªã®åˆ†é¡ã¾ãŸã¯ä¿¡é ¼åº¦ã®é–¾å€¤ã‚’è¨­å®šã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
    "\n",
    "2. æ¤œç´¢ï¼ˆQ&Aï¼‰è©•ä¾¡\n",
    "\n",
    "* `logprobs`ã¯æ¤œç´¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®è‡ªå·±è©•ä¾¡ã‚’æ”¯æ´ã§ãã¾ã™ã€‚Q&Aã®ä¾‹ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯ä½œç‚ºçš„ãª`has_sufficient_context_for_answer`ãƒ–ãƒ¼ãƒ«å€¤ã‚’å‡ºåŠ›ã—ã€ã“ã‚Œã¯ç­”ãˆãŒæ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚ã“ã®ç¨®ã®è©•ä¾¡ã¯æ¤œç´¢ãƒ™ãƒ¼ã‚¹ã®å¹»è¦šã‚’æ¸›ã‚‰ã—ã€ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "3. ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆ\n",
    "*  `logprobs`ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ã¦ã„ã‚‹éš›ã«å˜èªã‚’ææ¡ˆã™ã‚‹æ–¹æ³•ã‚’æ±ºå®šã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚\n",
    "\n",
    "4. ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¤ãƒ©ã‚¤ãƒˆã¨ãƒã‚¤ãƒˆå‡ºåŠ›\n",
    "*  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯`logprobs`ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã§ä»˜å±ã™ã‚‹çµ„ã¿è¾¼ã¿ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’ä½¿ç”¨ã—ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¤ãƒ©ã‚¤ã‚¿ãƒ¼ã‚’ç°¡å˜ã«ä½œæˆã§ãã¾ã™ã€‚ã•ã‚‰ã«ã€bytesãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯å„å‡ºåŠ›æ–‡å­—ã®ASCIIã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã¯çµµæ–‡å­—ã‚„ç‰¹æ®Šæ–‡å­—ã‚’å†ç¾ã™ã‚‹ã®ã«ç‰¹ã«æœ‰ç”¨ã§ã™ã€‚\n",
    "\n",
    "5. ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã®è¨ˆç®—\n",
    "* `logprobs`ã¯ã€çµæœã«å¯¾ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å…¨ä½“çš„ãªä¿¡é ¼åº¦ã‚’è©•ä¾¡ã—ã€ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ã®çµæœã®ä¿¡é ¼åº¦ã‚’æ¯”è¼ƒã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from math import exp\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹ä¿¡é ¼åº¦è©•ä¾¡ã®ãŸã‚ã®`logprobs`ã®ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã®ã‚»ãƒƒãƒˆã«åˆ†é¡ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆã—ãŸã„ã¨ã—ã¾ã—ã‚‡ã†ã€‚`logprobs`ãªã—ã§ã‚‚ã€Chat Completionsã‚’ä½¿ç”¨ã—ã¦ã“ã‚Œã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ãŒã€ãƒ¢ãƒ‡ãƒ«ãŒãã®åˆ†é¡ã‚’ã©ã®ç¨‹åº¦ã®ç¢ºä¿¡åº¦ã§è¡Œã£ãŸã‹ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ã¯ã¯ã‚‹ã‹ã«å›°é›£ã§ã™ã€‚\n",
    "\n",
    "ç¾åœ¨ã€`logprobs`ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒãã®äºˆæ¸¬ã«ã©ã®ç¨‹åº¦ç¢ºä¿¡ã‚’æŒã£ã¦ã„ã‚‹ã‹ã‚’æ­£ç¢ºã«ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ã“ã‚Œã¯ã€æ­£ç¢ºã§ä¿¡é ¼ã§ãã‚‹åˆ†é¡å™¨ã‚’ä½œæˆã™ã‚‹ä¸Šã§éå¸¸ã«é‡è¦ã§ã™ã€‚ä¾‹ãˆã°ã€é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã®å¯¾æ•°ç¢ºç‡ãŒé«˜ã„å ´åˆã€ã“ã‚Œã¯ãƒ¢ãƒ‡ãƒ«ãŒãã®åˆ†é¡ã«éå¸¸ã«ç¢ºä¿¡ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¾ã™ã€‚ä½ã„å ´åˆã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ç¢ºä¿¡åº¦ãŒä½ã„ã“ã¨ã‚’ç¤ºå”†ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ã®åˆ†é¡ãŒæœŸå¾…ã—ãŸã‚‚ã®ã§ã¯ãªã„å ´åˆã‚„ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’äººé–“ãŒãƒ¬ãƒ“ãƒ¥ãƒ¼ã¾ãŸã¯æ¤œè¨¼ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã«ç‰¹ã«æœ‰ç”¨ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã«4ã¤ã®ã‚«ãƒ†ã‚´ãƒªï¼ˆ**ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã€æ”¿æ²»ã€ã‚¹ãƒãƒ¼ãƒ„ã€èŠ¸è¡“**ï¼‰ã‚’æç¤ºã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰å§‹ã‚ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯ã€è¨˜äº‹ã®è¦‹å‡ºã—ã®ã¿ã«åŸºã¥ã„ã¦ã€ã“ã‚Œã‚‰ã®ã‚«ãƒ†ã‚´ãƒªã«è¨˜äº‹ã‚’åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_PROMPT = \"\"\"You will be given a headline of a news article.\n",
    "Classify the article into one of the following categories: Technology, Politics, Sports, and Art.\n",
    "Return only the name of the category, and nothing else.\n",
    "MAKE SURE your output is one of the four categories stated.\n",
    "Article headline: {headline}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3ã¤ã®ã‚µãƒ³ãƒ—ãƒ«è¦‹å‡ºã—ã‚’è¦‹ã¦ã€ã¾ãš`logprobs`ã‚’ä½¿ã‚ãªã„æ¨™æº–çš„ãªChat Completionsã®å‡ºåŠ›ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [\n",
    "    \"Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\",\n",
    "    \"Local Mayor Launches Initiative to Enhance Urban Public Transport.\",\n",
    "    \"Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n",
      "Category: Technology\n",
      "\n",
      "\n",
      "Headline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\n",
      "Category: Politics\n",
      "\n",
      "\n",
      "Headline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\n",
      "Category: Art\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for headline in headlines:\n",
    "    print(f\"\\nHeadline: {headline}\")\n",
    "    API_RESPONSE = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "    print(f\"Category: {API_RESPONSE.choices[0].message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã“ã§ã¯ã€å„è¦‹å‡ºã—ã«å¯¾ã—ã¦é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã‚’ç¢ºèªã§ãã¾ã™ã€‚ã—ã‹ã—ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã«å¯¾ã™ã‚‹ä¿¡é ¼åº¦ã¯è¦‹ãˆã¾ã›ã‚“ã€‚åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å†å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ä»Šåº¦ã¯`logprobs`ã‚’æœ‰åŠ¹ã«ã—ã€`top_logprobs`ã‚’2ã«è¨­å®šã—ã¾ã™ï¼ˆã“ã‚Œã«ã‚ˆã‚Šã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„2ã¤ã®å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼‰ã€‚ã•ã‚‰ã«ã€å„å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®ç·šå½¢ç¢ºç‡ã‚‚å‡ºåŠ›ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å¯¾æ•°ç¢ºç‡ã‚’ã‚ˆã‚Šè§£é‡ˆã—ã‚„ã™ã„0-100%ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«å¤‰æ›ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Technology, <span style='color: darkorange'>logprobs:</span> 0.0, <span style='color: magenta'>linear probability:</span> 100.0%<br><span style='color: cyan'>Output token 2:</span>  Technology, <span style='color: darkorange'>logprobs:</span> -18.75, <span style='color: magenta'>linear probability:</span> 0.0%<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Headline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Politics, <span style='color: darkorange'>logprobs:</span> -3.1281633e-07, <span style='color: magenta'>linear probability:</span> 100.0%<br><span style='color: cyan'>Output token 2:</span> Polit, <span style='color: darkorange'>logprobs:</span> -16.0, <span style='color: magenta'>linear probability:</span> 0.0%<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Headline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style='color: cyan'>Output token 1:</span> Art, <span style='color: darkorange'>logprobs:</span> -0.028133942, <span style='color: magenta'>linear probability:</span> 97.23%<br><span style='color: cyan'>Output token 2:</span> Sports, <span style='color: darkorange'>logprobs:</span> -4.278134, <span style='color: magenta'>linear probability:</span> 1.39%<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for headline in headlines:\n",
    "    print(f\"\\nHeadline: {headline}\")\n",
    "    API_RESPONSE = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        logprobs=True,\n",
    "        top_logprobs=2,\n",
    "    )\n",
    "    top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n",
    "    html_content = \"\"\n",
    "    for i, logprob in enumerate(top_two_logprobs, start=1):\n",
    "        html_content += (\n",
    "            f\"<span style='color: cyan'>Output token {i}:</span> {logprob.token}, \"\n",
    "            f\"<span style='color: darkorange'>logprobs:</span> {logprob.logprob}, \"\n",
    "            f\"<span style='color: magenta'>linear probability:</span> {np.round(np.exp(logprob.logprob)*100,2)}%<br>\"\n",
    "        )\n",
    "    display(HTML(html_content))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åˆã®2ã¤ã®è¦‹å‡ºã—ã‹ã‚‰äºˆæƒ³ã•ã‚Œã‚‹é€šã‚Šã€gpt-4o-miniã¯ãã®åˆ†é¡ã«100%ã®ç¢ºä¿¡ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒãã‚Œãã‚Œæ˜ç¢ºã«ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã¨æ”¿æ²»ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚ã—ã‹ã—ã€3ã¤ç›®ã®è¦‹å‡ºã—ã¯ã‚¹ãƒãƒ¼ãƒ„ã¨ã‚¢ãƒ¼ãƒˆé–¢é€£ã®ãƒ†ãƒ¼ãƒã®ä¸¡æ–¹ã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã‚‹ãŸã‚ã€ä¿¡é ¼åº¦ãŒã‚ãšã‹ã«ä½ã„97%ã¨ãªã£ã¦ã„ã¾ã™ãŒã€ãã‚Œã§ã‚‚åˆ†é¡ã«å¯¾ã—ã¦å¼·ã„ç¢ºä¿¡ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "`logprobs`ã¯åˆ†é¡ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦éå¸¸ã«æœ‰ç”¨ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä¿¡é ¼åº¦ã®é–¾å€¤ã‚’è¨­å®šã—ãŸã‚Šã€é¸æŠã•ã‚ŒãŸå‡ºåŠ›ã®å¯¾æ•°ç¢ºç‡ãŒååˆ†ã«é«˜ããªã„å ´åˆã«è¤‡æ•°ã®æ½œåœ¨çš„ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡ºåŠ›ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€è¨˜äº‹ã«ã‚¿ã‚°ä»˜ã‘ã™ã‚‹ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆã™ã‚‹éš›ã€ç‰¹å®šã®é–¾å€¤ã‚’è¶…ãˆã‚‹è¦‹å‡ºã—ã‚’è‡ªå‹•çš„ã«åˆ†é¡ã—ã€ç¢ºä¿¡åº¦ã®ä½ã„ã‚‚ã®ã¯æ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«é€ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å¹»è¦šã‚’æ¸›ã‚‰ã™ãŸã‚ã®æ¤œç´¢ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¹»è¦šã‚’æ¸›ã‚‰ã—ã€RAGãƒ™ãƒ¼ã‚¹ã®Q&Aã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€`logprobs`ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ãŒæ¤œç´¢ã«ã©ã®ç¨‹åº¦ç¢ºä¿¡ã‚’æŒã£ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¾ã™ï¼š\n",
    "\n",
    "Q&Aç”¨ã«RAGã‚’ä½¿ç”¨ã—ãŸæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ãŸãŒã€è³ªå•ã«å¯¾ã™ã‚‹å¹»è¦šçš„ãªå›ç­”ã«æ‚©ã¾ã•ã‚Œã¦ã„ã‚‹ã¨ã—ã¾ã—ã‚‡ã†ã€‚*æ³¨æ„:* ã“ã®ä¾‹ã§ã¯ã€ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸè¨˜äº‹ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€Q&Aã§RAGã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã«ã¤ã„ã¦ã¯ã€ã‚¯ãƒƒã‚¯ãƒ–ãƒƒã‚¯ã®ä»–ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article retrieved\n",
    "ada_lovelace_article = \"\"\"Augusta Ada King, Countess of Lovelace (nÃ©e Byron; 10 December 1815 â€“ 27 November 1852) was an English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation.\n",
    "Ada Byron was the only legitimate child of poet Lord Byron and reformer Lady Byron. All Lovelace's half-siblings, Lord Byron's other children, were born out of wedlock to other women. Byron separated from his wife a month after Ada was born and left England forever. He died in Greece when Ada was eight. Her mother was anxious about her upbringing and promoted Ada's interest in mathematics and logic in an effort to prevent her from developing her father's perceived insanity. Despite this, Ada remained interested in him, naming her two sons Byron and Gordon. Upon her death, she was buried next to him at her request. Although often ill in her childhood, Ada pursued her studies assiduously. She married William King in 1835. King was made Earl of Lovelace in 1838, Ada thereby becoming Countess of Lovelace.\n",
    "Her educational and social exploits brought her into contact with scientists such as Andrew Crosse, Charles Babbage, Sir David Brewster, Charles Wheatstone, Michael Faraday, and the author Charles Dickens, contacts which she used to further her education. Ada described her approach as \"poetical science\" and herself as an \"Analyst (& Metaphysician)\".\n",
    "When she was eighteen, her mathematical talents led her to a long working relationship and friendship with fellow British mathematician Charles Babbage, who is known as \"the father of computers\". She was in particular interested in Babbage's work on the Analytical Engine. Lovelace first met him in June 1833, through their mutual friend, and her private tutor, Mary Somerville.\n",
    "Between 1842 and 1843, Ada translated an article by the military engineer Luigi Menabrea (later Prime Minister of Italy) about the Analytical Engine, supplementing it with an elaborate set of seven notes, simply called \"Notes\".\n",
    "Lovelace's notes are important in the early history of computers, especially since the seventh one contained what many consider to be the first computer programâ€”that is, an algorithm designed to be carried out by a machine. Other historians reject this perspective and point out that Babbage's personal notes from the years 1836/1837 contain the first programs for the engine. She also developed a vision of the capability of computers to go beyond mere calculating or number-crunching, while many others, including Babbage himself, focused only on those capabilities. Her mindset of \"poetical science\" led her to ask questions about the Analytical Engine (as shown in her notes) examining how individuals and society relate to technology as a collaborative tool.\n",
    "\"\"\"\n",
    "\n",
    "# Questions that can be easily answered given the article\n",
    "easy_questions = [\n",
    "    \"What nationality was Ada Lovelace?\",\n",
    "    \"What was an important finding from Lovelace's seventh note?\",\n",
    "]\n",
    "\n",
    "# Questions that are not fully covered in the article\n",
    "medium_questions = [\n",
    "    \"Did Lovelace collaborate with Charles Dickens\",\n",
    "    \"What concepts did Lovelace build with Charles Babbage\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»Šåº¦ã¯ã€ãƒ¢ãƒ‡ãƒ«ã«è³ªå•ã«å›ç­”ã—ã¦ã‚‚ã‚‰ã†ã ã‘ã§ãªãã€ãã®å›ç­”ã‚’è©•ä¾¡ã—ã¦ã‚‚ã‚‰ã†ã“ã¨ãŒã§ãã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã«ãƒ–ãƒ¼ãƒ«å€¤ã®`has_sufficient_context_for_answer`ã‚’å‡ºåŠ›ã™ã‚‹ã‚ˆã†æ±‚ã‚ã¾ã™ã€‚ãã—ã¦ã€`logprobs`ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ã§ã€æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç­”ãˆãŒå«ã¾ã‚Œã¦ã„ãŸã“ã¨ã«ã¤ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã‚Œã»ã©ç¢ºä¿¡ã‚’æŒã£ã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"You retrieved this article: {article}. The question is: {question}.\n",
    "Before even answering the question, consider whether you have sufficient information in the article to answer the question fully.\n",
    "Your output should JUST be the boolean true or false, of if you have sufficient information in the article to answer the question.\n",
    "Respond with just one word, the boolean true or false. You must output the word 'True', or the word 'False', nothing else.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Questions clearly answered in article<p style=\"color:green\">Question: What nationality was Ada Lovelace?</p><p style=\"color:cyan\">has_sufficient_context_for_answer: True, <span style=\"color:darkorange\">logprobs: -3.1281633e-07, <span style=\"color:magenta\">linear probability: 100.0%</span></p><p style=\"color:green\">Question: What was an important finding from Lovelace's seventh note?</p><p style=\"color:cyan\">has_sufficient_context_for_answer: True, <span style=\"color:darkorange\">logprobs: -7.89631e-07, <span style=\"color:magenta\">linear probability: 100.0%</span></p>Questions only partially covered in the article<p style=\"color:green\">Question: Did Lovelace collaborate with Charles Dickens</p><p style=\"color:cyan\">has_sufficient_context_for_answer: False, <span style=\"color:darkorange\">logprobs: -0.008654992, <span style=\"color:magenta\">linear probability: 99.14%</span></p><p style=\"color:green\">Question: What concepts did Lovelace build with Charles Babbage</p><p style=\"color:cyan\">has_sufficient_context_for_answer: True, <span style=\"color:darkorange\">logprobs: -0.004082317, <span style=\"color:magenta\">linear probability: 99.59%</span></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html_output = \"\"\n",
    "html_output += \"Questions clearly answered in article\"\n",
    "\n",
    "for question in easy_questions:\n",
    "    API_RESPONSE = get_completion(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(\n",
    "                    article=ada_lovelace_article, question=question\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        logprobs=True,\n",
    "    )\n",
    "    html_output += f'<p style=\"color:green\">Question: {question}</p>'\n",
    "    for logprob in API_RESPONSE.choices[0].logprobs.content:\n",
    "        html_output += f'<p style=\"color:cyan\">has_sufficient_context_for_answer: {logprob.token}, <span style=\"color:darkorange\">logprobs: {logprob.logprob}, <span style=\"color:magenta\">linear probability: {np.round(np.exp(logprob.logprob)*100,2)}%</span></p>'\n",
    "\n",
    "html_output += \"Questions only partially covered in the article\"\n",
    "\n",
    "for question in medium_questions:\n",
    "    API_RESPONSE = get_completion(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(\n",
    "                    article=ada_lovelace_article, question=question\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o\",\n",
    "        logprobs=True,\n",
    "        top_logprobs=3,\n",
    "    )\n",
    "    html_output += f'<p style=\"color:green\">Question: {question}</p>'\n",
    "    for logprob in API_RESPONSE.choices[0].logprobs.content:\n",
    "        html_output += f'<p style=\"color:cyan\">has_sufficient_context_for_answer: {logprob.token}, <span style=\"color:darkorange\">logprobs: {logprob.logprob}, <span style=\"color:magenta\">linear probability: {np.round(np.exp(logprob.logprob)*100,2)}%</span></p>'\n",
    "\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åˆã®2ã¤ã®è³ªå•ã«ã¤ã„ã¦ã€æˆ‘ã€…ã®ãƒ¢ãƒ‡ãƒ«ã¯è¨˜äº‹ãŒæç¤ºã•ã‚ŒãŸè³ªå•ã«ç­”ãˆã‚‹ã®ã«ååˆ†ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’ï¼ˆã»ã¼ï¼‰100%ã®ä¿¡é ¼åº¦ã§æ–­è¨€ã—ã¦ã„ã¾ã™ã€‚<br><br>\n",
    "ä¸€æ–¹ã€è¨˜äº‹å†…ã§ã‚ˆã‚Šæ˜ç¢ºã«ç­”ãˆã‚‰ã‚Œã¦ã„ãªã„ã€ã‚ˆã‚Šé›£ã—ã„è³ªå•ã«ã¤ã„ã¦ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯ååˆ†ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒã£ã¦ã„ã‚‹ã¨ã„ã†ä¿¡é ¼åº¦ãŒä½ããªã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒååˆ†ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã®å„ªã‚ŒãŸã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã§ã™ã€‚<br><br>\n",
    "ã“ã®è‡ªå·±è©•ä¾¡ã¯ã€`sufficient_context_for_answer`ã®å¯¾æ•°ç¢ºç‡ãŒç‰¹å®šã®é–¾å€¤ã‚’ä¸‹å›ã£ãŸå ´åˆã«å›ç­”ã‚’åˆ¶é™ã—ãŸã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿ƒã—ãŸã‚Šã™ã‚‹ã“ã¨ã§ã€ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¸›ã‚‰ã™ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã“ã®ã‚ˆã†ãªæ‰‹æ³•ã¯ã€Q&Aã«ãŠã‘ã‚‹RAGã®ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚„ã‚¨ãƒ©ãƒ¼ã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ï¼ˆ[ä¾‹](https://jfan001.medium.com/how-we-cut-the-rate-of-gpt-hallucinations-from-20-to-less-than-2-f3bfcc10e4ec)ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logprobs`ã®ã‚‚ã†ä¸€ã¤ã®ä½¿ç”¨ä¾‹ã¯ã€ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã‚’ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ä½œæˆã™ã‚‹ã“ã¨ãªãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ã¦ã„ã‚‹éš›ã«å˜èªã‚’ææ¡ˆã™ã‚‹æ–¹æ³•ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã«`logprobs`ãŒã©ã®ã‚ˆã†ã«å½¹ç«‹ã¤ã‹ã‚’å®Ÿæ¼”ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¾ãšã€ã‚µãƒ³ãƒ—ãƒ«æ–‡ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ï¼š`\"My least favorite TV show is Breaking Bad.\"`ã“ã®æ–‡ã‚’å…¥åŠ›ã—ã¦ã„ã‚‹éš›ã«ã€æ¬¡ã®å˜èªã‚„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‹•çš„ã«æ¨è–¦ã—ãŸã„ã¨ã—ã¾ã™ã€‚ãŸã ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒæ¬¡ã®å˜èªã«ã¤ã„ã¦*ã‹ãªã‚Šç¢ºä¿¡ã‚’æŒã£ã¦ã„ã‚‹*å ´åˆã®ã¿ã§ã™ã€‚ã“ã‚Œã‚’å®Ÿæ¼”ã™ã‚‹ãŸã‚ã«ã€æ–‡ã‚’é †æ¬¡çš„ãªæ§‹æˆè¦ç´ ã«åˆ†è§£ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [\n",
    "    \"My\",\n",
    "    \"My least\",\n",
    "    \"My least favorite\",\n",
    "    \"My least favorite TV\",\n",
    "    \"My least favorite TV show\",\n",
    "    \"My least favorite TV show is\",\n",
    "    \"My least favorite TV show is Breaking Bad\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§ã€`gpt-4o-mini`ã«å¯¾ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã‚ªãƒ¼ãƒˆã‚³ãƒ³ãƒ—ãƒªãƒ¼ãƒˆã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦å‹•ä½œã™ã‚‹ã‚ˆã†æŒ‡ç¤ºã§ãã¾ã™ã€‚`logprobs`ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒãã®äºˆæ¸¬ã«ã©ã®ç¨‹åº¦ç¢ºä¿¡ã‚’æŒã£ã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Sentence: My</p><p style=\"color:cyan\">Predicted next token: My, <span style=\"color:darkorange\">logprobs: -0.08344023, <span style=\"color:magenta\">linear probability: 91.99%</span></p><p style=\"color:cyan\">Predicted next token: dog, <span style=\"color:darkorange\">logprobs: -3.3334403, <span style=\"color:magenta\">linear probability: 3.57%</span></p><p style=\"color:cyan\">Predicted next token: ap, <span style=\"color:darkorange\">logprobs: -3.5834403, <span style=\"color:magenta\">linear probability: 2.78%</span></p><br><p>Sentence: My least</p><p style=\"color:cyan\">Predicted next token: My, <span style=\"color:darkorange\">logprobs: -0.1271426, <span style=\"color:magenta\">linear probability: 88.06%</span></p><p style=\"color:cyan\">Predicted next token: favorite, <span style=\"color:darkorange\">logprobs: -2.1271427, <span style=\"color:magenta\">linear probability: 11.92%</span></p><p style=\"color:cyan\">Predicted next token:  My, <span style=\"color:darkorange\">logprobs: -9.127143, <span style=\"color:magenta\">linear probability: 0.01%</span></p><br><p>Sentence: My least favorite</p><p style=\"color:cyan\">Predicted next token: My, <span style=\"color:darkorange\">logprobs: -0.052905332, <span style=\"color:magenta\">linear probability: 94.85%</span></p><p style=\"color:cyan\">Predicted next token: food, <span style=\"color:darkorange\">logprobs: -4.0529056, <span style=\"color:magenta\">linear probability: 1.74%</span></p><p style=\"color:cyan\">Predicted next token: color, <span style=\"color:darkorange\">logprobs: -5.0529056, <span style=\"color:magenta\">linear probability: 0.64%</span></p><br><p>Sentence: My least favorite TV</p><p style=\"color:cyan\">Predicted next token: show, <span style=\"color:darkorange\">logprobs: -0.57662326, <span style=\"color:magenta\">linear probability: 56.18%</span></p><p style=\"color:cyan\">Predicted next token: My, <span style=\"color:darkorange\">logprobs: -0.82662326, <span style=\"color:magenta\">linear probability: 43.75%</span></p><p style=\"color:cyan\">Predicted next token:  show, <span style=\"color:darkorange\">logprobs: -8.201623, <span style=\"color:magenta\">linear probability: 0.03%</span></p><br><p>Sentence: My least favorite TV show</p><p style=\"color:cyan\">Predicted next token: is, <span style=\"color:darkorange\">logprobs: -0.70817715, <span style=\"color:magenta\">linear probability: 49.25%</span></p><p style=\"color:cyan\">Predicted next token: My, <span style=\"color:darkorange\">logprobs: -0.70817715, <span style=\"color:magenta\">linear probability: 49.25%</span></p><p style=\"color:cyan\">Predicted next token: was, <span style=\"color:darkorange\">logprobs: -4.833177, <span style=\"color:magenta\">linear probability: 0.8%</span></p><br><p>Sentence: My least favorite TV show is</p><p style=\"color:cyan\">Predicted next token: My, <span style=\"color:darkorange\">logprobs: -0.47896808, <span style=\"color:magenta\">linear probability: 61.94%</span></p><p style=\"color:cyan\">Predicted next token: one, <span style=\"color:darkorange\">logprobs: -1.7289681, <span style=\"color:magenta\">linear probability: 17.75%</span></p><p style=\"color:cyan\">Predicted next token: the, <span style=\"color:darkorange\">logprobs: -2.9789681, <span style=\"color:magenta\">linear probability: 5.08%</span></p><br><p>Sentence: My least favorite TV show is Breaking Bad</p><p style=\"color:cyan\">Predicted next token: because, <span style=\"color:darkorange\">logprobs: -0.034502674, <span style=\"color:magenta\">linear probability: 96.61%</span></p><p style=\"color:cyan\">Predicted next token: ,, <span style=\"color:darkorange\">logprobs: -3.7845027, <span style=\"color:magenta\">linear probability: 2.27%</span></p><p style=\"color:cyan\">Predicted next token:  because, <span style=\"color:darkorange\">logprobs: -5.0345025, <span style=\"color:magenta\">linear probability: 0.65%</span></p><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "high_prob_completions = {}\n",
    "low_prob_completions = {}\n",
    "html_output = \"\"\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    PROMPT = \"\"\"Complete this sentence. You are acting as auto-complete. Simply complete the sentence to the best of your ability, make sure it is just ONE sentence: {sentence}\"\"\"\n",
    "    API_RESPONSE = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": PROMPT.format(sentence=sentence)}],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        logprobs=True,\n",
    "        top_logprobs=3,\n",
    "    )\n",
    "    html_output += f'<p>Sentence: {sentence}</p>'\n",
    "    first_token = True\n",
    "    for token in API_RESPONSE.choices[0].logprobs.content[0].top_logprobs:\n",
    "        html_output += f'<p style=\"color:cyan\">Predicted next token: {token.token}, <span style=\"color:darkorange\">logprobs: {token.logprob}, <span style=\"color:magenta\">linear probability: {np.round(np.exp(token.logprob)*100,2)}%</span></p>'\n",
    "        if first_token:\n",
    "            if np.exp(token.logprob) > 0.95:\n",
    "                high_prob_completions[sentence] = token.token\n",
    "            if np.exp(token.logprob) < 0.60:\n",
    "                low_prob_completions[sentence] = token.token\n",
    "        first_token = False\n",
    "    html_output += \"<br>\"\n",
    "\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é«˜ä¿¡é ¼åº¦ã®è‡ªå‹•è£œå®Œã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'My least favorite TV show is Breaking Bad': 'because'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_prob_completions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã‚‰ã¯å¦¥å½“ã«è¦‹ãˆã¾ã™ï¼ã“ã‚Œã‚‰ã®ææ¡ˆã«è‡ªä¿¡ã‚’æŒã¤ã“ã¨ãŒã§ãã¾ã™ã€‚ã€ŒMy least favorite TVã€ã¨æ›¸ã„ãŸå¾Œã«ã€Œshowã€ã¨æ›¸ããŸããªã‚‹å¯èƒ½æ€§ã¯éå¸¸ã«é«˜ã„ã§ã—ã‚‡ã†ï¼ã§ã¯æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚ã¾ã‚Šç¢ºä¿¡ã‚’æŒã¦ãªã‹ã£ãŸè‡ªå‹•è£œå®Œã®ææ¡ˆã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'My least favorite TV': 'show', 'My least favorite TV show': 'is'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_prob_completions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã‚‰ã‚‚è«–ç†çš„ã§ã™ã€‚ã€Œmy least favoriteã€ã¨ã„ã†æ¥é ­è¾ã ã‘ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä½•ã‚’è¨€ãŠã†ã¨ã—ã¦ã„ã‚‹ã®ã‹ã‹ãªã‚Šä¸æ˜ç¢ºã§ã™ã—ã€è‘—è€…ã®ãŠæ°—ã«å…¥ã‚Šã®ãƒ†ãƒ¬ãƒ“ç•ªçµ„ãŒä½•ãªã®ã‹ã¯æœ¬å½“ã«èª°ã«ã‚‚åˆ†ã‹ã‚Šã¾ã›ã‚“ã€‚<br><br>\n",
    "ãã“ã§ã€`gpt-4o-mini`ã‚’ä½¿ç”¨ã—ã¦ã€`logprobs`ã§å‹•çš„è‡ªå‹•è£œå®Œã‚¨ãƒ³ã‚¸ãƒ³ã®åŸºç›¤ã‚’ä½œã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ãƒã‚¤ãƒ©ã‚¤ã‚¿ãƒ¼ã¨bytesãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logprobs`ã‚’ä½¿ç”¨ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¤ãƒ©ã‚¤ã‚¿ãƒ¼ã®ä½œæˆã¨ã€bytesãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä½¿ç”¨ã«ã¤ã„ã¦ç°¡å˜ã«èª¬æ˜ã—ã¾ã—ã‚‡ã†ã€‚ã¾ãšã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¦ãƒã‚¤ãƒ©ã‚¤ãƒˆã™ã‚‹é–¢æ•°ã‚’ä½œæˆã§ãã¾ã™ã€‚ã“ã‚Œã¯å¯¾æ•°ç¢ºç‡ã‚’ä½¿ç”¨ã—ã¾ã›ã‚“ãŒã€`logprobs`ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã§åˆ©ç”¨ã§ãã‚‹çµ„ã¿è¾¼ã¿ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"What's the longest word in the English language?\"\"\"\n",
    "\n",
    "API_RESPONSE = get_completion(\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}], model=\"gpt-4o\", logprobs=True, top_logprobs=5\n",
    ")\n",
    "\n",
    "\n",
    "def highlight_text(api_response):\n",
    "    colors = [\n",
    "        \"#FF00FF\",  # Magenta\n",
    "        \"#008000\",  # Green\n",
    "        \"#FF8C00\",  # Dark Orange\n",
    "        \"#FF0000\",  # Red\n",
    "        \"#0000FF\",  # Blue\n",
    "    ]\n",
    "    tokens = api_response.choices[0].logprobs.content\n",
    "\n",
    "    color_idx = 0  # Initialize color index\n",
    "    html_output = \"\"  # Initialize HTML output\n",
    "    for t in tokens:\n",
    "        token_str = bytes(t.bytes).decode(\"utf-8\")  # Decode bytes to string\n",
    "\n",
    "        # Add colored token to HTML output\n",
    "        html_output += f\"<span style='color: {colors[color_idx]}'>{token_str}</span>\"\n",
    "\n",
    "        # Move to the next color\n",
    "        color_idx = (color_idx + 1) % len(colors)\n",
    "    display(HTML(html_output))  # Display HTML output\n",
    "    print(f\"Total number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color: #FF00FF'>The</span><span style='color: #008000'> longest</span><span style='color: #FF8C00'> word</span><span style='color: #FF0000'> in</span><span style='color: #0000FF'> the</span><span style='color: #FF00FF'> English</span><span style='color: #008000'> language</span><span style='color: #FF8C00'> is</span><span style='color: #FF0000'> often</span><span style='color: #0000FF'> considered</span><span style='color: #FF00FF'> to</span><span style='color: #008000'> be</span><span style='color: #FF8C00'> \"</span><span style='color: #FF0000'>p</span><span style='color: #0000FF'>ne</span><span style='color: #FF00FF'>um</span><span style='color: #008000'>on</span><span style='color: #FF8C00'>oul</span><span style='color: #FF0000'>tr</span><span style='color: #0000FF'>amic</span><span style='color: #FF00FF'>ros</span><span style='color: #008000'>cop</span><span style='color: #FF8C00'>ics</span><span style='color: #FF0000'>ilic</span><span style='color: #0000FF'>ovol</span><span style='color: #FF00FF'>can</span><span style='color: #008000'>ocon</span><span style='color: #FF8C00'>iosis</span><span style='color: #FF0000'>,\"</span><span style='color: #0000FF'> a</span><span style='color: #FF00FF'> term</span><span style='color: #008000'> referring</span><span style='color: #FF8C00'> to</span><span style='color: #FF0000'> a</span><span style='color: #0000FF'> type</span><span style='color: #FF00FF'> of</span><span style='color: #008000'> lung</span><span style='color: #FF8C00'> disease</span><span style='color: #FF0000'> caused</span><span style='color: #0000FF'> by</span><span style='color: #FF00FF'> inhal</span><span style='color: #008000'>ing</span><span style='color: #FF8C00'> very</span><span style='color: #FF0000'> fine</span><span style='color: #0000FF'> sil</span><span style='color: #FF00FF'>icate</span><span style='color: #008000'> or</span><span style='color: #FF8C00'> quartz</span><span style='color: #FF0000'> dust</span><span style='color: #0000FF'>.</span><span style='color: #FF00FF'> However</span><span style='color: #008000'>,</span><span style='color: #FF8C00'> it's</span><span style='color: #FF0000'> worth</span><span style='color: #0000FF'> noting</span><span style='color: #FF00FF'> that</span><span style='color: #008000'> this</span><span style='color: #FF8C00'> word</span><span style='color: #FF0000'> was</span><span style='color: #0000FF'> coined</span><span style='color: #FF00FF'> more</span><span style='color: #008000'> for</span><span style='color: #FF8C00'> its</span><span style='color: #FF0000'> length</span><span style='color: #0000FF'> than</span><span style='color: #FF00FF'> for</span><span style='color: #008000'> practical</span><span style='color: #FF8C00'> use</span><span style='color: #FF0000'>.</span><span style='color: #0000FF'> There</span><span style='color: #FF00FF'> are</span><span style='color: #008000'> also</span><span style='color: #FF8C00'> chemical</span><span style='color: #FF0000'> names</span><span style='color: #0000FF'> for</span><span style='color: #FF00FF'> proteins</span><span style='color: #008000'> and</span><span style='color: #FF8C00'> other</span><span style='color: #FF0000'> compounds</span><span style='color: #0000FF'> that</span><span style='color: #FF00FF'> can</span><span style='color: #008000'> be</span><span style='color: #FF8C00'> much</span><span style='color: #FF0000'> longer</span><span style='color: #0000FF'>,</span><span style='color: #FF00FF'> but</span><span style='color: #008000'> they</span><span style='color: #FF8C00'> are</span><span style='color: #FF0000'> typically</span><span style='color: #0000FF'> not</span><span style='color: #FF00FF'> used</span><span style='color: #008000'> in</span><span style='color: #FF8C00'> everyday</span><span style='color: #FF0000'> language</span><span style='color: #0000FF'>.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 95\n"
     ]
    }
   ],
   "source": [
    "highlight_text(API_RESPONSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€bytesãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦æ–‡ã‚’å†æ§‹ç¯‰ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚`logprobs`ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—ã®ASCIIï¼ˆ10é€²æ•°ã®utf-8ï¼‰å€¤ã®ä¸¡æ–¹ãŒæä¾›ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã‚‰ã®ASCIIå€¤ã¯ã€çµµæ–‡å­—ã‚„ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€ãƒˆãƒ¼ã‚¯ãƒ³ã‚„ã€ãã‚Œã‚‰ã§æ§‹æˆã•ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†ã™ã‚‹éš›ã«å½¹ç«‹ã¡ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Here\n",
      "Log prob: -0.054242473\n",
      "Linear prob: 94.72 %\n",
      "Bytes: [72, 101, 114, 101] \n",
      "\n",
      "Token:  is\n",
      "Log prob: -0.0044352207\n",
      "Linear prob: 99.56 %\n",
      "Bytes: [32, 105, 115] \n",
      "\n",
      "Token:  the\n",
      "Log prob: -2.1008714e-06\n",
      "Linear prob: 100.0 %\n",
      "Bytes: [32, 116, 104, 101] \n",
      "\n",
      "Token:  blue\n",
      "Log prob: -0.0013290489\n",
      "Linear prob: 99.87 %\n",
      "Bytes: [32, 98, 108, 117, 101] \n",
      "\n",
      "Token:  heart\n",
      "Log prob: 0.0\n",
      "Linear prob: 100.0 %\n",
      "Bytes: [32, 104, 101, 97, 114, 116] \n",
      "\n",
      "Token:  emoji\n",
      "Log prob: 0.0\n",
      "Linear prob: 100.0 %\n",
      "Bytes: [32, 101, 109, 111, 106, 105] \n",
      "\n",
      "Token:  and\n",
      "Log prob: -0.038287632\n",
      "Linear prob: 96.24 %\n",
      "Bytes: [32, 97, 110, 100] \n",
      "\n",
      "Token:  its\n",
      "Log prob: 0.0\n",
      "Linear prob: 100.0 %\n",
      "Bytes: [32, 105, 116, 115] \n",
      "\n",
      "Token:  name\n",
      "Log prob: -1.569009e-05\n",
      "Linear prob: 100.0 %\n",
      "Bytes: [32, 110, 97, 109, 101] \n",
      "\n",
      "Token: :\n",
      "\n",
      "\n",
      "Log prob: -0.11313002\n",
      "Linear prob: 89.3 %\n",
      "Bytes: [58, 10, 10] \n",
      "\n",
      "Token: \\xf0\\x9f\\x92\n",
      "Log prob: -0.09048584\n",
      "Linear prob: 91.35 %\n",
      "Bytes: [240, 159, 146] \n",
      "\n",
      "Token: \\x99\n",
      "Log prob: 0.0\n",
      "Linear prob: 100.0 %\n",
      "Bytes: [153] \n",
      "\n",
      "Token:  Blue\n",
      "Log prob: -0.023958502\n",
      "Linear prob: 97.63 %\n",
      "Bytes: [32, 66, 108, 117, 101] \n",
      "\n",
      "Token:  Heart\n",
      "Log prob: -6.2729996e-06\n",
      "Linear prob: 100.0 %\n",
      "Bytes: [32, 72, 101, 97, 114, 116] \n",
      "\n",
      "Bytes array: [72, 101, 114, 101, 32, 105, 115, 32, 116, 104, 101, 32, 98, 108, 117, 101, 32, 104, 101, 97, 114, 116, 32, 101, 109, 111, 106, 105, 32, 97, 110, 100, 32, 105, 116, 115, 32, 110, 97, 109, 101, 58, 10, 10, 240, 159, 146, 153, 32, 66, 108, 117, 101, 32, 72, 101, 97, 114, 116]\n",
      "Decoded bytes: Here is the blue heart emoji and its name:\n",
      "\n",
      "ğŸ’™ Blue Heart\n",
      "Joint prob: 72.19 %\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"Output the blue heart emoji and its name.\"\"\"\n",
    "API_RESPONSE = get_completion(\n",
    "    [{\"role\": \"user\", \"content\": PROMPT}], model=\"gpt-4o\", logprobs=True\n",
    ")\n",
    "\n",
    "aggregated_bytes = []\n",
    "joint_logprob = 0.0\n",
    "\n",
    "# Iterate over tokens, aggregate bytes and calculate joint logprob\n",
    "for token in API_RESPONSE.choices[0].logprobs.content:\n",
    "    print(\"Token:\", token.token)\n",
    "    print(\"Log prob:\", token.logprob)\n",
    "    print(\"Linear prob:\", np.round(exp(token.logprob) * 100, 2), \"%\")\n",
    "    print(\"Bytes:\", token.bytes, \"\\n\")\n",
    "    aggregated_bytes += token.bytes\n",
    "    joint_logprob += token.logprob\n",
    "\n",
    "# Decode the aggregated bytes to text\n",
    "aggregated_text = bytes(aggregated_bytes).decode(\"utf-8\")\n",
    "\n",
    "# Assert that the decoded text is the same as the message content\n",
    "assert API_RESPONSE.choices[0].message.content == aggregated_text\n",
    "\n",
    "# Print the results\n",
    "print(\"Bytes array:\", aggregated_bytes)\n",
    "print(f\"Decoded bytes: {aggregated_text}\")\n",
    "print(\"Joint prob:\", np.round(exp(joint_logprob) * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã“ã§ã¯ã€æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒ `\\xf0\\x9f\\x92'` ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ãŒã€ãã®ASCIIå€¤ã‚’å–å¾—ã—ã¦bytesé…åˆ—ã«è¿½åŠ ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãã®å¾Œã€ã“ã®é…åˆ—ã‚’å®Œå…¨ãªæ–‡ã«ç°¡å˜ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã€ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸbytesãŒå®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨åŒã˜ã§ã‚ã‚‹ã“ã¨ã‚’assertæ–‡ã§æ¤œè¨¼ã§ãã¾ã™ï¼\n",
    "\n",
    "ã•ã‚‰ã«ã€å®Œäº†å…¨ä½“ã®çµåˆç¢ºç‡ã‚’å–å¾—ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã“ã‚Œã¯å„ãƒˆãƒ¼ã‚¯ãƒ³ã®å¯¾æ•°ç¢ºç‡ã®æŒ‡æ•°ç©ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä¸ãˆã‚‰ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦ã“ã®ç‰¹å®šã®å®Œäº†ãŒã©ã®ç¨‹åº¦`å¯èƒ½æ€§ãŒé«˜ã„`ã‹ãŒã‚ã‹ã‚Šã¾ã™ã€‚ç§ãŸã¡ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯éå¸¸ã«æŒ‡ç¤ºçš„ï¼ˆç‰¹å®šã®çµµæ–‡å­—ã¨ãã®åå‰ã‚’æ±‚ã‚ã¦ã„ã‚‹ï¼‰ã§ã‚ã‚‹ãŸã‚ã€ã“ã®å‡ºåŠ›ã®çµåˆç¢ºç‡ã¯é«˜ããªã‚Šã¾ã™ï¼ã—ã‹ã—ã€ãƒ©ãƒ³ãƒ€ãƒ ãªå‡ºåŠ›ã‚’æ±‚ã‚ãŸå ´åˆã€çµåˆç¢ºç‡ã¯ã¯ã‚‹ã‹ã«ä½ããªã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¸­ã®é–‹ç™ºè€…ã«ã¨ã£ã¦ã‚‚è‰¯ã„æˆ¦è¡“ã¨ãªã‚Šå¾—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã®è¨ˆç®—\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã®çµæœã«å¯¾ã™ã‚‹ä¿¡é ¼åº¦ã‚’è©•ä¾¡ã™ã‚‹éš›ã€ä¸ç¢ºå®Ÿæ€§ã®æŒ‡æ¨™ã§ã‚ã‚‹ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ãŒæœ‰ç”¨ã§ã™ã€‚ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã¯ã€logprobsã®å¹³å‡å€¤ã®è² ã®å€¤ã‚’æŒ‡æ•°åŒ–ã™ã‚‹ã“ã¨ã§è¨ˆç®—ã§ãã¾ã™ã€‚ä¸€èˆ¬çš„ã«ã€ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ãŒé«˜ã„ã»ã©ä¸ç¢ºå®Ÿãªçµæœã‚’ç¤ºã—ã€ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ãŒä½ã„ã»ã©ä¿¡é ¼åº¦ã®é«˜ã„çµæœã‚’ç¤ºã—ã¾ã™ã€‚ãã®ãŸã‚ã€ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã¯å€‹ã€…ã®ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œçµæœã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã ã‘ã§ãªãã€è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œçµæœé–“ã®ç›¸å¯¾çš„ãªä¿¡é ¼åº¦ã‚’æ¯”è¼ƒã™ã‚‹ãŸã‚ã«ã‚‚ä½¿ç”¨ã§ãã¾ã™ã€‚é«˜ã„ä¿¡é ¼åº¦ãŒçµæœã®æ­£ç¢ºæ€§ã‚’ä¿è¨¼ã™ã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ä»–ã®è©•ä¾¡æŒ‡æ¨™ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å‹•ä½œã‚’ã‚ˆã‚Šæ·±ãç†è§£ã™ã‚‹ãŸã‚ã®æœ‰ç”¨ãªã‚·ã‚°ãƒŠãƒ«ã¨ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "ä¾‹ãˆã°ã€`gpt-4o-mini`ã‚’ä½¿ç”¨ã—ã¦äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦ã‚ˆã‚Šè©³ã—ãå­¦ã³ãŸã„ã¨ã—ã¾ã—ã‚‡ã†ã€‚æœ€è¿‘ã®æ­´å²ã«ã¤ã„ã¦è³ªå•ã™ã‚‹ã“ã¨ã‚‚ã§ãã‚Œã°ã€æœªæ¥ã«ã¤ã„ã¦è³ªå•ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:     In a short sentence, has artifical intelligence grown in the last decade?\n",
      "Response:   Yes, artificial intelligence has grown significantly in the last decade, advancing in capabilities and applications across various fields. \n",
      "\n",
      "Tokens:                Yes              ,     artificial   intelligence            has          grown  significantly             in            the           last         decade              ,      advancing             in   capabilities            and   applications         across        various         fields              .\n",
      "Logprobs:            -0.00           0.00          -0.00           0.00          -0.00          -0.73          -0.00          -0.01          -0.02          -0.00           0.00          -0.02          -0.66          -0.03          -0.62          -0.47          -0.02          -0.39          -0.01          -0.20          -0.00\n",
      "Perplexity: 1.1644170003987546 \n",
      "\n",
      "Prompt:     In a short sentence, what are your thoughts on the future of artificial intelligence?\n",
      "Response:   The future of artificial intelligence holds immense potential for transformative advancements across various sectors, but it also requires careful consideration of ethical and societal impacts. \n",
      "\n",
      "Tokens:                 The          future              of      artificial    intelligence           holds         immense       potential             for  transformative    advancements          across         various         sectors               ,             but              it            also        requires         careful   consideration              of         ethical             and        societal         impacts               .\n",
      "Logprobs:             -0.02           -0.00            0.00           -0.00            0.00           -0.05           -0.35           -0.01           -0.02           -0.64           -0.43           -0.25           -0.16           -0.51           -0.02           -0.43           -0.08           -0.07           -0.97           -0.02           -0.48           -0.00           -0.00           -0.48           -0.01           -0.58           -0.00\n",
      "Perplexity: 1.2292170270768858 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"In a short sentence, has artifical intelligence grown in the last decade?\",\n",
    "    \"In a short sentence, what are your thoughts on the future of artificial intelligence?\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    API_RESPONSE = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        logprobs=True,\n",
    "    )\n",
    "\n",
    "    logprobs = [token.logprob for token in API_RESPONSE.choices[0].logprobs.content]\n",
    "    response_text = API_RESPONSE.choices[0].message.content\n",
    "    response_text_tokens = [token.token for token in API_RESPONSE.choices[0].logprobs.content]\n",
    "    max_starter_length = max(len(s) for s in [\"Prompt:\", \"Response:\", \"Tokens:\", \"Logprobs:\", \"Perplexity:\"])\n",
    "    max_token_length = max(len(s) for s in response_text_tokens)\n",
    "    \n",
    "\n",
    "    formatted_response_tokens = [s.rjust(max_token_length) for s in response_text_tokens]\n",
    "    formatted_lps = [f\"{lp:.2f}\".rjust(max_token_length) for lp in logprobs]\n",
    "\n",
    "    perplexity_score = np.exp(-np.mean(logprobs))\n",
    "    print(\"Prompt:\".ljust(max_starter_length), prompt)\n",
    "    print(\"Response:\".ljust(max_starter_length), response_text, \"\\n\")\n",
    "    print(\"Tokens:\".ljust(max_starter_length), \" \".join(formatted_response_tokens))\n",
    "    print(\"Logprobs:\".ljust(max_starter_length), \" \".join(formatted_lps))\n",
    "    print(\"Perplexity:\".ljust(max_starter_length), perplexity_score, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã®ä¾‹ã§ã¯ã€`gpt-4o-mini`ã¯æœ€è¿‘ã®æ­´å²ã«é–¢ã™ã‚‹ã‚ˆã‚Šæ±ºå®šè«–çš„ãªè³ªå•ã«å¯¾ã—ã¦ã‚ˆã‚Šä½ã„perplexityã‚¹ã‚³ã‚¢ã‚’è¿”ã—ã€è¿‘ã„å°†æ¥ã«é–¢ã™ã‚‹ã‚ˆã‚Šæ¨æ¸¬çš„ãªè©•ä¾¡ã«å¯¾ã—ã¦ã¯ã‚ˆã‚Šé«˜ã„perplexityã‚¹ã‚³ã‚¢ã‚’è¿”ã—ã¾ã—ãŸã€‚ç¹°ã‚Šè¿”ã—ã«ãªã‚Šã¾ã™ãŒã€ã“ã‚Œã‚‰ã®é•ã„ã¯æ­£ç¢ºæ€§ã‚’ä¿è¨¼ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒ¢ãƒ‡ãƒ«ã®çµæœã®è§£é‡ˆã¨ä»Šå¾Œã®æ´»ç”¨æ–¹æ³•ã‚’ç¤ºã™æ‰‹ãŒã‹ã‚Šã¨ãªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. çµè«–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç´ æ™´ã‚‰ã—ã„ï¼`logprobs`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€ã‚ˆã‚Šå …ç‰¢ãªåˆ†é¡å™¨ã‚’æ§‹ç¯‰ã—ã€Q&Aã‚·ã‚¹ãƒ†ãƒ ã®æ¤œç´¢ã‚’è©•ä¾¡ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®å„ã€Œãƒã‚¤ãƒˆã€ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãŠã‚ˆã³ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸï¼`logprobs`ã¯å®Œäº†å‡ºåŠ›ã«æœ‰ç”¨ãªæƒ…å ±ã¨ã‚·ã‚°ãƒŠãƒ«ã‚’è¿½åŠ ã—ã€é–‹ç™ºè€…ãŒã“ã‚Œã‚’ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æ”¹å–„ã«ã©ã®ã‚ˆã†ã«çµ„ã¿è¾¼ã‚€ã‹ã‚’è¦‹ã‚‹ã®ãŒæ¥½ã—ã¿ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. å¯èƒ½ãªæ‹¡å¼µæ©Ÿèƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logprobs`ã«ã¯ã€ã“ã®ã‚¯ãƒƒã‚¯ãƒ–ãƒƒã‚¯ã§ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„ä»–ã®å¤šãã®ä½¿ç”¨ä¾‹ãŒã‚ã‚Šã¾ã™ã€‚`logprobs`ã¯ä»¥ä¸‹ã®ç”¨é€”ã«ä½¿ç”¨ã§ãã¾ã™ï¼š\n",
    "  - ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "  - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸æŠ\n",
    "  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ”¹å–„ã¨å‡ºåŠ›ã®è§£é‡ˆå¯èƒ½æ€§å‘ä¸Š\n",
    "  - ãƒˆãƒ¼ã‚¯ãƒ³ãƒ’ãƒ¼ãƒªãƒ³ã‚°\n",
    "  - ãã®ä»–å¤šæ•°ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
