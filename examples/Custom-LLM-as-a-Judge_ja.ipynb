{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Braintrustを使用してハルシネーションを検出するLLM-as-a-judge評価の構築\n",
    "\n",
    "カスタマーサービスボットに取り組んでおり、その応答の品質を評価しようとしているとしましょう。「返品ポリシーは何ですか？」のような質問を考えてみてください。正しい答えが「購入から30日以内に商品を返品できます」であるのに、あなたのボットが「30日以内に商品を返品できます」と生成した場合、これが良い応答かどうかをどのように評価しますか？\n",
    "\n",
    "`Levenshtein`文字列距離のようなヒューリスティックでは、この応答は不正確であることを示すでしょう。しかし、より良いアプローチは、LLM-as-a-judgeを使用して応答の正確性を評価することです。LLM-as-a-judgeは、LLMを活用して回答の品質をスコア化する技術です。LLMは表面的な文字列比較を超えて言語について推論できるため、より正確に回答を評価することができます。\n",
    "\n",
    "このクックブックでは、OpenAIのモデルと互換性のあるサードパーティ評価プラットフォームである[Braintrust](https://www.braintrust.dev/)を使用して、ハルシネーションを検出できるLLM-as-a-judgeスコアラーを構築する方法を説明します。\n",
    "\n",
    "## 依存関係のインストール\n",
    "\n",
    "いくつかの基本的な依存関係をインストールしましょう。CoQAデータセット（DuckDB経由）、評価用の[Braintrust](https://www.braintrust.dev/)、および[OpenAIのモデル](https://platform.openai.com/docs/models)を使用します。Braintrustはサードパーティの評価プラットフォームであることにご注意ください。続行する前に、彼らの[利用規約とプライバシーポリシー](https://www.braintrust.dev/legal/terms-of-service)を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install autoevals duckdb braintrust openai --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、OpenAIクライアントを初期化しましょう。リクエストを並列化できるように`AsyncOpenAI`クライアントを使用します。`braintrust.wrap_openai`関数は、LLM呼び出しを[Braintrust](https://www.braintrust.dev/)にログ記録できるようにOpenAIクライアントをラップします。以下の評価を促進するためにBraintrustを使用します。\n",
    "続行する前に、[Braintrustアカウント](https://www.braintrust.dev/signup)にサインアップし、環境変数`BRAINTRUST_API_KEY`に有効なAPIキーを設定してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import braintrust\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "braintrust.login(api_key=os.environ[\"BRAINTRUST_API_KEY\"])\n",
    "client = braintrust.wrap_openai(AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットを探索する\n",
    "\n",
    "多様な文章、質問、回答を含む[CoQAデータセット](https://stanfordnlp.github.io/coqa/)を使用します。CoQAは非常に大きなデータセットなので、最初のいくつかの文章のみを見ていきます。どの公開データセットでも同様ですが、基盤となるLLMがデータセットの一部を記憶している可能性があるため、独自のスコアラーを開発する際は、自分のプライベートデータを使用してテストすることをお勧めします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:\n",
      "(CNN)A chiseled boxer's Instagram feed shows him making constant references to the Bible and enjoying gospel singing with his wife. \n",
      "\n",
      "Another features his formidable opponent counting stacks of money, hanging out in strip clubs, and flashing diamond watches and Ferraris. \n",
      "\n",
      "Welcome to the world of boxing promotion, circa 2015. \n",
      "\n",
      "American Floyd Mayweather and Filipino Manny Pacquiao are set to officially announce their heavily anticipated boxing match at a press conference in Los Angeles Wednesday. \n",
      "\n",
      "With the combined purse for the May 2 bout in Las Vegas reported to touch $300 million pending viewership numbers, the incentives to self-promote could not be higher. \n",
      "\n",
      "\"Nowadays you have to be on social media to launch the fight and to build hype,\" says boxing promoter Nisse Sauerland, CEO of Team Sauerland. \"It couldn't be done without it.\" \n",
      "\n",
      "Thirty-eight year old Mayweather (47-0, 26 knockouts), who favors the moniker \"The Money Man\" or \"TBE\" (The Best Ever), boasts nearly five million Instagram followers, 5.65 million followers on Twitter and 9.2 million Facebook likes. \n",
      "\n",
      "He famously confirmed the fight via Shots, a photo sharing social media application that he's invested in, and displays links to his clothing brand, The Money Team, on all his accounts. \n",
      "\n",
      "Along with professing to the be the best fighter of all time, he could also stake a claim to be one of the greatest social media users in sports. \n",
      "\n",
      "\"I think they're both playing their roles,\" says Sauerland, who promotes over 45 boxers. \"You've got the bad guy and the good guy, really. You've got the guy who throws the money around (Mayweather), that's his image, and Pacquiao, he's the hope of a nation.\" \n",
      "\n",
      "Question:\n",
      "Who are the two boxer featured in this article?\n",
      "\n",
      "Answer:\n",
      "Floyd Mayweather and Manny Pacquiao\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# DuckDB has an easy wrapper for loading datasets from Hugging Face.\n",
    "con = duckdb.connect(\":memory:\")\n",
    "full_result = con.query(\"\"\"\n",
    "    SELECT * FROM 'hf://datasets/stanfordnlp/coqa/data/validation-00000-of-00001.parquet'\n",
    "        LIMIT 40\n",
    "\"\"\").fetchall()\n",
    "\n",
    "single_result = full_result[10]\n",
    "\n",
    "print(\"Passage:\")\n",
    "print(single_result[1])\n",
    "\n",
    "print(\"\\nQuestion:\")\n",
    "print(single_result[2][0])\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(single_result[3][\"input_text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データには一連の文章が含まれており、それぞれに複数の質問と回答があります。これを`(passage, question, answer)`タプルのリストに平坦化しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QuestionAnswer:\n",
    "    passage: str\n",
    "    question: str\n",
    "    expected_answer: str\n",
    "    generated_answer: str\n",
    "\n",
    "\n",
    "qa_pairs = [\n",
    "    QuestionAnswer(\n",
    "        passage=r[1],\n",
    "        question=question,\n",
    "        generated_answer=r[3][\"input_text\"][i],\n",
    "        expected_answer=r[3][\"input_text\"][i],\n",
    "    )\n",
    "    for r in full_result\n",
    "    for (i, question) in enumerate(r[2])\n",
    "]\n",
    "\n",
    "print(len(qa_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハルシネーションの追加\n",
    "\n",
    "Braintrustのスコアラーはハルシネーションをテストするように設計されているため、QAペアを使用して既知のハルシネーションを生成できます。各質問に対して、パッセージを使用せずにLLMに自信を持って答えを生成させることで、ハルシネーションされた回答を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:\n",
      "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
      "\n",
      "\"What are you doing, Cotton?!\" \n",
      "\n",
      "\"I only wanted to be more like you\". \n",
      "\n",
      "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
      "\n",
      "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
      "\n",
      "Then Cotton thought, \"I change my mind. I like being special\".\n",
      "\n",
      "Question:\n",
      "Where did she live?\n",
      "\n",
      "Expected Answer:\n",
      "in a barn\n",
      "\n",
      "Generated Answer:\n",
      "She lived in a quaint cottage on the edge of the Misty Hollow Forest, where elves and talking owls often hosted moonlit storytelling festivals.\n",
      "\n",
      "\n",
      "Number of hallucinations: 270\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "async def hallucinate_answer(qa):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\\\n",
    "You are a helpful hallucinating assistant, who makes up fake answers to questions.\n",
    "\n",
    "Answer the following question in 1 sentence. If you know the answer, then make up some fake\n",
    "superfluous details that are not in the passage you have memorized.\n",
    "\n",
    "Make sure to always answer it confidently, even if you don't know the answer. Do not use words\n",
    "like \"perhaps\", \"likely\", \"maybe\", etc. or punctuation like \"...\".Do not admit that you cannot\n",
    "or do not know the answer.\"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": qa.question},\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "hallucinated_answers = await asyncio.gather(\n",
    "    *[hallucinate_answer(qa) for qa in qa_pairs]\n",
    ")\n",
    "\n",
    "\n",
    "hallucinations = [\n",
    "    QuestionAnswer(\n",
    "        passage=qa.passage,\n",
    "        question=qa.question,\n",
    "        expected_answer=qa.expected_answer,\n",
    "        generated_answer=hallucination,\n",
    "    )\n",
    "    for (qa, hallucination) in zip(qa_pairs, hallucinated_answers)\n",
    "    # Exclude simple yes/no answers.\n",
    "    if \"yes\" not in hallucination.lower() and \"no\" not in hallucination.lower()\n",
    "]\n",
    "\n",
    "print(\"Passage:\")\n",
    "print(hallucinations[0].passage)\n",
    "print(\"\\nQuestion:\")\n",
    "print(hallucinations[0].question)\n",
    "print(\"\\nExpected Answer:\")\n",
    "print(hallucinations[0].expected_answer)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(hallucinations[0].generated_answer)\n",
    "\n",
    "print(\"\\n\\nNumber of hallucinations:\", len(hallucinations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価器の作成\n",
    "\n",
    "LLM-as-a-judgeを作成するためのいくつかの人気なアプローチを検討します。各アプローチについて、スコアラーを作成し、その後「メタ評価」を行ってパフォーマンスを確認します。\n",
    "幻覚的な回答が間違っていることがわかっているため、幻覚的な回答を`0`としてスコア付けする頻度をテストすることで評価器の品質を評価します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge #1: 数値評価者\n",
    "\n",
    "LLM-as-a-judgeを作成する際の一般的な最初の直感は、LLMに回答を1から5のスケールで評価してもらうことです。このアプローチの利点は、LLMの出力を数値スコアに変換するのが簡単なことです。\n",
    "\n",
    "[Factuality](https://github.com/braintrustdata/autoevals/blob/main/templates/factuality.yaml)テンプレートの修正版を使用しますが、LLMに回答を1から10のスケールで評価してもらいます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1.0\n",
      "What? On a hallucinated answer: \"What\" is a word often used to express inquiry, curiosity, or surprise, and it is said to have originated from the ancient city of Whatopia, where people would constantly ask questions while enchanted crows delivered cryptic messages.\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "PROMPT = \"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {input}\n",
    "************\n",
    "[Expert]: {expected}\n",
    "************\n",
    "[Submission]: {output}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
    "Rate the submission on a scale of 1 to 10.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def numeric_rater(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(input=input, output=output, expected=expected),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Rate the submission on a scale of 1 to 10.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"rating\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n",
    "                        },\n",
    "                        \"required\": [\"rating\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    return (arguments[\"rating\"] - 1) / 9\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        qa_pairs[10].question,\n",
    "        qa_pairs[10].generated_answer,\n",
    "        qa_pairs[10].expected_answer,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    hallucinations[10].question,\n",
    "    \"On a hallucinated answer:\",\n",
    "    hallucinations[10].generated_answer,\n",
    ")\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        hallucinations[10].question,\n",
    "        hallucinations[10].generated_answer,\n",
    "        hallucinations[10].expected_answer,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは有望に見えます！単一の例で健全性をチェックしたので、今度は適切な評価を実行して、より広いデータセットでのパフォーマンスを確認してみましょう。評価は3つのコンポーネントで構成されます：\n",
    "\n",
    "- **データ**: この場合、`input`は質問、幻覚的な回答、および正解となります。スコアラーはこれを0から1の間のスコアに変換します。これは幻覚なので、期待されるスコアは0です。\n",
    "- **タスク**: タスクは単純に各入力に対して数値評価器を呼び出すことです。\n",
    "- **スコア**: 生成されたスコアと正解スコアを比較することで、生成されたスコアの品質を評価します。両方の数値が0から1の間にあることがわかっているため、正規化された差分をスコアとして使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Numeric rater is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater\n",
      "LLM-as-a-judge [experiment_name=Numeric rater] (data): 270it [00:00, 54634.41it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eeb99e0ae3f46ea84a7f6ee41ee0928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM-as-a-judge [experiment_name=Numeric rater] (tasks):   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "95.35% 'normalized_diff' score\n",
      "\n",
      "201.60tok prompt_tokens\n",
      "5tok completion_tokens\n",
      "206.60tok total_tokens\n",
      "\n",
      "See results for Numeric rater at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "from braintrust import Eval\n",
    "\n",
    "\n",
    "def data():\n",
    "    for pair in hallucinations:\n",
    "        yield dict(\n",
    "            input=dict(asdict(pair)), expected=0, metadata=dict(hallucination=True)\n",
    "        )\n",
    "\n",
    "\n",
    "async def task(input):\n",
    "    return await numeric_rater(\n",
    "        input=input[\"question\"],\n",
    "        output=input[\"generated_answer\"],\n",
    "        expected=input[\"expected_answer\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def normalized_diff(output, expected):\n",
    "    return 1 - abs(output - expected)\n",
    "\n",
    "\n",
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Numeric rater\",\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数値評価者は全体で約94%のスコアを記録したようです。悪くはありませんが、評価の6%が誤って判定されている場合、それらを信頼するのは非常に困難になる可能性があります。何が起こっているのかを洞察するために、BraintrustのUIを詳しく調べてみましょう。\n",
    "\n",
    "![Partial credit](../images/Custom-LLM-as-a-Judge-Partial-Credit.gif)\n",
    "\n",
    "不正解の多くが1から10の間の数値でスコア付けされているようです。しかし、現在のところ、モデルがこれらのスコアを与えた理由についての洞察はありません。次にそれを修正できるかどうか見てみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge #2: 推論の追加\n",
    "\n",
    "LLMが評価について推論も行うようにプロンプトを調整してみましょう。この手法は[Chain of Thought Reasoning](https://en.wikipedia.org/wiki/Chain_of_thought_reasoning)と呼ばれます。スコアを改善する可能性があることに加えて、モデルがなぜこれらのスコアを付けたのかについての洞察を得ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1.0\n",
      "What? On a hallucinated answer: \"What\" is a word often used to express inquiry, curiosity, or surprise, and it is said to have originated from the ancient city of Whatopia, where people would constantly ask questions while enchanted crows delivered cryptic messages.\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "@braintrust.traced\n",
    "async def numeric_rater(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(input=input, output=output, expected=expected),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Rate the submission on a scale of 1 to 10.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"reasons\": {\n",
    "                                \"description\": \"Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\",\n",
    "                                \"title\": \"Reasoning\",\n",
    "                                \"type\": \"string\",\n",
    "                            },\n",
    "                            \"rating\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n",
    "                        },\n",
    "                        \"required\": [\"rating\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    return (arguments[\"rating\"] - 1) / 9\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        qa_pairs[10].question,\n",
    "        qa_pairs[10].generated_answer,\n",
    "        qa_pairs[10].expected_answer,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    hallucinations[10].question,\n",
    "    \"On a hallucinated answer:\",\n",
    "    hallucinations[10].generated_answer,\n",
    ")\n",
    "print(\n",
    "    await numeric_rater(\n",
    "        hallucinations[10].question,\n",
    "        hallucinations[10].generated_answer,\n",
    "        hallucinations[10].expected_answer,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Numeric rater with reasoning is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater%20with%20reasoning\n",
      "LLM-as-a-judge [experiment_name=Numeric rater with reasoning] (data): 270it [00:00, 111715.70it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1623ec8d55524e569700616c240818e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM-as-a-judge [experiment_name=Numeric rater with reasoning] (tasks):   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Numeric rater with reasoning compared to Numeric rater:\n",
      "92.10% (-03.25%) 'normalized_diff' score\t(5 improvements, 63 regressions)\n",
      "\n",
      "3.68s duration\n",
      "3.68s llm_duration\n",
      "239.60tok (+3800.00%) 'prompt_tokens'    \t(0 improvements, 270 regressions)\n",
      "136.82tok (+13182.22%) 'completion_tokens'\t(0 improvements, 270 regressions)\n",
      "376.43tok (+16982.22%) 'total_tokens'     \t(0 improvements, 270 regressions)\n",
      "0.00$ estimated_cost\n",
      "\n",
      "See results for Numeric rater with reasoning at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Numeric%20rater%20with%20reasoning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Numeric rater with reasoning\",\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論を追加してもスコアの向上には役立たなかったようです（実際には3%悪化しています）。しかし、失敗例の一つを見ることで、モデルが何を考えていたかについての洞察を得ることができます。以下は幻覚による回答の例です：\n",
    "\n",
    "![Output](../images/Custom-LLM-as-a-Judge-Output.png)\n",
    "\n",
    "そして、その推論とともにスコアが示されています：\n",
    "\n",
    "![Reasoning](../images/Custom-LLM-as-a-Judge-Reasoning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルが部分点を計算するために独自の判断を適用しているようです。これは数値評価における一般的な問題で、モデルと人間の両方に当てはまり、より良いプロンプトを使用することで解決できることがよくあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge #3: 評価の代わりに分類を行う\n",
    "\n",
    "次に、具体的な基準を明示し、それらの基準に従って回答を分類するようモデルに求めます。この手法により、テストしたい幻覚（ハルシネーション）に向けてモデルをより正確に導くことができます。直感的に、モデルに評価のための具体的な基準を与えることで、より正確なスコアが得られるでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the other cats do when Cotton emerged from the bucket of water? On a correct answer: licked her face\n",
      "1\n",
      "What? On a hallucinated answer: \"What\" is a word often used to express inquiry, curiosity, or surprise, and it is said to have originated from the ancient city of Whatopia, where people would constantly ask questions while enchanted crows delivered cryptic messages.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {input}\n",
    "************\n",
    "[Expert]: {expected}\n",
    "************\n",
    "[Submission]: {output}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
    "The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n",
    "(A) The submitted answer is a subset of the expert answer and is fully consistent with it.\n",
    "(B) The submitted answer is a superset of the expert answer and is fully consistent with it.\n",
    "(C) The submitted answer contains all the same details as the expert answer.\n",
    "(D) There is a disagreement between the submitted answer and the expert answer.\n",
    "(E) The answers differ, but these differences don't matter from the perspective of factuality.\n",
    "\n",
    "Answer the question by calling `select_choice` with your reasoning in a step-by-step matter to be\n",
    "sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Select a\n",
    "single choice by setting the `choice` parameter to a single choice from A, B, C, D, or E.\n",
    "\"\"\"\n",
    "\n",
    "# Since we're testing for hallucinations, penalize (B) as much as (D).\n",
    "CHOICE_SCORES = {\n",
    "    \"A\": 0.5,\n",
    "    \"B\": 0,\n",
    "    \"C\": 1,\n",
    "    \"D\": 0,\n",
    "    \"E\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "@braintrust.traced\n",
    "async def classifier(input, output, expected):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT.format(input=input, output=output, expected=expected),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"rate\",\n",
    "                    \"description\": \"Call this function to select a choice.\",\n",
    "                    \"parameters\": {\n",
    "                        \"properties\": {\n",
    "                            \"reasons\": {\n",
    "                                \"description\": \"Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.\",\n",
    "                                \"type\": \"string\",\n",
    "                            },\n",
    "                            \"choice\": {\n",
    "                                \"description\": \"The choice\",\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"A\", \"B\", \"C\", \"D\", \"E\"],\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"reasons\", \"choice\"],\n",
    "                        \"type\": \"object\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"rate\"}},\n",
    "    )\n",
    "    arguments = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    choice = arguments[\"choice\"]\n",
    "    return CHOICE_SCORES[choice] if choice in CHOICE_SCORES else None\n",
    "\n",
    "\n",
    "print(qa_pairs[10].question, \"On a correct answer:\", qa_pairs[10].generated_answer)\n",
    "print(\n",
    "    await classifier(\n",
    "        qa_pairs[10].question,\n",
    "        qa_pairs[10].generated_answer,\n",
    "        qa_pairs[10].expected_answer,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    hallucinations[10].question,\n",
    "    \"On a hallucinated answer:\",\n",
    "    hallucinations[10].generated_answer,\n",
    ")\n",
    "print(\n",
    "    await classifier(\n",
    "        hallucinations[10].question,\n",
    "        hallucinations[10].generated_answer,\n",
    "        hallucinations[10].expected_answer,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment Classifier is running at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Classifier\n",
      "LLM-as-a-judge [experiment_name=Classifier] (data): 270it [00:00, 84930.41it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb4cff5ff7646d59410ab7ae42b838b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM-as-a-judge [experiment_name=Classifier] (tasks):   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Classifier compared to Numeric rater with reasoning:\n",
      "98.15% (+06.05%) 'normalized_diff' score\t(86 improvements, 5 regressions)\n",
      "\n",
      "4.41s (+72.60%) 'duration'         \t(104 improvements, 165 regressions)\n",
      "4.40s (+72.59%) 'llm_duration'     \t(104 improvements, 165 regressions)\n",
      "418.60tok (+17900.00%) 'prompt_tokens'    \t(0 improvements, 270 regressions)\n",
      "164.91tok (+2809.26%) 'completion_tokens'\t(64 improvements, 204 regressions)\n",
      "583.52tok (+20709.26%) 'total_tokens'     \t(0 improvements, 270 regressions)\n",
      "0.00$ (+00.07%) 'estimated_cost'   \t(8 improvements, 255 regressions)\n",
      "\n",
      "See results for Classifier at https://www.braintrust.dev/app/braintrustdata.com/p/LLM-as-a-judge/experiments/Classifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def task(input):\n",
    "    return await classifier(\n",
    "        input=input[\"question\"],\n",
    "        output=input[\"generated_answer\"],\n",
    "        expected=input[\"expected_answer\"],\n",
    "    )\n",
    "\n",
    "\n",
    "await Eval(\n",
    "    \"LLM-as-a-judge\",\n",
    "    data=data,\n",
    "    task=task,\n",
    "    scores=[normalized_diff],\n",
    "    experiment_name=\"Classifier\",\n",
    "    max_concurrency=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分類器は98%のスコアを記録し、これは大幅な改善です！\n",
    "\n",
    "### このパターンの体系化\n",
    "\n",
    "上記の分類器は、以下のように簡潔に書き直すことができます：\n",
    "\n",
    "```python\n",
    "PROMPT = \"\"\"\\\n",
    "You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question]: {{input}}\n",
    "************\n",
    "[Expert]: {{expected}}\n",
    "************\n",
    "[Submission]: {{output}}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n",
    "The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n",
    "(A) The submitted answer is a subset of the expert answer and is fully consistent with it.\n",
    "(B) The submitted answer is a superset of the expert answer and is fully consistent with it.\n",
    "(C) The submitted answer contains all the same details as the expert answer.\n",
    "(D) There is a disagreement between the submitted answer and the expert answer.\n",
    "(E) The answers differ, but these differences don't matter from the perspective of factuality.\n",
    "\n",
    "Answer the question by calling `select_choice` with your reasoning in a step-by-step matter to be\n",
    "sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Select a\n",
    "single choice by setting the `choice` parameter to a single choice from A, B, C, D, or E.\n",
    "\"\"\"\n",
    "\n",
    "Classifier = autoevals.LLMClassifier(\n",
    "    name=\"Hallucination detector\",\n",
    "    prompt_template=PROMPT,\n",
    "    choice_scores={\"A\": 0.5, \"B\": 0, \"C\": 1, \"D\": 0, \"E\": 1},\n",
    "    use_cot=True,\n",
    ")\n",
    "```\n",
    "\n",
    "## 次のステップ\n",
    "\n",
    "次のステップとして、個々の改善点と後退点を詳しく調査して評価し、プロンプトの将来的な改善を検討することができます。また、独自のデータでテストし、結果があなたのユースケースに適用できることを再確認することもできます。\n",
    "o1のようなモデルを測定したり、より小さなモデルをファインチューニングして結果が再現可能かどうかを確認したり、few-shotプロンプティングを使用してより主観的な基準にモデルを合わせることもできます。\n",
    "いずれの場合も、各変更の影響を厳密に評価できるよう、結果を評価することに努めるべきです。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
