{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPTモデルへの入力をフォーマットする方法\n",
    "\n",
    "ChatGPTは、OpenAIの最も先進的なモデルである`gpt-3.5-turbo`と`gpt-4`によって動作しています。\n",
    "\n",
    "OpenAI APIを使用して、`gpt-3.5-turbo`または`gpt-4`で独自のアプリケーションを構築できます。\n",
    "\n",
    "チャットモデルは一連のメッセージを入力として受け取り、AIが生成したメッセージを出力として返します。\n",
    "\n",
    "このガイドでは、いくつかのAPI呼び出しの例を使ってチャット形式を説明します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. openaiライブラリをインポートする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the OpenAI Python library for calling the OpenAI API\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. チャット補完APIコールの例\n",
    "\n",
    "チャット補完APIコールのパラメータ：\n",
    "\n",
    "**必須**\n",
    "- `model`: 使用したいモデルの名前（例：`gpt-3.5-turbo`、`gpt-4`、`gpt-3.5-turbo-16k-1106`）\n",
    "- `messages`: メッセージオブジェクトのリスト。各オブジェクトには2つの必須フィールドがあります：\n",
    "    - `role`: メッセンジャーの役割（`system`、`user`、`assistant`、または`tool`のいずれか）\n",
    "    - `content`: メッセージの内容（例：`Write me a beautiful poem`）\n",
    "\n",
    "メッセージには、メッセンジャーに名前を付けるオプションの`name`フィールドを含めることもできます。例：`example-user`、`Alice`、`BlackbeardBot`。名前にはスペースを含めることはできません。\n",
    "\n",
    "**オプション**\n",
    "- `frequency_penalty`: 頻度に基づいてトークンにペナルティを課し、繰り返しを減らします。\n",
    "- `logit_bias`: 指定されたトークンの尤度をバイアス値で変更します。\n",
    "- `logprobs`: trueの場合、出力トークンの対数確率を返します。\n",
    "- `top_logprobs`: 各位置で返す最も可能性の高いトークンの数を指定します。\n",
    "- `max_tokens`: チャット補完で生成されるトークンの最大数を設定します。\n",
    "- `n`: 各入力に対して指定された数のチャット補完選択肢を生成します。\n",
    "- `presence_penalty`: テキスト内での存在に基づいて新しいトークンにペナルティを課します。\n",
    "- `response_format`: 出力形式を指定します（例：JSONモード）。\n",
    "- `seed`: 指定されたシードで決定論的サンプリングを保証します。\n",
    "- `stop`: APIがトークン生成を停止すべき最大4つのシーケンスを指定します。\n",
    "- `stream`: トークンが利用可能になると部分的なメッセージデルタを送信します。\n",
    "- `temperature`: 0から2の間のサンプリング温度を設定します。\n",
    "- `top_p`: nucleus samplingを使用し、top_p確率質量を持つトークンを考慮します。\n",
    "- `tools`: モデルが呼び出す可能性のある関数をリストします。\n",
    "- `tool_choice`: モデルの関数呼び出しを制御します（none/auto/function）。\n",
    "- `user`: エンドユーザーの監視と不正使用検出のための一意識別子。\n",
    "\n",
    "2024年1月時点では、GPTが関数に渡すJSONを生成できるかどうかを伝える`functions`のリストをオプションで送信することもできます。詳細については、[ドキュメント](https://platform.openai.com/docs/guides/function-calling)、[APIリファレンス](https://platform.openai.com/docs/api-reference/chat)、またはCookbookガイド[How to call functions with chat models](How_to_call_functions_with_chat_models.ipynb)を参照してください。\n",
    "\n",
    "通常、会話はアシスタントの動作を指示するシステムメッセージから始まり、その後にユーザーとアシスタントのメッセージが交互に続きますが、この形式に従う必要はありません。\n",
    "\n",
    "チャット形式が実際にどのように機能するかを確認するために、チャットAPIコールの例を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example OpenAI Python library request\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-8dee9DuEFcg2QILtT2a6EBXZnpirM\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"Orange who?\",\n",
      "                \"role\": \"assistant\",\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1704461729,\n",
      "    \"model\": \"gpt-3.5-turbo-0613\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"system_fingerprint\": null,\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 3,\n",
      "        \"prompt_tokens\": 35,\n",
      "        \"total_tokens\": 38\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(json.loads(response.model_dump_json()), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ご覧のとおり、レスポンスオブジェクトにはいくつかのフィールドがあります：\n",
    "- `id`: リクエストのID\n",
    "- `choices`: 完了オブジェクトのリスト（`n`を1より大きく設定しない限り、1つのみ）\n",
    "    - `finish_reason`: モデルがテキスト生成を停止した理由（`stop`、または`max_tokens`制限に達した場合は`length`）\n",
    "    - `index`: 選択肢リスト内での選択肢のインデックス\n",
    "    - `logprobs`: 選択肢の対数確率情報\n",
    "    - `message`: モデルによって生成されたメッセージオブジェクト\n",
    "        - `content`: メッセージの内容\n",
    "        - `role`: このメッセージの作成者の役割\n",
    "        - `tool_calls`: 関数呼び出しなど、モデルによって生成されたツール呼び出し（toolsが指定されている場合）\n",
    "- `created`: リクエストのタイムスタンプ\n",
    "- `model`: レスポンス生成に使用されたモデルの完全名\n",
    "- `object`: 返されるオブジェクトのタイプ（例：`chat.completion`）\n",
    "- `system_fingerprint`: モデルが実行されるバックエンド設定を表すフィンガープリント\n",
    "- `usage`: 返答生成に使用されたトークン数（プロンプト、完了、合計をカウント）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下で返信のみを抽出してください："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Orange who?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会話ベースではないタスクでも、最初のユーザーメッセージに指示を配置することで、チャット形式に適合させることができます。\n",
    "\n",
    "例えば、海賊ブラックベアードのスタイルで非同期プログラミングを説明するようモデルに求める場合、以下のように会話を構成できます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr, me matey! Let me tell ye a tale of asynchronous programming, in the style of the fearsome pirate Blackbeard!\n",
      "\n",
      "Picture this, me hearties. In the vast ocean of programming, there be times when ye need to perform multiple tasks at once. But fear not, for asynchronous programming be here to save the day!\n",
      "\n",
      "Ye see, in traditional programming, ye be waitin' for one task to be done before movin' on to the next. But with asynchronous programming, ye can be takin' care of multiple tasks at the same time, just like a pirate multitaskin' on the high seas!\n",
      "\n",
      "Instead of waitin' for a task to be completed, ye can be sendin' it off on its own journey, while ye move on to the next task. It be like havin' a crew of trusty sailors, each takin' care of their own duties, without waitin' for the others.\n",
      "\n",
      "Now, ye may be wonderin', how does this sorcery work? Well, me matey, it be all about callbacks and promises. When ye be sendin' off a task, ye be attachin' a callback function to it. This be like leavin' a message in a bottle, tellin' the task what to do when it be finished.\n",
      "\n",
      "While the task be sailin' on its own, ye can be movin' on to the next task, without wastin' any precious time. And when the first task be done, it be sendin' a signal back to ye, lettin' ye know it be finished. Then ye can be takin' care of the callback function, like openin' the bottle and readin' the message inside.\n",
      "\n",
      "But wait, there be more! With promises, ye can be makin' even fancier arrangements. Instead of callbacks, ye be makin' a promise that the task will be completed. It be like a contract between ye and the task, swearin' that it will be done.\n",
      "\n",
      "Ye can be attachin' multiple promises to a task, promisin' different outcomes. And when the task be finished, it be fulfillin' the promises, lettin' ye know it be done. Then ye can be handlin' the fulfillments, like collectin' the rewards of yer pirate adventures!\n",
      "\n",
      "So, me hearties, that be the tale of asynchronous programming, told in the style of the fearsome pirate Blackbeard! With callbacks and promises, ye can be takin' care of multiple tasks at once, just like a pirate conquerin' the seven seas!\n"
     ]
    }
   ],
   "source": [
    "# example with a system message\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr, me hearties! Gather 'round and listen up, for I be tellin' ye about the mysterious art of asynchronous programming, in the style of the fearsome pirate Blackbeard!\n",
      "\n",
      "Now, ye see, in the world of programming, there be times when we need to perform tasks that take a mighty long time to complete. These tasks might involve fetchin' data from the depths of the internet, or performin' complex calculations that would make even Davy Jones scratch his head.\n",
      "\n",
      "In the olden days, we pirates used to wait patiently for each task to finish afore movin' on to the next one. But that be a waste of precious time, me hearties! We be pirates, always lookin' for ways to be more efficient and plunder more booty!\n",
      "\n",
      "That be where asynchronous programming comes in, me mateys. It be a way to tackle multiple tasks at once, without waitin' for each one to finish afore movin' on. It be like havin' a crew of scallywags workin' on different tasks simultaneously, while ye be overseein' the whole operation.\n",
      "\n",
      "Ye see, in asynchronous programming, we be breakin' down our tasks into smaller chunks called \"coroutines.\" Each coroutine be like a separate pirate, workin' on its own task. When a coroutine be startin' its work, it don't wait for the task to finish afore movin' on to the next one. Instead, it be movin' on to the next task, lettin' the first one continue in the background.\n",
      "\n",
      "Now, ye might be wonderin', \"But Blackbeard, how be we know when a task be finished if we don't wait for it?\" Ah, me hearties, that be where the magic of callbacks and promises come in!\n",
      "\n",
      "When a coroutine be startin' its work, it be attachin' a callback or a promise to it. This be like leavin' a message in a bottle, tellin' the coroutine what to do when it be finished. So, while the coroutine be workin' away, the rest of the crew be movin' on to other tasks, plunderin' more booty along the way.\n",
      "\n",
      "When a coroutine be finished with its task, it be sendin' a signal to the callback or fulfillin' the promise, lettin' the rest of the crew know that it be done. Then, the crew can gather 'round and handle the results of the completed task, celebratin' their victory and countin' their plunder.\n",
      "\n",
      "So, me hearties, asynchronous programming be like havin' a crew of pirates workin' on different tasks at once, without waitin' for each one to finish afore movin' on. It be a way to be more efficient, plunder more booty, and conquer the vast seas of programming!\n",
      "\n",
      "Now, set sail, me mateys, and embrace the power of asynchronous programming like true pirates of the digital realm! Arr!\n"
     ]
    }
   ],
   "source": [
    "# example without a system message\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. gpt-3.5-turbo-0301への指示のコツ\n",
    "\n",
    "モデルへの指示のベストプラクティスは、モデルのバージョンごとに変わる可能性があります。以下のアドバイスは`gpt-3.5-turbo-0301`に適用されるものであり、将来のモデルには適用されない可能性があります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### システムメッセージ\n",
    "\n",
    "システムメッセージは、アシスタントに異なる性格や行動を設定するために使用できます。\n",
    "\n",
    "`gpt-3.5-turbo-0301`は一般的に`gpt-4-0314`や`gpt-3.5-turbo-0613`ほどシステムメッセージに注意を払わないことに注意してください。そのため、`gpt-3.5-turbo-0301`については、重要な指示をユーザーメッセージに配置することを推奨します。一部の開発者は、会話が長くなるにつれてモデルの注意が逸れることを防ぐために、システムメッセージを会話の終わり近くに継続的に移動させることで成功を収めています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Fractions are a way to represent parts of a whole. They are made up of two numbers: a numerator and a denominator. The numerator tells you how many parts you have, and the denominator tells you how many equal parts make up the whole.\n",
      "\n",
      "Let's take an example to understand this better. Imagine you have a pizza that is divided into 8 equal slices. If you eat 3 slices, you can represent that as the fraction 3/8. Here, the numerator is 3 because you ate 3 slices, and the denominator is 8 because the whole pizza is divided into 8 slices.\n",
      "\n",
      "Fractions can also be used to represent numbers less than 1. For example, if you eat half of a pizza, you can write it as 1/2. Here, the numerator is 1 because you ate one slice, and the denominator is 2 because the whole pizza is divided into 2 equal parts.\n",
      "\n",
      "Now, let's talk about equivalent fractions. Equivalent fractions are different fractions that represent the same amount. For example, 1/2 and 2/4 are equivalent fractions because they both represent half of something. To find equivalent fractions, you can multiply or divide both the numerator and denominator by the same number.\n",
      "\n",
      "Here's a question to check your understanding: If you have a cake divided into 12 equal slices and you eat 4 slices, what fraction of the cake did you eat?\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to explain concepts in great depth\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractions represent parts of a whole. They have a numerator (top number) and a denominator (bottom number).\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to give brief, to-the-point answers\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "場合によっては、モデルに何をしてほしいかを説明するよりも、実際に見せる方が簡単なことがあります。\n",
    "\n",
    "モデルに何をしてほしいかを示す一つの方法は、偽の例示メッセージを使用することです。\n",
    "\n",
    "例えば："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sudden change in direction means we don't have enough time to complete the entire project for the client.\n"
     ]
    }
   ],
   "source": [
    "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルが例示メッセージを実際の会話の一部として扱わず、後で参照しないようにするために、`system`メッセージの`name`フィールドを`example_user`と`example_assistant`に設定することを試すことができます。\n",
    "\n",
    "上記のfew-shotの例を変換すると、次のように書くことができます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sudden change in direction means we don't have enough time to complete the entire project for the client.\n"
     ]
    }
   ],
   "source": [
    "# The business jargon translation example, but with example names for the example messages\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エンジニアリングされた会話のすべての試みが最初から成功するわけではありません。\n",
    "\n",
    "最初の試みが失敗した場合は、モデルをプライミングまたは条件付けする異なる方法を実験することを恐れないでください。\n",
    "\n",
    "例として、ある開発者は「これまで素晴らしい仕事です、これらは完璧でした」というユーザーメッセージを挿入することで、モデルがより高品質な応答を提供するよう条件付けし、精度の向上を発見しました。\n",
    "\n",
    "モデルの信頼性を向上させる方法についてのより多くのアイデアについては、[信頼性を向上させるテクニック](../techniques_to_improve_reliability)に関するガイドを読むことを検討してください。これは非チャットモデル向けに書かれましたが、その原則の多くは今でも適用されます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. トークンのカウント\n",
    "\n",
    "リクエストを送信すると、APIはメッセージをトークンのシーケンスに変換します。\n",
    "\n",
    "使用されるトークン数は以下に影響します：\n",
    "- リクエストのコスト\n",
    "- レスポンス生成にかかる時間\n",
    "- 最大トークン制限に達した際の返答の切り捨て（`gpt-3.5-turbo`では4,096、`gpt-4`では8,192）\n",
    "\n",
    "メッセージのリストが使用するトークン数をカウントするには、以下の関数を使用できます。\n",
    "\n",
    "メッセージからトークンがカウントされる正確な方法は、モデルによって変わる可能性があることに注意してください。以下の関数からのカウントは推定値として考え、永続的な保証ではないことを理解してください。\n",
    "\n",
    "特に、オプションのfunctions入力を使用するリクエストは、以下で計算される推定値に加えて追加のトークンを消費します。\n",
    "\n",
    "トークンのカウントについて詳しくは、[tiktokenでトークンをカウントする方法](How_to_count_tokens_with_tiktoken.ipynb)をお読みください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-1106\n",
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-3.5-turbo\n",
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4\n",
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4-1106-preview\n",
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's verify the function above matches the OpenAI API response\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for model in [\n",
    "    # \"gpt-3.5-turbo-0301\",\n",
    "    # \"gpt-4-0314\",\n",
    "    # \"gpt-4-0613\",\n",
    "    \"gpt-3.5-turbo-1106\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4\",\n",
    "    \"gpt-4-1106-preview\",\n",
    "    ]:\n",
    "    print(model)\n",
    "    # example token count from the function defined above\n",
    "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
    "    # example token count from the OpenAI API\n",
    "    response = client.chat.completions.create(model=model,\n",
    "    messages=example_messages,\n",
    "    temperature=0,\n",
    "    max_tokens=1)\n",
    "    token = response.usage.prompt_tokens\n",
    "    print(f'{token} prompt tokens counted by the OpenAI API.')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
