{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a492f6a4",
   "metadata": {},
   "source": [
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç§»è¡Œã‚¬ã‚¤ãƒ‰\n",
    "GPT-4.1ãªã©ã®æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨æŒ‡ç¤ºã®éµå®ˆã«ãŠã„ã¦æœ€é«˜ã‚¯ãƒ©ã‚¹ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šè³¢ããªã‚‹ã«ã¤ã‚Œã¦ã€å…ƒã€…ã¯åˆæœŸãƒ¢ãƒ‡ãƒ«ã®åˆ¶é™ã«åˆã‚ã›ã¦èª¿æ•´ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é©å¿œã•ã›ã€æ–°ä¸–ä»£ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã‚‚åŠ¹æœçš„ã§æ˜ç¢ºã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºä¿ã™ã‚‹ç¶™ç¶šçš„ãªå¿…è¦æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "GPT-4.1ãªã©ã®ãƒ¢ãƒ‡ãƒ«ã¯æŒ‡ç¤ºã«å³å¯†ã«å¾“ã†ã“ã¨ã«å„ªã‚Œã¦ã„ã¾ã™ãŒã€ã“ã®ç²¾å¯†ã•ã¯ä¸æ˜ç¢ºã¾ãŸã¯ä¸é©åˆ‡ã«è¡¨ç¾ã•ã‚ŒãŸæŒ‡ç¤ºã‚’**æ–‡å­—é€šã‚Š**è§£é‡ˆã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã€äºˆæœŸã—ãªã„çµæœã‚„ä¸æ­£ç¢ºãªçµæœã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚GPT-4.1ã®æ½œåœ¨èƒ½åŠ›ã‚’æœ€å¤§é™ã«æ´»ç”¨ã™ã‚‹ã«ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ´—ç·´ã—ã€å„æŒ‡ç¤ºãŒæ˜ç¤ºçš„ã§æ›–æ˜§ã•ãŒãªãã€æ„å›³ã—ãŸçµæœã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºä¿ã™ã‚‹ã“ã¨ãŒä¸å¯æ¬ ã§ã™ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "ä¸æ˜ç¢ºãªæŒ‡ç¤ºã®ä¾‹ï¼š\n",
    "\n",
    "- æ›–æ˜§ãªä¾‹ï¼š\n",
    "\n",
    "> \"ç„¡é–¢ä¿‚ãªæƒ…å ±ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚\"\n",
    "\n",
    "å•é¡Œï¼šGPT-4.1ã¯ã€Œç„¡é–¢ä¿‚ã€ã¨ã¯ä½•ã‹ãŒæ˜ç¤ºçš„ã«å®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã€åˆ¤æ–­ã«è‹¦åŠ´ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€éåº¦ã«æ…é‡ãªè§£é‡ˆã®ãŸã‚ã«é‡è¦ãªè©³ç´°ã‚’çœç•¥ã—ãŸã‚Šã€æ„å›³ã›ãšã«è©³ç´°ã‚’å«ã‚ã™ããŸã‚Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "- æ”¹å–„ã•ã‚ŒãŸä¾‹ï¼š\n",
    "\n",
    "> \"ãƒ¡ã‚¤ãƒ³ãƒˆãƒ”ãƒƒã‚¯ï¼ˆXï¼‰ã«ç›´æ¥é–¢é€£ã™ã‚‹äº‹å®Ÿã®ã¿ã‚’å«ã‚ã¦ãã ã•ã„ã€‚å€‹äººçš„ãªé€¸è©±ã€ç„¡é–¢ä¿‚ãªæ­´å²çš„èƒŒæ™¯ã€ã¾ãŸã¯è„‡é“ã®è­°è«–ã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚\"\n",
    "\n",
    "---\n",
    "\n",
    "**ç›®çš„**ï¼šã“ã®å¯¾è©±å‹ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆä»–ã®ãƒ¢ãƒ‡ãƒ«ç”¨ã«æ›¸ã‹ã‚ŒãŸã‚‚ã®ï¼‰ã‚’ã€æ˜ç¢ºã§æ›–æ˜§ã•ãŒãªãã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«å¾“ã£ã¦GPT-4.1ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚‚ã®ã«æ”¹å–„ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚\n",
    "\n",
    "**ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ¦‚è¦**  \n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ä»¥ä¸‹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½¿ç”¨ã—ã¾ã™ï¼š\n",
    "\n",
    "- [ã‚¹ãƒ†ãƒƒãƒ—1. å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›](#step-1-input-your-original-prompt)  \n",
    "- [ã‚¹ãƒ†ãƒƒãƒ—2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ã™ã¹ã¦ã®æŒ‡ç¤ºã‚’ç‰¹å®š](#step-2-identify-all-instructions-in-your-prompt)  \n",
    "- [ã‚¹ãƒ†ãƒƒãƒ—3. GPT-4.1ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’*æ‰¹è©•*ã—ã¦ã‚‚ã‚‰ã†](#step-3-ask-gpt-4-1-to-critique-the-prompt)  \n",
    "- [ã‚¹ãƒ†ãƒƒãƒ—4. ä¿®æ­£ã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªå‹•ç”Ÿæˆ](#step-4-auto-generate-a-revised-system-prompt)  \n",
    "- [ã‚¹ãƒ†ãƒƒãƒ—5. è©•ä¾¡ã¨åå¾©](#step-5-evaluate-and-iterate)  \n",
    "- [ã‚¹ãƒ†ãƒƒãƒ—6. ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰GPT-4.1ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’è‡ªå‹•é©ç”¨](#step-6-optional-automatically-apply-gpt-4-1-best-practices)\n",
    "\n",
    "**å‰ææ¡ä»¶**\n",
    "- `openai` Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨`OPENAI_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0c4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai pydantic tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "819c46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & API connection\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, Iterable, List, Optional\n",
    "import tiktoken\n",
    "import html\n",
    "from html import escape  \n",
    "import difflib\n",
    "import sys\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "try:\n",
    "    from IPython.display import HTML, display\n",
    "    _IN_IPYTHON = True\n",
    "except ImportError:\n",
    "    _IN_IPYTHON = False\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "MODEL = \"gpt-4.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546cbcd8",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åˆ†æã¨ä¿®æ­£ã‚’ç°¡å˜ã«ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f924a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "_COLORS = {\n",
    "    '+': (\"#d2f5d6\", \"#22863a\"),   # additions  (green)\n",
    "    '-': (\"#f8d7da\", \"#b31d28\"),   # deletions  (red)\n",
    "    '@': (None,      \"#6f42c1\"),   # hunk header (purple)\n",
    "}\n",
    "\n",
    "def _css(**rules: str) -> str:\n",
    "    \"\"\"Convert kwargs to a CSS string (snake_case â†’ kebab-case).\"\"\"\n",
    "    return \";\".join(f\"{k.replace('_', '-')}: {v}\" for k, v in rules.items())\n",
    "\n",
    "def _render(html_str: str) -> None:\n",
    "    \"\"\"Render inside Jupyter if available, else print to stdout.\"\"\"\n",
    "    try:\n",
    "        display  # type: ignore[name-defined]\n",
    "        from IPython.display import HTML  # noqa: WPS433\n",
    "        display(HTML(html_str))\n",
    "    except NameError:\n",
    "        print(html_str, flush=True)\n",
    "\n",
    "# ---------- diff helpers ------------------------------------------------------\n",
    "\n",
    "def _style(line: str) -> str:\n",
    "    \"\"\"Wrap a diff line in a <span> with optional colors.\"\"\"\n",
    "    bg, fg = _COLORS.get(line[:1], (None, None))\n",
    "    css = \";\".join(s for s in (f\"background:{bg}\" if bg else \"\",\n",
    "                               f\"color:{fg}\" if fg else \"\") if s)\n",
    "    return f'<span style=\"{css}\">{html.escape(line)}</span>'\n",
    "\n",
    "def _wrap(lines: Iterable[str]) -> str:\n",
    "    body = \"<br>\".join(lines)\n",
    "    return (\n",
    "        \"<details>\"\n",
    "        \"<summary>ğŸ•µï¸â€â™‚ï¸ Critique & Diff (click to expand)</summary>\"\n",
    "        f'<div style=\"font-family:monospace;white-space:pre;\">{body}</div>'\n",
    "        \"</details>\"\n",
    "    )\n",
    "\n",
    "def show_critique_and_diff(old: str, new: str) -> str:\n",
    "    \"\"\"Display & return a GitHub-style HTML diff between *old* and *new*.\"\"\"\n",
    "    diff = difflib.unified_diff(old.splitlines(), new.splitlines(),\n",
    "                                fromfile=\"old\", tofile=\"new\", lineterm=\"\")\n",
    "    html_block = _wrap(map(_style, diff))\n",
    "    _render(html_block)\n",
    "    return html_block\n",
    "\n",
    "# ---------- â€œcardâ€ helpers ----------------------------------------------------\n",
    "\n",
    "CARD    = _css(background=\"#f8f9fa\", border_radius=\"8px\", padding=\"18px 22px\",\n",
    "               margin_bottom=\"18px\", border=\"1px solid #e0e0e0\",\n",
    "               box_shadow=\"0 1px 4px #0001\")\n",
    "TITLE   = _css(font_weight=\"600\", font_size=\"1.1em\", color=\"#2d3748\",\n",
    "               margin_bottom=\"6px\")\n",
    "LABEL   = _css(color=\"#718096\", font_size=\"0.95em\", font_weight=\"500\",\n",
    "               margin_right=\"6px\")\n",
    "EXTRACT = _css(font_family=\"monospace\", background=\"#f1f5f9\", padding=\"7px 10px\",\n",
    "               border_radius=\"5px\", display=\"block\", margin_top=\"3px\",\n",
    "               white_space=\"pre-wrap\", color=\"#1a202c\")\n",
    "\n",
    "def display_cards(\n",
    "    items: Iterable[Any],\n",
    "    *,\n",
    "    title_attr: str,\n",
    "    field_labels: Optional[Dict[str, str]] = None,\n",
    "    card_title_prefix: str = \"Item\",\n",
    ") -> None:\n",
    "    \"\"\"Render objects as HTML â€œcardsâ€ (or plaintext when not in IPython).\"\"\"\n",
    "    items = list(items)\n",
    "    if not items:\n",
    "        _render(\"<em>No data to display.</em>\")\n",
    "        return\n",
    "\n",
    "    # auto-derive field labels if none supplied\n",
    "    if field_labels is None:\n",
    "        sample = items[0]\n",
    "        field_labels = {\n",
    "            a: a.replace(\"_\", \" \").title()\n",
    "            for a in dir(sample)\n",
    "            if not a.startswith(\"_\")\n",
    "            and not callable(getattr(sample, a))\n",
    "            and a != title_attr\n",
    "        }\n",
    "\n",
    "    cards = []\n",
    "    for idx, obj in enumerate(items, 1):\n",
    "        title_html = html.escape(str(getattr(obj, title_attr, \"<missing title>\")))\n",
    "        rows = [f'<div style=\"{TITLE}\">{card_title_prefix} {idx}: {title_html}</div>']\n",
    "\n",
    "        for attr, label in field_labels.items():\n",
    "            value = getattr(obj, attr, None)\n",
    "            if value is None:\n",
    "                continue\n",
    "            rows.append(\n",
    "                f'<div><span style=\"{LABEL}\">{html.escape(label)}:</span>'\n",
    "                f'<span style=\"{EXTRACT}\">{html.escape(str(value))}</span></div>'\n",
    "            )\n",
    "\n",
    "        cards.append(f'<div style=\"{CARD}\">{\"\".join(rows)}</div>')\n",
    "\n",
    "    _render(\"\\n\".join(cards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3163f30",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ— 1. å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã™ã‚‹\n",
    "æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸‰é‡å¼•ç”¨ç¬¦ï¼ˆ\"\"\"ï¼‰ã§å›²ã‚“ã§æ˜ç¢ºã«æä¾›ã—ã¦ãã ã•ã„ã€‚ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ”¹å–„ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ä¾‹ã§ã¯ã€ä»¥ä¸‹ã®[è«–æ–‡](https://arxiv.org/pdf/2306.05685)ã§æä¾›ã•ã‚Œã¦ã„ã‚‹LLM-as-a-Judgeã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a47cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original prompt length: 243 tokens\n"
     ]
    }
   ],
   "source": [
    "original_prompt = \"\"\"\n",
    "[System]\n",
    "Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
    "AI assistants to the user question displayed below. You should choose the assistant that\n",
    "follows the userâ€™s instructions and answers the userâ€™s question better. Your evaluation\n",
    "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
    "and level of detail of their responses. Begin your evaluation by comparing the two\n",
    "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
    "order in which the responses were presented does not influence your decision. Do not allow\n",
    "the length of the responses to influence your evaluation. Do not favor certain names of\n",
    "the assistants. Be as objective as possible. After providing your explanation, output your\n",
    "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
    "if assistant B is better, and \"[[C]]\" for a tie.\n",
    "\n",
    "[User Question]\n",
    "{question}\n",
    "\n",
    "[The Start of Assistant Aâ€™s Answer]\n",
    "{answer_a}\n",
    "[The End of Assistant Aâ€™s Answer]\n",
    "\n",
    "[The Start of Assistant Bâ€™s Answer]\n",
    "{answer_b}\n",
    "[The End of Assistant Bâ€™s Answer]\n",
    "\"\"\"\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "num_tokens = len(encoding.encode(original_prompt))\n",
    "print(\"Original prompt length:\", num_tokens, \"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cea51e",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ— 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ã™ã¹ã¦ã®æŒ‡ç¤ºã‚’ç‰¹å®šã™ã‚‹\n",
    "ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã§LLMãŒç‰¹å®šã™ã‚‹ã™ã¹ã¦ã®**æŒ‡ç¤º**ã‚’æŠ½å‡ºã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒªã‚¹ãƒˆã‚’ç¢ºèªã—ã€æŒ‡ç¤ºã§ã‚ã‚‹ã¹ãã§ãªã„æ–‡ã‚’è¦‹ã¤ã‘ã€æ›–æ˜§ãªæŒ‡ç¤ºã‚’æ˜ç¢ºåŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã•ã‚ŒãŸå„æŒ‡ç¤ºãŒæ­£ç¢ºã§ã€ä¿æŒã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹ã“ã¨ã‚’æ…é‡ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae7cbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instruction(BaseModel):\n",
    "    instruction_title: str = Field(description=\"A 2-8 word title of the instruction that the LLM has to follow.\")\n",
    "    extracted_instruction: str = Field(description=\"The exact text that was extracted from the system prompt that the instruction is derived from.\")\n",
    "\n",
    "class InstructionList(BaseModel):\n",
    "    instructions: list[Instruction] = Field(description=\"A list of instructions and their corresponding extracted text that the LLM has to follow.\")\n",
    "\n",
    "\n",
    "EXTRACT_INSTRUCTIONS_SYSTEM_PROMPT = \"\"\"\n",
    "## Role & Objective\n",
    "You are an **Instruction-Extraction Assistant**.  \n",
    "Your job is to read a System Prompt provided by the user and distill the **mandatory instructions** the target LLM must obey.\n",
    "\n",
    "## Instructions\n",
    "1. **Identify Mandatory Instructions**  \n",
    "   â€¢ Locate every instruction in the System Prompt that the LLM is explicitly required to follow.  \n",
    "   â€¢ Ignore suggestions, best-practice tips, or optional guidance.\n",
    "\n",
    "2. **Generate Rules**  \n",
    "   â€¢ Re-express each mandatory instruction as a clear, concise rule.\n",
    "   â€¢ Provide the extracted text that the instruction is derived from.\n",
    "   â€¢ Each rule must be standalone and imperative.\n",
    "\n",
    "## Output Format\n",
    "Return a json object with a list of instructions which contains an instruction_title and their corresponding extracted text that the LLM has to follow. Do not include any other text or comments.\n",
    "\n",
    "## Constraints\n",
    "- Include **only** rules that the System Prompt explicitly enforces.  \n",
    "- Omit any guidance that is merely encouraged, implied, or optional.  \n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=MODEL,\n",
    "    input=\"SYSTEM_PROMPT TO ANALYZE: \" + original_prompt,\n",
    "    instructions=EXTRACT_INSTRUCTIONS_SYSTEM_PROMPT,\n",
    "    temperature=0.0,\n",
    "    text_format=InstructionList,\n",
    ")\n",
    "\n",
    "instructions_list = response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0985a544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Instruction 1: Act as an impartial judge</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.</span></div></div>\n",
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Instruction 2: Choose the better assistant</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">You should choose the assistant that follows the userâ€™s instructions and answers the userâ€™s question better.</span></div></div>\n",
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Instruction 3: Consider specific evaluation factors</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.</span></div></div>\n",
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Instruction 4: Begin with a comparison and explanation</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">Begin your evaluation by comparing the two responses and provide a short explanation.</span></div></div>\n",
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Instruction 5: Avoid position biases</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.</span></div></div>\n",
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Instruction 6: Do not let response length influence evaluation</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">Do not allow the length of the responses to influence your evaluation.</span></div></div>\n",
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Instruction 7: Do not favor assistant names</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">Do not favor certain names of the assistants.</span></div></div>\n",
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Instruction 8: Be objective</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">Be as objective as possible.</span></div></div>\n",
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Instruction 9: Output final verdict in strict format</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">After providing your explanation, output your final verdict by strictly following this format: &quot;[[A]]&quot; if assistant A is better, &quot;[[B]]&quot; if assistant B is better, and &quot;[[C]]&quot; for a tie.</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_cards(\n",
    "    instructions_list.instructions,\n",
    "    title_attr=\"instruction_title\",\n",
    "    field_labels={\"extracted_instruction\": \"Extracted Text\"},\n",
    "    card_title_prefix=\"Instruction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4347d",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã©ã®éƒ¨åˆ†ã‚’æŒ‡ç¤ºã¨ã—ã¦èªè­˜ã—ã¦ã„ã‚‹ã‹ã‚’èª¿ã¹ã‚‹ã“ã¨ã¯æœ‰ç”¨ã§ã™ã€‚æŒ‡ç¤ºã¯è‡ªç„¶è¨€èªã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã€Œãƒ—ãƒ­ã‚°ãƒ©ãƒ ã€ã™ã‚‹æ–¹æ³•ãªã®ã§ã€ãã‚Œã‚‰ãŒæ˜ç¢ºã§æ­£ç¢ºã‹ã¤é©åˆ‡ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºå®Ÿã«ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db52dd3",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ—3. GPT-4.1ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’*æ‰¹è©•*ã—ã¦ã‚‚ã‚‰ã†\n",
    "æ¬¡ã«ã€GPT-4.1è‡ªèº«ãŒå…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ‰¹è©•ã—ã€æ··ä¹±ã‚„ã‚¨ãƒ©ãƒ¼ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ã®ã‚ã‚‹é ˜åŸŸã‚’å…·ä½“çš„ã«ç‰¹å®šã—ã¾ã™ï¼š\n",
    "\n",
    "- æ›–æ˜§ã•ï¼šè¤‡æ•°ã®è§£é‡ˆãŒå¯èƒ½ãªãƒ•ãƒ¬ãƒ¼ã‚º\n",
    "\n",
    "- å®šç¾©ã®ä¸è¶³ï¼šæ˜ç¢ºã«å®šç¾©ã•ã‚Œã¦ã„ãªã„ãƒ©ãƒ™ãƒ«ã‚„ç”¨èªã§ã€ãƒ¢ãƒ‡ãƒ«ãŒæ„å›³ã•ã‚ŒãŸæ„å‘³ã‚’æ¨æ¸¬ã—ãŸã‚Šæ†¶æ¸¬ã—ãŸã‚Šã™ã‚‹åŸå› ã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã‚‚ã®\n",
    "\n",
    "- çŸ›ç›¾ã™ã‚‹æŒ‡ç¤ºï¼šçŸ›ç›¾ã—ãŸã‚Šé‡è¤‡ã—ãŸã‚Šã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚„æ¡ä»¶\n",
    "\n",
    "- æ–‡è„ˆã‚„å‰æã®æ¬ å¦‚ï¼šæ˜ç¤ºçš„ã«æä¾›ã•ã‚Œã¦ã„ãªã„å¿…è¦ãªæƒ…å ±ã‚„æ–‡è„ˆ\n",
    "\n",
    "æ‰¹è©•ã®å‡ºåŠ›ã¯æ˜ç¢ºã«æ•´ç†ã•ã‚Œã€æ”¹å–„ã®ãŸã‚ã®å®Ÿè¡Œå¯èƒ½ãªææ¡ˆã¨ã¨ã‚‚ã«å…·ä½“çš„ãªå•é¡Œç‚¹ãŒå¼·èª¿ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã¯**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸­ã§æ›–æ˜§ã¾ãŸã¯æ··ä¹±ã‚’æ‹›ãã¨æ„Ÿã˜ã‚‹éƒ¨åˆ†ã‚’ç‰¹å®šã™ã‚‹**ã“ã¨ãŒéå¸¸ã«å¾—æ„ã§ã™ã€‚ã“ã‚Œã‚‰ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã«ã¨ã£ã¦ã‚ˆã‚Šæ˜ç¢ºã§åŠ¹æœçš„ãªæŒ‡ç¤ºã¨ãªã‚‹ã‚ˆã†å·¥å¤«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96af823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CritiqueIssue(BaseModel):\n",
    "    issue: str\n",
    "    snippet: str\n",
    "    explanation: str\n",
    "    suggestion: str\n",
    "\n",
    "class CritiqueIssues(BaseModel):\n",
    "    issues: List[CritiqueIssue] = Field(..., min_length=1, max_length=6)\n",
    "    \n",
    "CRITIQUE_SYSTEM_PROMPT = \"\"\"\n",
    "## Role & Objective  \n",
    "You are a **Prompt-Critique Assistant**.\n",
    "Examine a user-supplied LLM prompt (targeting GPT-4.1 or compatible) and surface any weaknesses.\n",
    "\n",
    "## Instructions\n",
    "Check for the following issues:\n",
    "- Ambiguity: Could any wording be interpreted in more than one way?\n",
    "- Lacking Definitions: Are there any class labels, terms, or concepts that are not defined that might be misinterpreted by an LLM?\n",
    "- Conflicting, missing, or vague instructions: Are directions incomplete or contradictory?\n",
    "- Unstated assumptions: Does the prompt assume the model has to be able to do something that is not explicitly stated?\n",
    "\n",
    "## Do **NOT** list issues of the following types:\n",
    "- Invent new instructions, tool calls, or external information. You do not know what tools need to be added that are missing.\n",
    "- Issues that you are not sure about.\n",
    "\n",
    "## Output Format  \n",
    "Return a JSON **array** (not an object) with 1-6 items, each following this schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"issue\":      \"<1-6 word label>\",\n",
    "  \"snippet\":    \"<â‰¤50-word excerpt>\",\n",
    "  \"explanation\":\"<Why it matters>\",\n",
    "  \"suggestion\": \"<Actionable fix>\"\n",
    "}\n",
    "Return a JSON array of these objects. If the prompt is already clear, complete, and effective, return an empty list: `[]`.\n",
    "\"\"\"\n",
    "\n",
    "CRITIQUE_USER_PROMPT = f\"\"\"\n",
    "Evaluate the following prompt for clarity, completeness, and effectiveness:\n",
    "###\n",
    "{original_prompt}\n",
    "###\n",
    "Return your critique using the specified JSON format only.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0697e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.parse(\n",
    "    model=MODEL,\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": CRITIQUE_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": CRITIQUE_USER_PROMPT},\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    text_format=CritiqueIssues,\n",
    ")\n",
    "\n",
    "critique = response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cfb2877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Issue 1: Ambiguous evaluation criteria</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Snippet:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail</span></div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Explanation:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">The prompt lists several evaluation factors but does not define them or explain how to weigh them. This could lead to inconsistent or subjective judgments.</span></div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Suggestion:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">Provide clear definitions for each criterion and specify if any should be prioritized over others.</span></div></div>\n",
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Issue 2: Unclear handling of ties</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Snippet:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">&quot;[[C]]&quot; for a tie</span></div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Explanation:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">The prompt allows for a tie verdict but does not specify under what circumstances a tie is appropriate, which may lead to inconsistent use.</span></div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Suggestion:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">Clarify when a tie should be chosen, e.g., if both responses are equally strong across all criteria.</span></div></div>\n",
       "<div style=\"background: #f8f9fa; border-radius: 8px; padding: 18px 22px; margin-bottom: 18px; border: 1px solid #e0e0e0; box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600; font-size: 1.1em; color: #2d3748; margin-bottom: 6px\">Issue 3: Potential ambiguity in &#x27;objectivity&#x27;</div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Snippet:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">Be as objective as possible.</span></div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Explanation:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">The prompt asks for objectivity but does not specify what constitutes objectivity in this context, especially given the subjective nature of some criteria.</span></div><div><span style=\"color: #718096; font-size: 0.95em; font-weight: 500; margin-right: 6px\">Suggestion:</span><span style=\"font-family: monospace; background: #f1f5f9; padding: 7px 10px; border-radius: 5px; display: block; margin-top: 3px; white-space: pre-wrap; color: #1a202c\">Define what is meant by objectivity in this evaluation context, possibly by referencing adherence to the listed criteria.</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_cards(\n",
    "    critique.issues,\n",
    "    title_attr=\"issue\",\n",
    "    field_labels={\n",
    "        \"snippet\": \"Snippet\",\n",
    "        \"explanation\": \"Explanation\",\n",
    "        \"suggestion\": \"Suggestion\"\n",
    "    },\n",
    "    card_title_prefix=\"Issue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "572d4591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue: Ambiguous evaluation criteria\n",
      "Snippet: consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail\n",
      "Explanation: The prompt lists several evaluation factors but does not define them or explain how to weigh them. This could lead to inconsistent or subjective judgments.\n",
      "Suggestion: Provide clear definitions for each criterion and specify if any should be prioritized over others.\n",
      "\n",
      "Issue: Unclear handling of ties\n",
      "Snippet: \"[[C]]\" for a tie\n",
      "Explanation: The prompt allows for a tie verdict but does not specify under what circumstances a tie is appropriate, which may lead to inconsistent use.\n",
      "Suggestion: Clarify when a tie should be chosen, e.g., if both responses are equally strong across all criteria.\n",
      "\n",
      "Issue: Potential ambiguity in 'objectivity'\n",
      "Snippet: Be as objective as possible.\n",
      "Explanation: The prompt asks for objectivity but does not specify what constitutes objectivity in this context, especially given the subjective nature of some criteria.\n",
      "Suggestion: Define what is meant by objectivity in this evaluation context, possibly by referencing adherence to the listed criteria.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a string of the issues\n",
    "issues_str = \"\\n\".join(\n",
    "    f\"Issue: {issue.issue}\\nSnippet: {issue.snippet}\\nExplanation: {issue.explanation}\\nSuggestion: {issue.suggestion}\\n\"\n",
    "    for issue in critique.issues\n",
    ")\n",
    "\n",
    "print(issues_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef8ed9",
   "metadata": {},
   "source": [
    "å•é¡Œã®ãƒªã‚¹ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š\n",
    "- å•é¡Œã«æº€è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—#4ã«é€²ã‚“ã§ãã ã•ã„ã€‚\n",
    "- ä¸€éƒ¨ã®å•é¡ŒãŒé–¢é€£æ€§ãŒãªã„ã¨æ€ã‚ã‚Œã‚‹å ´åˆã¯ã€ä¸Šè¨˜ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¬¡ã®ã‚»ãƒ«ã«ã‚³ãƒ”ãƒ¼ã—ã¦ã€ãã‚Œã‚‰ã®å•é¡Œã‚’å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚ã“ã®å ´åˆã€3ã¤ã®å•é¡Œã™ã¹ã¦ãŒåˆç†çš„ã«æ„å‘³ã‚’ãªã—ã¦ã„ã‚‹ãŸã‚ã€ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54ce81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# issues_str = \"\"\"\n",
    "# PLACEHOLDER FOR ISSUES YOU WANT TO CORRECT, DO NOT RUN THIS CELL UNLESS YOU HAVE COPY-PASTED THE ISSUES FROM ABOVE\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab10b4",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ—4. æ”¹è¨‚ã•ã‚ŒãŸ*system*ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è‡ªå‹•ç”Ÿæˆ\n",
    "æ¬¡ã«ã€æ‰¹è©•ã‚’GPT-4.1ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã€å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ”¹è‰¯ç‰ˆã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†ä¾é ¼ã—ã¾ã™ã€‚ã“ã‚Œã¯`system`ãƒ­ãƒ¼ãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ãã®ã¾ã¾ä½¿ç”¨ã§ãã‚‹å½¢ã§å‡ºåŠ›ã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a2980a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REVISE_SYSTEM_PROMPT = \"\"\"\n",
    "## Role & Objective  \n",
    "Revise the userâ€™s original prompt to resolve most of the listed issues, while preserving the original wording and structure as much as possible.\n",
    "\n",
    "## Instructions\n",
    "1. Carefully review the original prompt and the list of issues.\n",
    "2. Apply targeted edits directly addressing the listed issues. The edits should be as minimal as possible while still addressing the issue.\n",
    "3. Do not introduce new content or make assumptions beyond the provided information.\n",
    "4. Maintain the original structure and format of the prompt.\n",
    "\n",
    "## Output Format\n",
    "Return only the fully revised prompt. Do not include commentary, summaries, or code fences.\n",
    "\"\"\"\n",
    "\n",
    "REVISE_USER_PROMPT = f\"\"\"\n",
    "Here is the original prompt:\n",
    "---\n",
    "{original_prompt}\n",
    "---\n",
    "\n",
    "Here are the issues to fix:\n",
    "{issues_str}\n",
    "\n",
    "Please return **only** the fully revised prompt. Do not include commentary, summaries, or explanations.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f90e43df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Revised prompt:\n",
      "------------------\n",
      "[System]\n",
      "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the userâ€™s instructions and answers the userâ€™s question better. Your evaluation should be based on the following criteria:\n",
      "\n",
      "- Helpfulness: The extent to which the response addresses the userâ€™s needs and provides useful information.\n",
      "- Relevance: How closely the response pertains to the userâ€™s question and instructions.\n",
      "- Accuracy: The correctness and factual reliability of the information provided.\n",
      "- Depth: The level of insight, explanation, or reasoning demonstrated in the response.\n",
      "- Creativity: The originality or resourcefulness shown in addressing the question, where appropriate.\n",
      "- Level of Detail: The thoroughness and completeness of the response.\n",
      "\n",
      "All criteria should be considered equally unless the userâ€™s instructions indicate otherwise. \n",
      "\n",
      "Begin your evaluation by comparing the two responses according to these criteria and provide a short explanation. Remain impartial by avoiding any position biases and ensuring that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses or the names of the assistants to influence your evaluation.\n",
      "\n",
      "Be as objective as possible by strictly adhering to the defined criteria above and basing your judgment solely on how well each response meets them.\n",
      "\n",
      "After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie. Choose \"[[C]]\" only if both responses are equally strong across all criteria.\n",
      "\n",
      "[User Question]\n",
      "{question}\n",
      "\n",
      "[The Start of Assistant Aâ€™s Answer]\n",
      "{answer_a}\n",
      "[The End of Assistant Aâ€™s Answer]\n",
      "\n",
      "[The Start of Assistant Bâ€™s Answer]\n",
      "{answer_b}\n",
      "[The End of Assistant Bâ€™s Answer]\n"
     ]
    }
   ],
   "source": [
    "revised_response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=REVISE_USER_PROMPT,\n",
    "    instructions=REVISE_SYSTEM_PROMPT,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "revised_prompt = revised_response.output_text\n",
    "print(\"\\nğŸ”„ Revised prompt:\\n------------------\")\n",
    "print(revised_response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98a4ff",
   "metadata": {},
   "source": [
    "æ”¹è‰¯ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ´—ç·´ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¤‰æ›´ç‚¹ã‚’å¼·èª¿ã—ãŸä¸¦åˆ—æ¯”è¼ƒã§å¤‰æ›´å†…å®¹ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6dc093c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details><summary>ğŸ•µï¸â€â™‚ï¸ Critique & Diff (click to expand)</summary><div style=\"font-family:monospace;white-space:pre;\"><span style=\"background:#f8d7da;color:#b31d28\">--- old</span><br><span style=\"background:#d2f5d6;color:#22863a\">+++ new</span><br><span style=\"color:#6f42c1\">@@ -1,15 +1,20 @@</span><br><span style=\"\"> [System]</span><br><span style=\"background:#f8d7da;color:#b31d28\">-Please act as an impartial judge and evaluate the quality of the responses provided by two</span><br><span style=\"background:#f8d7da;color:#b31d28\">-AI assistants to the user question displayed below. You should choose the assistant that</span><br><span style=\"background:#f8d7da;color:#b31d28\">-follows the userâ€™s instructions and answers the userâ€™s question better. Your evaluation</span><br><span style=\"background:#f8d7da;color:#b31d28\">-should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,</span><br><span style=\"background:#f8d7da;color:#b31d28\">-and level of detail of their responses. Begin your evaluation by comparing the two</span><br><span style=\"background:#f8d7da;color:#b31d28\">-responses and provide a short explanation. Avoid any position biases and ensure that the</span><br><span style=\"background:#f8d7da;color:#b31d28\">-order in which the responses were presented does not influence your decision. Do not allow</span><br><span style=\"background:#f8d7da;color:#b31d28\">-the length of the responses to influence your evaluation. Do not favor certain names of</span><br><span style=\"background:#f8d7da;color:#b31d28\">-the assistants. Be as objective as possible. After providing your explanation, output your</span><br><span style=\"background:#f8d7da;color:#b31d28\">-final verdict by strictly following this format: &quot;[[A]]&quot; if assistant A is better, &quot;[[B]]&quot;</span><br><span style=\"background:#f8d7da;color:#b31d28\">-if assistant B is better, and &quot;[[C]]&quot; for a tie.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the userâ€™s instructions and answers the userâ€™s question better. Your evaluation should be based on the following criteria:</span><br><span style=\"background:#d2f5d6;color:#22863a\">+</span><br><span style=\"background:#d2f5d6;color:#22863a\">+- Helpfulness: The extent to which the response addresses the userâ€™s needs and provides useful information.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+- Relevance: How closely the response pertains to the userâ€™s question and instructions.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+- Accuracy: The correctness and factual reliability of the information provided.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+- Depth: The level of insight, explanation, or reasoning demonstrated in the response.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+- Creativity: The originality or resourcefulness shown in addressing the question, where appropriate.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+- Level of Detail: The thoroughness and completeness of the response.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+</span><br><span style=\"background:#d2f5d6;color:#22863a\">+All criteria should be considered equally unless the userâ€™s instructions indicate otherwise. </span><br><span style=\"background:#d2f5d6;color:#22863a\">+</span><br><span style=\"background:#d2f5d6;color:#22863a\">+Begin your evaluation by comparing the two responses according to these criteria and provide a short explanation. Remain impartial by avoiding any position biases and ensuring that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses or the names of the assistants to influence your evaluation.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+</span><br><span style=\"background:#d2f5d6;color:#22863a\">+Be as objective as possible by strictly adhering to the defined criteria above and basing your judgment solely on how well each response meets them.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+</span><br><span style=\"background:#d2f5d6;color:#22863a\">+After providing your explanation, output your final verdict by strictly following this format: &quot;[[A]]&quot; if assistant A is better, &quot;[[B]]&quot; if assistant B is better, and &quot;[[C]]&quot; for a tie. Choose &quot;[[C]]&quot; only if both responses are equally strong across all criteria.</span><br><span style=\"\"> </span><br><span style=\"\"> [User Question]</span><br><span style=\"\"> {question}</span></div></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_critique_and_diff(original_prompt, revised_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf2edf5",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ— 5. è©•ä¾¡ã¨åå¾©\n",
    "æœ€å¾Œã«ã€ä»¥ä¸‹ã®æ–¹æ³•ã§æ´—ç·´ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è©•ä¾¡ã—ã¾ã™ï¼š\n",
    "\n",
    "- ä»£è¡¨çš„ãªè©•ä¾¡ä¾‹ã‚„ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆã™ã‚‹ã€‚\n",
    "\n",
    "- æœŸå¾…ã™ã‚‹çµæœãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’åˆ†æã™ã‚‹ã€‚\n",
    "\n",
    "- ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦ãªå ´åˆã¯ã€å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç¹°ã‚Šè¿”ã™ã€‚\n",
    "\n",
    "ä¸€è²«ã—ãŸãƒ†ã‚¹ãƒˆã¨æ”¹å–„ã«ã‚ˆã‚Šã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ„å›³ã—ãŸçµæœã‚’ç¶™ç¶šçš„ã«é”æˆã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0dc7f",
   "metadata": {},
   "source": [
    "### ç¾åœ¨ã®ä¾‹\n",
    "\n",
    "ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç§»è¡ŒãŒã€ã“ã®åˆ¤å®šã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦å®Ÿéš›ã«æ”¹å–„ã•ã‚ŒãŸã‹ã©ã†ã‹ã‚’è©•ä¾¡ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã“ã®[è«–æ–‡](https://arxiv.org/pdf/2306.05685)ã‹ã‚‰å¼•ç”¨ã•ã‚ŒãŸå…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€2ã¤ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å›ç­”é–“ã®åˆ¤å®šã‚’è¡Œã†ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚éƒ½åˆã‚ˆãã€ã“ã®è«–æ–‡ã¯äººé–“ã«ã‚ˆã‚‹æ³¨é‡ˆä»˜ãã®æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æä¾›ã—ã¦ã„ã‚‹ãŸã‚ã€LLMåˆ¤å®šè€…ãŒäººé–“ã®åˆ¤æ–­ã¨ã©ã®ç¨‹åº¦ä¸€è‡´ã™ã‚‹ã‹ã‚’æ¸¬å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "ã—ãŸãŒã£ã¦ã€æˆåŠŸã®æŒ‡æ¨™ã¯ã€ç§»è¡Œã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸåˆ¤å®šãŒã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç”Ÿæˆã•ã‚ŒãŸåˆ¤å®šã¨æ¯”è¼ƒã—ã¦ã€äººé–“ã®è©•ä¾¡ã¨ã©ã®ç¨‹åº¦å¯†æ¥ã«ä¸€è‡´ã™ã‚‹ã‹ã‚’æ¸¬å®šã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚æ–‡è„ˆã¨ã—ã¦ã€ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯MT-Benchã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§ã€ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‚’ç‰¹å¾´ã¨ã—ã¦ã„ã¾ã™ã€‚ã“ã®ä¾‹ã§ã¯ã€200ã®ä¼šè©±è¡Œã‚’è©•ä¾¡ã—ã¦ãŠã‚Šã€ãã‚Œãã‚ŒãŒç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ãƒšã‚¢ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¯”è¼ƒã—ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50f9a0",
   "metadata": {},
   "source": [
    "ç§ãŸã¡ã®è©•ä¾¡ã‚µãƒ–ã‚»ãƒƒãƒˆã«ãŠã„ã¦ã€æœ‰ç”¨ãªå‚è€ƒåŸºæº–ã¯äººé–“åŒå£«ã®ä¸€è‡´åº¦ã§ã™ã€‚å„ä¼šè©±ã¯è¤‡æ•°ã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã£ã¦è©•ä¾¡ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚äººé–“åŒå£«ã§ã‚‚ã€ã©ã¡ã‚‰ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å›ç­”ãŒå„ªã‚Œã¦ã„ã‚‹ã‹ã«ã¤ã„ã¦å¸¸ã«æ„è¦‹ãŒä¸€è‡´ã™ã‚‹ã‚ã‘ã§ã¯ãªã„ãŸã‚ã€ç§ãŸã¡ã®åˆ¤å®šè€…ãŒå®Œå…¨ãªä¸€è‡´ã‚’é”æˆã™ã‚‹ã“ã¨ã¯æœŸå¾…ã§ãã¾ã›ã‚“ã€‚ã‚¿ãƒ¼ãƒ³1ï¼ˆå¼•ãåˆ†ã‘ã‚’é™¤ãï¼‰ã§ã¯ã€äººé–“åŒå£«ã®ä¸€è‡´ç‡ã¯81%ã§ã‚ã‚Šã€ã‚¿ãƒ¼ãƒ³2ã§ã¯76%ã¨ãªã£ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0337b",
   "metadata": {},
   "source": [
    "![ãƒ¢ãƒ‡ãƒ«åˆæ„ã®ã‚°ãƒ©ãƒ•3](../images/prompt_migrator_fig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800da674",
   "metadata": {},
   "source": [
    "ç§»è¡Œå‰ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€GPT-4ï¼ˆè«–æ–‡ã§ä½¿ç”¨ã•ã‚ŒãŸã‚‚ã®ï¼‰ã¯ã€ã‚¿ãƒ¼ãƒ³1ã§74%ã€ã‚¿ãƒ¼ãƒ³2ã§71%ã®äººé–“ã®åˆ¤æ–­ã¨ã®ä¸€è‡´ç‡ã‚’é”æˆã—ã¦ãŠã‚Šã€ã“ã‚Œã¯æ‚ªãã‚ã‚Šã¾ã›ã‚“ãŒã€ãã‚Œã§ã‚‚äººé–“åŒå£«ã®ä¸Šé™ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚GPT-4.1ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ï¼ˆåŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ï¼‰ã¨ä¸€è‡´ç‡ãŒå‘ä¸Šã—ã€ã‚¿ãƒ¼ãƒ³1ã§77%ã€ã‚¿ãƒ¼ãƒ³2ã§72%ã¨ãªã‚Šã¾ã™ã€‚æœ€å¾Œã«ã€GPT-4.1å°‚ç”¨ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç§»è¡Œãƒ»èª¿æ•´ã—ãŸå¾Œã€ä¸€è‡´ç‡ã¯ã•ã‚‰ã«å‘ä¸Šã—ã€ã‚¿ãƒ¼ãƒ³1ã§80%ã€ã‚¿ãƒ¼ãƒ³2ã§72%ã«é”ã—ã€äººé–“ã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼é–“ã§è¦‹ã‚‰ã‚Œã‚‹ä¸€è‡´ãƒ¬ãƒ™ãƒ«ã«ã»ã¼åŒ¹æ•µã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae2ba5",
   "metadata": {},
   "source": [
    "å…¨ä½“çš„ã«è¦‹ã‚‹ã¨ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç§»è¡Œã¨ã‚ˆã‚Šå¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã«ã‚ˆã‚Šã€ã‚µãƒ³ãƒ—ãƒ«ã‚¿ã‚¹ã‚¯ã§ã®ä¸€è‡´åº¦ãŒå‘ä¸Šã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ä»Šã™ãã‚ãªãŸã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§è©¦ã—ã¦ã¿ã¦ãã ã•ã„ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed1776",
   "metadata": {},
   "source": [
    "## ã‚¹ãƒ†ãƒƒãƒ— 6. ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰GPT-4.1 ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®è‡ªå‹•é©ç”¨\n",
    "\n",
    "ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€GPT-4.1ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ãŒè‡ªå‹•çš„ã«é©ç”¨ã•ã‚Œã¾ã™ã€‚ç·¨é›†ã•ã‚ŒãŸå†…å®¹ã‚’æ‰‹å‹•ã§ç¢ºèªã—ã€ä¿æŒã™ã‚‹ã‹ã©ã†ã‹ã‚’æ±ºå®šã™ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚\n",
    "\n",
    "å‚è€ƒã¨ã—ã¦[4.1 Prompting Guide](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)ã‚’ã”è¦§ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02951cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PRACTICES_SYSTEM_PROMPT = \"\"\"\n",
    "## Task\n",
    "Your task is to take a **Baseline Prompt** (provided by the user) and output a **Revised Prompt** that keeps the original wording and order as intact as possible **while surgically inserting improvements that follow the â€œGPTâ€‘4.1â€¯Bestâ€¯Practicesâ€ reference**.\n",
    "\n",
    "## How to Edit\n",
    "1. **Keep original text** â€” Only remove something if it directly goes against a best practice. Otherwise, keep the wording, order, and examples as they are.\n",
    "2. **Add best practices only when clearly helpful.** If a guideline doesnâ€™t fit the prompt or its use case (e.g., diffâ€‘format guidance on a nonâ€‘coding prompt), just leave that part of the prompt unchanged.\n",
    "3. **Where to add improvements** (use Markdown `#` headings):\n",
    "   - At the very top, add *Agentic Reminders* (like Persistence, Tool-calling, or Planning) â€” only if relevant. Donâ€™t add these if the prompt doesnâ€™t require agentic behavior (agentic means prompts that involve planning or running tools for a while).\n",
    "   - When adding sections, follow this order if possible. If some sections do not make sense, don't add them:\n",
    "     1. `# Role & Objective`  \n",
    "        - State who the model is supposed to be (the role) and what its main goal is.\n",
    "     2. `# Instructions`  \n",
    "        - List the steps, rules, or actions the model should follow to complete the task.\n",
    "     3. *(Any sub-sections)*  \n",
    "        - Include any extra sections such as sub-instructions, notes or guidelines already in the prompt that donâ€™t fit into the main categories.\n",
    "     4. `# Reasoning Steps`  \n",
    "        - Explain the step-by-step thinking or logic the model should use when working through the task.\n",
    "     5. `# Output Format`  \n",
    "        - Describe exactly how the answer should be structured or formatted (e.g., what sections to include, how to label things, or what style to use).\n",
    "     6. `# Examples`  \n",
    "        - Provide sample questions and answers or sample outputs to show the model what a good response looks like.\n",
    "     7. `# Context`  \n",
    "        - Supply any background information, retrieved context, or extra details that help the model understand the task better.\n",
    "   - Donâ€™t introduce new sections that donâ€™t exist in the Baseline Prompt. For example, if thereâ€™s no `# Examples` or no `# Context` section, donâ€™t add one.\n",
    "4. If the prompt is for long context analysis or long tool use, repeat key Agentic Reminders, Important Reminders and Output Format points at the end.\n",
    "5. If there are class labels, evaluation criterias or key concepts, add a definition to each to define them concretely.\n",
    "5. Add a chain-of-thought trigger at the end of main instructions (like â€œThink step by step...â€), unless one is already there or it would be repetitive.\n",
    "6. For prompts involving tools or sample phrases, add Failure-mode bullets:\n",
    "   - â€œIf you donâ€™t have enough info to use a tool, ask the user first.â€\n",
    "   - â€œVary sample phrases to avoid repetition.â€\n",
    "7. Match the original tone (formal or casual) in anything you add.\n",
    "8. **Only output the full Revised Prompt** â€” no explanations, comments, or diffs. Do not output \"keep the original...\", you need to fully output the prompt, no shortcuts.\n",
    "9. Do not delete any sections or parts that are useful and add value to the prompt and doesn't go against the best practices.\n",
    "10. **Self-check before sending:** Make sure there are no typos, duplicated lines, missing headings, or missed steps.\n",
    "\n",
    "\n",
    "## GPTâ€‘4.1 Best Practices Reference  \n",
    "1. **Persistence reminder**: Explicitly instructs the model to continue working until the user's request is fully resolved, ensuring the model does not stop early.\n",
    "2. **Toolâ€‘calling reminder**: Clearly tells the model to use available tools or functions instead of making assumptions or guesses, which reduces hallucinations.\n",
    "3. **Planning reminder**: Directs the model to create a stepâ€‘byâ€‘step plan and reflect before and after tool calls, leading to more accurate and thoughtful output.\n",
    "4. **Scaffold structure**: Requires a consistent and predictable heading order (e.g., Role, Instructions, Outputâ€¯Format) to make prompts easier to maintain.\n",
    "5. **Instruction placement (long context)**: Ensures that key instructions are duplicated or placed strategically so they remain visible and effective in very long prompts.\n",
    "6. **Chainâ€‘ofâ€‘thought trigger**: Adds a phrase that encourages the model to reason step by step, which improves logical and thorough responses.\n",
    "7. **Instructionâ€‘conflict hygiene**: Checks for and removes any contradictory instructions, ensuring that the most recent or relevant rule takes precedence.\n",
    "8. **Failureâ€‘mode mitigations**: Adds safeguards against common errors, such as making empty tool calls or repeating phrases, to improve reliability.\n",
    "9. **Diffâ€¯/â€¯codeâ€‘edit format**: Specifies a robust, lineâ€‘numberâ€‘free diff or codeâ€‘edit style for output, making changes clear and easy to apply.\n",
    "10. **Label Definitions**: Defines all the key labels or terms that are used in the prompt so that the model knows what they mean.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c23ac8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved prompt:\n",
      "\n",
      "# Role & Objective\n",
      "You are an impartial judge. Your goal is to determine which of two AI assistant answers better fulfills the userâ€™s request.\n",
      "\n",
      "# Instructions  \n",
      "Follow the steps below exactly and remain strictly neutral:\n",
      "\n",
      "1. Read the User Question and both assistant answers in full.  \n",
      "2. Evaluate each answer against **all** six criteria, treating them with equal weight unless the user explicitly states otherwise:\n",
      "   â€¢ Helpfulness â€“ Does the response address the userâ€™s needs and provide useful information?  \n",
      "   â€¢ Relevance â€“ How closely does the response pertain to the userâ€™s question and instructions?  \n",
      "   â€¢ Accuracy â€“ Is the information correct and factually reliable?  \n",
      "   â€¢ Depth â€“ Does the answer show insight, explanation, or reasoning?  \n",
      "   â€¢ Creativity â€“ Is the approach original or resourceful when appropriate?  \n",
      "   â€¢ Level of Detail â€“ Is the response thorough and complete?  \n",
      "3. Stay impartial:  \n",
      "   â€¢ Ignore the order in which the answers appear.  \n",
      "   â€¢ Ignore the length of each answer.  \n",
      "   â€¢ Ignore the assistantsâ€™ names.  \n",
      "4. Make your decision solely on how well each response meets the criteria above.  \n",
      "5. After your analysis, produce a final verdict using the exact format in the Output Format section.\n",
      "\n",
      "# Reasoning Steps\n",
      "Think step by step:\n",
      "1. For each criterion, briefly note strengths and weaknesses for Assistant A.  \n",
      "2. Repeat for Assistant B.  \n",
      "3. Compare the two sets of notes criterion by criterion.  \n",
      "4. Decide which answer is overall superior, or declare a tie if both are equally strong across all criteria.\n",
      "\n",
      "# Output Format\n",
      "First provide a short, objective explanation (1â€“3 concise paragraphs).  \n",
      "Then on a new line output only one of the following tokens (without quotes or extra text):\n",
      "â€¢ [[A]]  â€“ if Assistant A is better  \n",
      "â€¢ [[B]]  â€“ if Assistant B is better  \n",
      "â€¢ [[C]]  â€“ if it is a tie  \n",
      "\n",
      "# Context (inserted at runtime)\n",
      "[User Question]  \n",
      "{question}\n",
      "\n",
      "[The Start of Assistant Aâ€™s Answer]  \n",
      "{answer_a}  \n",
      "[The End of Assistant Aâ€™s Answer]\n",
      "\n",
      "[The Start of Assistant Bâ€™s Answer]  \n",
      "{answer_b}  \n",
      "[The End of Assistant Bâ€™s Answer]\n"
     ]
    }
   ],
   "source": [
    "best_practices_response = client.responses.create(\n",
    "    model=\"o3\",\n",
    "    input=\"BASELINE_PROMPT: \" + revised_prompt,\n",
    "    instructions=BEST_PRACTICES_SYSTEM_PROMPT,\n",
    "    reasoning={\"effort\": \"high\"}\n",
    ")\n",
    "\n",
    "improved_prompt = best_practices_response.output_text\n",
    "print(\"\\nImproved prompt:\\n\")\n",
    "print(improved_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c79c019a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details><summary>ğŸ•µï¸â€â™‚ï¸ Critique & Diff (click to expand)</summary><div style=\"font-family:monospace;white-space:pre;\"><span style=\"background:#f8d7da;color:#b31d28\">--- old</span><br><span style=\"background:#d2f5d6;color:#22863a\">+++ new</span><br><span style=\"color:#6f42c1\">@@ -1,28 +1,46 @@</span><br><span style=\"background:#f8d7da;color:#b31d28\">-[System]</span><br><span style=\"background:#f8d7da;color:#b31d28\">-Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the userâ€™s instructions and answers the userâ€™s question better. Your evaluation should be based on the following criteria:</span><br><span style=\"background:#d2f5d6;color:#22863a\">+# Role &amp; Objective</span><br><span style=\"background:#d2f5d6;color:#22863a\">+You are an impartial judge. Your goal is to determine which of two AI assistant answers better fulfills the userâ€™s request.</span><br><span style=\"\"> </span><br><span style=\"background:#f8d7da;color:#b31d28\">-- Helpfulness: The extent to which the response addresses the userâ€™s needs and provides useful information.</span><br><span style=\"background:#f8d7da;color:#b31d28\">-- Relevance: How closely the response pertains to the userâ€™s question and instructions.</span><br><span style=\"background:#f8d7da;color:#b31d28\">-- Accuracy: The correctness and factual reliability of the information provided.</span><br><span style=\"background:#f8d7da;color:#b31d28\">-- Depth: The level of insight, explanation, or reasoning demonstrated in the response.</span><br><span style=\"background:#f8d7da;color:#b31d28\">-- Creativity: The originality or resourcefulness shown in addressing the question, where appropriate.</span><br><span style=\"background:#f8d7da;color:#b31d28\">-- Level of Detail: The thoroughness and completeness of the response.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+# Instructions  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+Follow the steps below exactly and remain strictly neutral:</span><br><span style=\"\"> </span><br><span style=\"background:#f8d7da;color:#b31d28\">-All criteria should be considered equally unless the userâ€™s instructions indicate otherwise. </span><br><span style=\"background:#d2f5d6;color:#22863a\">+1. Read the User Question and both assistant answers in full.  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+2. Evaluate each answer against **all** six criteria, treating them with equal weight unless the user explicitly states otherwise:</span><br><span style=\"background:#d2f5d6;color:#22863a\">+   â€¢ Helpfulness â€“ Does the response address the userâ€™s needs and provide useful information?  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+   â€¢ Relevance â€“ How closely does the response pertain to the userâ€™s question and instructions?  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+   â€¢ Accuracy â€“ Is the information correct and factually reliable?  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+   â€¢ Depth â€“ Does the answer show insight, explanation, or reasoning?  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+   â€¢ Creativity â€“ Is the approach original or resourceful when appropriate?  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+   â€¢ Level of Detail â€“ Is the response thorough and complete?  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+3. Stay impartial:  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+   â€¢ Ignore the order in which the answers appear.  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+   â€¢ Ignore the length of each answer.  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+   â€¢ Ignore the assistantsâ€™ names.  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+4. Make your decision solely on how well each response meets the criteria above.  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+5. After your analysis, produce a final verdict using the exact format in the Output Format section.</span><br><span style=\"\"> </span><br><span style=\"background:#f8d7da;color:#b31d28\">-Begin your evaluation by comparing the two responses according to these criteria and provide a short explanation. Remain impartial by avoiding any position biases and ensuring that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses or the names of the assistants to influence your evaluation.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+# Reasoning Steps</span><br><span style=\"background:#d2f5d6;color:#22863a\">+Think step by step:</span><br><span style=\"background:#d2f5d6;color:#22863a\">+1. For each criterion, briefly note strengths and weaknesses for Assistant A.  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+2. Repeat for Assistant B.  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+3. Compare the two sets of notes criterion by criterion.  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+4. Decide which answer is overall superior, or declare a tie if both are equally strong across all criteria.</span><br><span style=\"\"> </span><br><span style=\"background:#f8d7da;color:#b31d28\">-Be as objective as possible by strictly adhering to the defined criteria above and basing your judgment solely on how well each response meets them.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+# Output Format</span><br><span style=\"background:#d2f5d6;color:#22863a\">+First provide a short, objective explanation (1â€“3 concise paragraphs).  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+Then on a new line output only one of the following tokens (without quotes or extra text):</span><br><span style=\"background:#d2f5d6;color:#22863a\">+â€¢ [[A]]  â€“ if Assistant A is better  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+â€¢ [[B]]  â€“ if Assistant B is better  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+â€¢ [[C]]  â€“ if it is a tie  </span><br><span style=\"\"> </span><br><span style=\"background:#f8d7da;color:#b31d28\">-After providing your explanation, output your final verdict by strictly following this format: &quot;[[A]]&quot; if assistant A is better, &quot;[[B]]&quot; if assistant B is better, and &quot;[[C]]&quot; for a tie. Choose &quot;[[C]]&quot; only if both responses are equally strong across all criteria.</span><br><span style=\"background:#f8d7da;color:#b31d28\">-</span><br><span style=\"background:#f8d7da;color:#b31d28\">-[User Question]</span><br><span style=\"background:#d2f5d6;color:#22863a\">+# Context (inserted at runtime)</span><br><span style=\"background:#d2f5d6;color:#22863a\">+[User Question]  </span><br><span style=\"\"> {question}</span><br><span style=\"\"> </span><br><span style=\"background:#f8d7da;color:#b31d28\">-[The Start of Assistant Aâ€™s Answer]</span><br><span style=\"background:#f8d7da;color:#b31d28\">-{answer_a}</span><br><span style=\"background:#d2f5d6;color:#22863a\">+[The Start of Assistant Aâ€™s Answer]  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+{answer_a}  </span><br><span style=\"\"> [The End of Assistant Aâ€™s Answer]</span><br><span style=\"\"> </span><br><span style=\"background:#f8d7da;color:#b31d28\">-[The Start of Assistant Bâ€™s Answer]</span><br><span style=\"background:#f8d7da;color:#b31d28\">-{answer_b}</span><br><span style=\"background:#d2f5d6;color:#22863a\">+[The Start of Assistant Bâ€™s Answer]  </span><br><span style=\"background:#d2f5d6;color:#22863a\">+{answer_b}  </span><br><span style=\"\"> [The End of Assistant Bâ€™s Answer]</span></div></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_critique_and_diff(revised_prompt, improved_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
