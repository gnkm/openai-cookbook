{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 長文書コンテンツの抽出\n",
    "\n",
    "GPT-3は、コンテキストウィンドウに収まらない大きすぎる文書から、重要な数値、日付、その他の重要なコンテンツを抽出するのに役立ちます。これを解決するアプローチの一つは、文書をチャンクに分割し、各チャンクを個別に処理してから、一つの回答リストに結合することです。\n",
    "\n",
    "このノートブックでは、このアプローチを実行していきます：\n",
    "- 長いPDFを読み込み、テキストを抽出する\n",
    "- 重要な情報を抽出するために使用するプロンプトを作成する\n",
    "- 文書をチャンクに分割し、各チャンクを処理して回答を抽出する\n",
    "- 最後にそれらを結合する\n",
    "- このシンプルなアプローチを、より困難な3つの質問に拡張する\n",
    "\n",
    "## アプローチ\n",
    "\n",
    "- **セットアップ**: PDFファイル（パワーユニットに関するフォーミュラ1財務規則文書）を取得し、エンティティ抽出のためにテキストを抽出します。これを使用して、コンテンツに埋もれている回答を抽出することを試みます。\n",
    "- **シンプルなエンティティ抽出**: 以下の方法で文書のチャンクから重要な情報を抽出します：\n",
    "    - 質問と期待する形式の例を含むテンプレートプロンプトを作成する\n",
    "    - テキストのチャンクを入力として受け取り、プロンプトと組み合わせて応答を取得する関数を作成する\n",
    "    - テキストをチャンクに分割し、回答を抽出して解析用に出力するスクリプトを実行する\n",
    "- **複雑なエンティティ抽出**: より困難な推論を必要とする、より難しい質問をする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textract\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "\n",
    "# Extract the raw text from each PDF using textract\n",
    "text = textract.process('data/fia_f1_power_unit_financial_regulations_issue_1_-_2022-08-16.pdf', method='pdfminer').decode('utf-8')\n",
    "clean_text = text.replace(\"  \", \" \").replace(\"\\n\", \"; \").replace(';',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## シンプルなエンティティ抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract key pieces of information from this regulation document.\n",
      "If a particular piece of information is not present, output \"Not specified\".\n",
      "When you extract a key piece of information, include the closest page number.\n",
      "Use the following format:\n",
      "0. Who is the author\n",
      "1. What is the amount of the \"Power Unit Cost Cap\" in USD, GBP and EUR\n",
      "2. What is the value of External Manufacturing Costs in USD\n",
      "3. What is the Capital Expenditure Limit in USD\n",
      "\n",
      "Document: \"\"\"<document>\"\"\"\n",
      "\n",
      "0. Who is the author: Tom Anderson (Page 1)\n",
      "1.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt - \n",
    "document = '<document>'\n",
    "template_prompt=f'''Extract key pieces of information from this regulation document.\n",
    "If a particular piece of information is not present, output \\\"Not specified\\\".\n",
    "When you extract a key piece of information, include the closest page number.\n",
    "Use the following format:\\n0. Who is the author\\n1. What is the amount of the \"Power Unit Cost Cap\" in USD, GBP and EUR\\n2. What is the value of External Manufacturing Costs in USD\\n3. What is the Capital Expenditure Limit in USD\\n\\nDocument: \\\"\\\"\\\"<document>\\\"\\\"\\\"\\n\\n0. Who is the author: Tom Anderson (Page 1)\\n1.'''\n",
    "print(template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \"\"\"Yield successive n-sized chunks from text.\"\"\"\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "def extract_chunk(document,template_prompt):\n",
    "    prompt = template_prompt.replace('<document>',document)\n",
    "\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You help extract information from documents.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model='gpt-4', \n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            max_tokens=1500,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )\n",
    "    return \"1.\" + response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "results = []\n",
    "    \n",
    "chunks = create_chunks(clean_text,1000,tokenizer)\n",
    "text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "for chunk in text_chunks:\n",
    "    results.append(extract_chunk(chunk,template_prompt))\n",
    "    #print(chunk)\n",
    "    print(results[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the amount of the \"Power Unit Cost Cap\" in USD, GBP and EUR: USD 95,000,000 (Page 2); GBP 76,459,000 (Page 2); EUR 90,210,000 (Page 2)',\n",
       " '2. What is the value of External Manufacturing Costs in USD: US Dollars 20,000,000 in respect of each of the Full Year Reporting Periods ending on 31 December 2023, 31 December 2024 and 31 December 2025, adjusted for Indexation (Page 10)',\n",
       " '3. What is the Capital Expenditure Limit in USD: US Dollars 30,000,000 (Page 32)']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = [r.split('\\n') for r in results]\n",
    "\n",
    "# zip the groups together\n",
    "zipped = list(zip(*groups))\n",
    "zipped = [x for y in zipped for x in y if \"Not specified\" not in x and \"__\" not in x]\n",
    "zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複雑なエンティティ抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract key pieces of information from this regulation document.\n",
      "If a particular piece of information is not present, output \"Not specified\".\n",
      "When you extract a key piece of information, include the closest page number.\n",
      "Use the following format:\n",
      "0. Who is the author\n",
      "1. How is a Minor Overspend Breach calculated\n",
      "2. How is a Major Overspend Breach calculated\n",
      "3. Which years do these financial regulations apply to\n",
      "\n",
      "Document: \"\"\"<document>\"\"\"\n",
      "\n",
      "0. Who is the author: Tom Anderson (Page 1)\n",
      "1.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt - \n",
    "template_prompt=f'''Extract key pieces of information from this regulation document.\n",
    "If a particular piece of information is not present, output \\\"Not specified\\\".\n",
    "When you extract a key piece of information, include the closest page number.\n",
    "Use the following format:\\n0. Who is the author\\n1. How is a Minor Overspend Breach calculated\\n2. How is a Major Overspend Breach calculated\\n3. Which years do these financial regulations apply to\\n\\nDocument: \\\"\\\"\\\"<document>\\\"\\\"\\\"\\n\\n0. Who is the author: Tom Anderson (Page 1)\\n1.'''\n",
    "print(template_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. How is a Minor Overspend Breach calculated: A Minor Overspend Breach arises when a Power Unit Manufacturer submits its Full Year Reporting Documentation and Relevant Costs reported therein exceed the Power Unit Cost Cap by less than 5% (Page 24)',\n",
       " '2. How is a Major Overspend Breach calculated: A Material Overspend Breach arises when a Power Unit Manufacturer submits its Full Year Reporting Documentation and Relevant Costs reported therein exceed the Power Unit Cost Cap by 5% or more (Page 25)',\n",
       " '3. Which years do these financial regulations apply to: 2026 onwards (Page 1)',\n",
       " '3. Which years do these financial regulations apply to: 2023, 2024, 2025, 2026 and subsequent Full Year Reporting Periods (Page 2)',\n",
       " '3. Which years do these financial regulations apply to: 2022-2025 (Page 6)',\n",
       " '3. Which years do these financial regulations apply to: 2023, 2024, 2025, 2026 and subsequent Full Year Reporting Periods (Page 10)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 14)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 16)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 19)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 21)',\n",
       " '3. Which years do these financial regulations apply to: 2026 onwards (Page 26)',\n",
       " '3. Which years do these financial regulations apply to: 2026 (Page 2)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 30)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 32)',\n",
       " '3. Which years do these financial regulations apply to: 2023, 2024 and 2025 (Page 1)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 37)',\n",
       " '3. Which years do these financial regulations apply to: 2026 onwards (Page 40)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 1)',\n",
       " '3. Which years do these financial regulations apply to: 2026 to 2030 seasons (Page 46)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 47)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 1)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 1)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 56)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 1)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 16)',\n",
       " '3. Which years do these financial regulations apply to: 2022 (Page 16)']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for chunk in text_chunks:\n",
    "    results.append(extract_chunk(chunk,template_prompt))\n",
    "    \n",
    "groups = [r.split('\\n') for r in results]\n",
    "\n",
    "# zip the groups together\n",
    "zipped = list(zip(*groups))\n",
    "zipped = [x for y in zipped for x in y if \"Not specified\" not in x and \"__\" not in x]\n",
    "zipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 統合\n",
    "\n",
    "最初の2つの回答は安全に抽出することができましたが、3つ目は各ページに表示される日付によって混乱しました。ただし、正しい回答もそこに含まれています。\n",
    "\n",
    "これをさらに調整するために、以下の実験を検討できます：\n",
    "- より説明的または具体的なプロンプト\n",
    "- 十分な訓練データがある場合、一連の出力を非常によく見つけるためのモデルのファインチューニング\n",
    "- データをチャンクする方法 - 私たちは重複なしで1000トークンを使用しましたが、情報をセクションに分割したり、トークンで切り分けるなど、より知的なチャンキングによってより良い結果が得られる可能性があります\n",
    "\n",
    "しかし、最小限の調整で、長い文書の内容を使用してさまざまな難易度の6つの質問に回答し、エンティティ抽出を必要とする任意の長い文書に適用できる再利用可能なアプローチを手に入れました。あなたがこれで何ができるかを楽しみにしています！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
