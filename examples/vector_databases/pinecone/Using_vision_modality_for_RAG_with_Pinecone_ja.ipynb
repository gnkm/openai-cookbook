{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411bc0cd6d650113",
   "metadata": {},
   "source": [
    "## GPT-4o Vision Modalityを使用したRetrieval-Augmented Generationの最適化\n",
    "\n",
    "画像、グラフィック、表が豊富に含まれる文書を扱う際、Retrieval-Augmented Generation（RAG）の実装には独特の課題があります。従来のRAGモデルはテキストデータでは優れた性能を発揮しますが、視覚的要素が情報伝達において重要な役割を果たす場合には、しばしば不十分な結果となります。このクックブックでは、vision modalityを活用して視覚的コンテンツを抽出・解釈することで、そのギャップを埋め、生成される応答が可能な限り情報豊富で正確になるようにします。\n",
    "\n",
    "私たちのアプローチでは、文書を画像に解析し、メタデータタグを使用して画像、グラフィック、表を含むページを識別します。セマンティック検索でそのようなページが取得された場合、テキストのみに依存するのではなく、ページ画像をvision modelに渡します。この方法により、視覚的データに関するユーザークエリを理解し、回答するモデルの能力が向上します。\n",
    "\n",
    "このクックブックでは、以下の主要概念を探求し、実演します：\n",
    "\n",
    "##### 1. Pineconeを使用したVector Storeの設定：\n",
    "- ベクター埋め込みを効率的に保存するためのPineconeの初期化と設定方法を学習します。\n",
    "\n",
    "##### 2. PDFの解析と視覚情報の抽出：\n",
    "- PDFページを画像に変換する技術を発見します。\n",
    "- GPT-4o vision modalityを使用して、画像、グラフィック、表を含むページからテキスト情報を抽出します。\n",
    "\n",
    "##### 3. 埋め込みの生成：\n",
    "- 埋め込みモデルを活用してテキストデータのベクター表現を作成します。\n",
    "- 視覚的コンテンツを持つページにフラグを立て、vector storeにメタデータフラグを設定し、GPT-4o vision modalityに渡すための画像を取得します。\n",
    "\n",
    "##### 4. Pineconeへの埋め込みのアップロード：\n",
    "- これらの埋め込みを保存と検索のためにPineconeにアップロードします。\n",
    "\n",
    "##### 5. 関連ページのセマンティック検索の実行：\n",
    "- ページテキストでセマンティック検索を実装し、ユーザーのクエリに最も適合するページを見つけます。\n",
    "- 一致するページテキストをGPT-4oにコンテキストとして提供し、ユーザーのクエリに回答します。\n",
    "\n",
    "##### 6. 視覚的コンテンツを含むページの処理（オプションステップ）：\n",
    "- 追加のコンテキストを使用した質問応答のために、GPT-4o vision modalityを使用して画像を渡す方法を学習します。\n",
    "- このプロセスが視覚的データに関する応答の精度をどのように向上させるかを理解します。\n",
    "\n",
    "このクックブックの終わりまでに、複雑な視覚的要素を含む文書を処理・解釈できるRAGシステムの実装について、堅実な理解を得ることができます。この知識により、より豊富で正確な情報を提供するAIソリューションを構築し、ユーザーの満足度とエンゲージメントを向上させることができるようになります。\n",
    "\n",
    "概念を説明するために、World Bank報告書 - [A Better Bank for a Better World: Annual Report 2024](https://documents1.worldbank.org/curated/en/099101824180532047/pdf/BOSIB13bdde89d07f1b3711dd8e86adb477.pdf) を使用します。この文書には画像、表、グラフィックデータが混在しているためです。\n",
    "\n",
    "Vision Modalityの使用はリソース集約的であり、レイテンシとコストの増加につながることを念頭に置いてください。プレーンテキスト抽出方法では評価ベンチマークでの性能が不十分な場合にのみ、Vision Modalityを使用することをお勧めします。この背景を踏まえて、詳しく見ていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1a7f1a653ef32",
   "metadata": {},
   "source": [
    "### ステップ1: Pineconeを使用したベクターストアの設定\n",
    "このセクションでは、Pineconeを使用してベクターストアを設定し、埋め込みを効率的に保存・管理します。Pineconeは高次元ベクターデータの処理に最適化されたベクターデータベースで、セマンティック検索や類似性マッチングなどのタスクに不可欠です。\n",
    "\n",
    "**前提条件**\n",
    "1. Pineconeにサインアップし、こちらの手順に従ってAPIキーを取得してください [Pinecone Database Quickstart](https://docs.pinecone.io/guides/get-started/quickstart)\n",
    "2. `pip install \"pinecone[grpc]\"`を使用してPinecone SDKをインストールしてください。gRPC（gRPC Remote Procedure Call）は、HTTP/2をトランスポートに使用し、Protocol Buffers（protobuf）をインターフェース定義言語として使用し、分散システムでクライアント・サーバー間通信を可能にする高性能でオープンソースの汎用RPCフレームワークです。マイクロサービスアーキテクチャに適したサービス間通信をより効率的にするように設計されています。\n",
    "\n",
    "**APIキーの安全な保存**\n",
    "1. セキュリティ上の理由から、プロジェクトディレクトリの.envファイルにAPIキーを以下のように保存してください：\n",
    " `PINECONE_API_KEY=your-api-key-here`\n",
    " 2. .envファイルからAPIキーを読み取るために`pip install python-dotenv`をインストールしてください。\n",
    "\n",
    "**Pineconeインデックスの作成**\n",
    "Pinecone上で埋め込みデータベースを初期化するために`create_index`関数を使用します。考慮すべき重要なパラメータが2つあります：\n",
    "\n",
    "1. Dimension（次元数）：これは選択したモデルが生成する埋め込みの次元数と一致する必要があります。例えば、OpenAIのtext-embedding-ada-002モデルは1536次元の埋め込みを生成し、text-embedding-3-largeは3072次元の埋め込みを生成します。このクックブックではtext-embedding-3-largeモデルを使用するため、次元数を3072に設定します。\n",
    "\n",
    "2. Metric（メトリック）：距離メトリックは、ベクター間の類似性がどのように計算されるかを決定します。Pineconeはcosine、dotproduct、euclideanなど複数のメトリックをサポートしています。このクックブックでは、コサイン類似度メトリックを使用します。距離メトリックについて詳しくは、[Pinecone Distance Metrics documentation](https://docs.pinecone.io/guides/indexes/understanding-indexes#distance-metrics)をご覧ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698c0591038c8d47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T18:26:46.653603Z",
     "start_time": "2024-11-13T18:26:46.215121Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "# Import the Pinecone library\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Initialize a Pinecone client with your API key\n",
    "pc = Pinecone(api_key)\n",
    "\n",
    "# Create a serverless index\n",
    "index_name = \"my-test-index\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Wait for the index to be ready\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5542a36f05623c",
   "metadata": {},
   "source": [
    "[Pinecone](https://app.pinecone.io/)のIndexesリストに移動すると、インデックスのリストに`my-test-index`が表示されるはずです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e8baf98bba582",
   "metadata": {},
   "source": [
    "### ステップ2: PDFの解析と視覚的情報の抽出:\n",
    "\n",
    "このセクションでは、世界銀行のレポート - [A Better Bank for a Better World: Annual Report 2024](https://documents1.worldbank.org/curated/en/099101824180532047/pdf/BOSIB13bdde89d07f1b3711dd8e86adb477.pdf) のPDF文書を解析し、画像、グラフィック、表の説明など、テキストと視覚的情報を抽出します。このプロセスには3つの主要なステップが含まれます：\n",
    "\n",
    "1. **PDFを個別のページに解析する:** 処理を簡単にするため、PDFを個別のページに分割します。\n",
    "2. **PDFページを画像に変換する:** これにより、GPT-4oのビジョン機能がページを画像として分析できるようになります。\n",
    "3. **画像と表を処理する:** GPT-4oにテキストを抽出し、文書内の画像、グラフィック、表を説明するよう指示を提供します。\n",
    "\n",
    "**前提条件**\n",
    "\n",
    "続行する前に、以下のパッケージがインストールされていることを確認してください。また、OpenAI APIキーが環境変数として設定されていることを確認してください。PDF レンダリングのためにPopplerのインストールも必要な場合があります。\n",
    "\n",
    "`pip install PyPDF2 pdf2image pytesseract pandas tqdm`\n",
    "\n",
    "**ステップの詳細:**\n",
    "\n",
    "**1. PDFのダウンロードとチャンク化:**\n",
    "- `chunk_document`関数は、提供されたURLからPDFをダウンロードし、PyPDF2を使用して個別のページに分割します。\n",
    "- 各ページは、リスト内の個別のPDFバイトストリームとして保存されます。\n",
    "\n",
    "**2. PDFページの画像への変換:**\n",
    "- `convert_page_to_image`関数は、単一ページのPDFバイトを受け取り、pdf2imageを使用して画像に変換します。\n",
    "- 画像は、さらなる処理のために'images'ディレクトリにローカルに保存されます。\n",
    "\n",
    "**3. GPT-4oビジョンモダリティを使用したテキスト抽出:**\n",
    "- `extract_text_from_image`関数は、GPT-4oのビジョン機能を使用してページの画像からテキストを抽出します。\n",
    "- この方法は、スキャンされた文書からでもテキスト情報を抽出できます。\n",
    "- このモダリティはリソース集約的であるため、より高いレイテンシとコストが関連することに注意してください。\n",
    "\n",
    "**4. 文書全体の処理:**\n",
    "- `process_document`関数は、各ページの処理を統括します。\n",
    "- 処理状況を表示するためにプログレスバー（tqdm）を使用します。\n",
    "- 各ページから抽出された情報はリストに収集され、その後Pandas DataFrameに変換されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a58943248b3eb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T18:26:03.723761Z",
     "start_time": "2024-11-13T18:07:06.051707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document processing started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pages: 100%|██████████| 49/49 [18:54<00:00, 23.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document processing completed.\n",
      "DataFrame created with page data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from pdf2image import convert_from_bytes\n",
    "from io import BytesIO\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Link to the document we will use as the example \n",
    "document_to_parse = \"https://documents1.worldbank.org/curated/en/099101824180532047/pdf/BOSIB13bdde89d07f1b3711dd8e86adb477.pdf\"\n",
    "\n",
    "# OpenAI client \n",
    "oai_client = OpenAI()\n",
    "\n",
    "\n",
    "# Chunk the PDF document into single page chunks \n",
    "def chunk_document(document_url):\n",
    "    # Download the PDF document\n",
    "    response = requests.get(document_url)\n",
    "    pdf_data = response.content\n",
    "\n",
    "    # Read the PDF data using PyPDF2\n",
    "    pdf_reader = PdfReader(BytesIO(pdf_data))\n",
    "    page_chunks = []\n",
    "\n",
    "    for page_number, page in enumerate(pdf_reader.pages, start=1):\n",
    "        pdf_writer = PdfWriter()\n",
    "        pdf_writer.add_page(page)\n",
    "        pdf_bytes_io = BytesIO()\n",
    "        pdf_writer.write(pdf_bytes_io)\n",
    "        pdf_bytes_io.seek(0)\n",
    "        pdf_bytes = pdf_bytes_io.read()\n",
    "        page_chunk = {\n",
    "            'pageNumber': page_number,\n",
    "            'pdfBytes': pdf_bytes\n",
    "        }\n",
    "        page_chunks.append(page_chunk)\n",
    "\n",
    "    return page_chunks\n",
    "\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(local_image_path):\n",
    "    with open(local_image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "# Function to convert page to image     \n",
    "def convert_page_to_image(pdf_bytes, page_number):\n",
    "    # Convert the PDF page to an image\n",
    "    images = convert_from_bytes(pdf_bytes)\n",
    "    image = images[0]  # There should be only one page\n",
    "\n",
    "    # Define the directory to save images (relative to your script)\n",
    "    images_dir = 'images'  # Use relative path here\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "    # Save the image to the images directory\n",
    "    image_file_name = f\"page_{page_number}.png\"\n",
    "    image_file_path = os.path.join(images_dir, image_file_name)\n",
    "    image.save(image_file_path, 'PNG')\n",
    "\n",
    "    # Return the relative image path\n",
    "    return image_file_path\n",
    "\n",
    "\n",
    "# Pass the image to the LLM for interpretation  \n",
    "def get_vision_response(prompt, image_path):\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(image_path)\n",
    "\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "# Process document function that brings it all together \n",
    "def process_document(document_url):\n",
    "    try:\n",
    "        # Update document status to 'Processing'\n",
    "        print(\"Document processing started\")\n",
    "\n",
    "        # Get per-page chunks\n",
    "        page_chunks = chunk_document(document_url)\n",
    "        total_pages = len(page_chunks)\n",
    "\n",
    "        # Prepare a list to collect page data\n",
    "        page_data_list = []\n",
    "\n",
    "        # Add progress bar here\n",
    "        for page_chunk in tqdm(page_chunks, total=total_pages, desc='Processing Pages'):\n",
    "            page_number = page_chunk['pageNumber']\n",
    "            pdf_bytes = page_chunk['pdfBytes']\n",
    "\n",
    "            # Convert page to image\n",
    "            image_path = convert_page_to_image(pdf_bytes, page_number)\n",
    "\n",
    "            # Prepare question for vision API\n",
    "            system_prompt = (\n",
    "                \"The user will provide you an image of a document file. Perform the following actions: \"\n",
    "                \"1. Transcribe the text on the page. **TRANSCRIPTION OF THE TEXT:**\"\n",
    "                \"2. If there is a chart, describe the image and include the text **DESCRIPTION OF THE IMAGE OR CHART**\"\n",
    "                \"3. If there is a table, transcribe the table and include the text **TRANSCRIPTION OF THE TABLE**\"\n",
    "            )\n",
    "\n",
    "            # Get vision API response\n",
    "            vision_response = get_vision_response(system_prompt, image_path)\n",
    "\n",
    "            # Extract text from vision response\n",
    "            text = vision_response.choices[0].message.content\n",
    "\n",
    "            # Collect page data\n",
    "            page_data = {\n",
    "                'PageNumber': page_number,\n",
    "                'ImagePath': image_path,\n",
    "                'PageText': text\n",
    "            }\n",
    "            page_data_list.append(page_data)\n",
    "\n",
    "        # Create DataFrame from page data\n",
    "        pdf_df = pd.DataFrame(page_data_list)\n",
    "        print(\"Document processing completed.\")\n",
    "        print(\"DataFrame created with page data.\")\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return pdf_df\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"Error processing document: {err}\")\n",
    "        # Update document status to 'Error'\n",
    "\n",
    "\n",
    "df = process_document(document_to_parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f760dd52b8ad996",
   "metadata": {},
   "source": [
    "DataFrameを調べて、ページが正しく処理されていることを確認しましょう。簡潔にするため、最初の5行のみを取得して表示します。また、'images'ディレクトリに生成されたページ画像も確認できるはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba17abe814692406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T18:26:52.923884Z",
     "start_time": "2024-11-13T18:26:52.916585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PageNumber</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>PageText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>images/page_1.png</td>\n",
       "      <td>**TRANSCRIPTION OF THE TEXT:**\\n\\nPublic Disclosure Authorized  \\nPublic Disclosure Authorized  \\nA BETTER BANK FOR A BETTER WORLD  \\nANNUAL REPORT 2024  \\nWORLD BANK GROUP  \\nIBRD · IDA  \\n\\n**DESCRIPTION OF THE IMAGE OR CHART:**\\n\\nThe image features a nighttime scene with a makeshift shelter illuminated from within. The shelter appears to be made of fabric and has patterns on it. Inside, there are people visible through the opening, and some items such as shoes can be seen on the ground outside the shelter. The setting suggests a community or family environment under starlit skies. The circular graphic elements overlaying the image may imply interconnectedness or global outreach.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>images/page_2.png</td>\n",
       "      <td>**TRANSCRIPTION OF THE TEXT:**\\n\\nCONTENTS\\n\\nMessage from the President           6  \\nMessage from the Executive Directors  8  \\nBecoming a Better Bank                        10  \\nFiscal 2024 Financial Summary             12  \\nResults by Region                               14  \\nResults by Theme                              44  \\nHow We Work                                  68  \\n\\nKEY TABLES\\nIBRD Key Financial Indicators, Fiscal 2020–24      84  \\nIDA Key Financial Indicators, Fiscal 2020–24      88  \\n\\nThis annual report, which covers the period from July 1, 2023, to June 30, 2024, has been prepared by the Executive Directors of both the International Bank for Reconstruction and Development (IBRD) and the International Development Association (IDA)—collectively known as the World Bank—in accordance with the respective bylaws of the two institutions. Ajay Banga, President of the World Bank Group and Chairman of the Board of Executive Directors, has submitted this report, together with the accompanying administrative budgets and audited financial statements, to the Board of Governors.\\n\\nAnnual reports for the other World Bank Group institutions—the International Finance Corporation (IFC), the Multilateral Investment Guarantee Agency (MIGA), and the International Centre for Settlement of Investment Disputes (ICSID)—are published separately. Key highlights from each institution's annual report are available in the World Bank Group Annual Report Summary.\\n\\nThroughout the report, the term World Bank and the abbreviated Bank refer only to IBRD and IDA; the term World Bank Group and the abbreviated Bank Group refer to the five institutions. All dollar amounts used in this report are current U.S. dollars unless otherwise specified. Funds allocated to multiregional projects are accounted for by recipient country where possible in tables and text when referring to regional breakdowns. For sector and theme breakdowns, funds are accounted for by operation. Fiscal year commitments and disbursements data are in accordance with the audited figures reported in the IBRD and IDA Financial Statements and Management's Discussion and Analysis documents for Fiscal 2024. As a result of rounding, numbers in tables may not add to totals, and percentages in figures may not add to 100.\\n\\n**DESCRIPTION OF THE IMAGE OR CHART**\\n\\nThe image shows a close-up of a hand holding a bundle of rice plants, with golden stalks of rice grains. The background is blurred, showing more rice fields.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>images/page_3.png</td>\n",
       "      <td>**TRANSCRIPTION OF THE TEXT:**\\n\\nABOUT US\\n\\nThe World Bank Group is one of the world’s largest sources of funding and knowledge for developing countries. Our five institutions share a commitment to reducing poverty, increasing shared prosperity, and promoting sustainable development.\\n\\nOUR VISION  \\nOur vision is to create a world free of poverty on a livable planet.\\n\\nOUR MISSION  \\nOur mission is to end extreme poverty and boost shared prosperity on a livable planet. This is threatened by multiple, intertwined crises. Time is of the essence. We are building a better Bank to drive impactful development that is:  \\n• Inclusive of everyone, including women and young people;  \\n• Resilient to shocks, including against climate and biodiversity crises, pandemics and fragility;  \\n• Sustainable, through growth and job creation, human development, fiscal and debt management, food security and access to clean air, water, and affordable energy.\\n\\nTo achieve this, we will work with all clients as one World Bank Group, in close partnership with other multilateral institutions, the private sector, and civil society.\\n\\nOUR CORE VALUES  \\nOur work is guided by our core values: impact, integrity, respect, teamwork, and innovation. These inform everything we do, everywhere we work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>images/page_4.png</td>\n",
       "      <td>**TRANSCRIPTION OF THE TEXT:**\\n\\nDRIVING ACTION, MEASURING RESULTS\\n\\nThe World Bank Group contributes to impactful, meaningful development results around the world. In the first half of fiscal 2024*, we:\\n\\n- Helped feed 156 million people\\n- Improved schooling for 280 million students\\n- Reached 287 million people living in poverty with effective social protection support†\\n- Provided healthy water, sanitation, and/or hygiene to 59 million people\\n- Enabled access to sustainable transportation for 77 million people\\n- Provided 17 gigawatts of renewable energy capacity\\n- Committed to devote 45 percent of annual financing to climate action by 2025, deployed equally between mitigation and adaptation\\n\\n*The development of the new Scorecard is ongoing at the time of printing; therefore, this report can only account for results up to December 31, 2023.\\nAs of the 2024 IMF-World Bank Group Annual Meetings, the full fiscal 2024 Scorecard data will be available at: https://scorecard.worldbankgroup.org\\n\\n† IBRD and IDA only indicator.\\n\\nIn fiscal 2024, the Bank Group announced the development of a new Scorecard that will track results across 22 indicators—a fraction of the previous 150—to provide a streamlined, clear picture of progress on all aspects of the Bank Group’s mission, from improving access to healthcare to making food systems sustainable to boosting private investment.\\n\\nFor the first time, the work of all Bank Group financing institutions will be tracked through the same set of indicators. The new Scorecard will track the Bank Group’s overarching vision of ending poverty on a livable planet.\\n\\nTHE WORLD BANK ANNUAL REPORT 2024\\n\\n**DESCRIPTION OF THE IMAGE OR CHART:**\\n\\nThe image displays a series of circular photographs connected with text highlights depicting World Bank Group achievements. The photos include people and infrastructure related to food, education, social protection, water, transportation, renewable energy, and environmental initiatives. Each photo correlates with a text entry describing a specific achievement or commitment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>images/page_5.png</td>\n",
       "      <td>**TRANSCRIPTION OF THE TEXT:**\\n\\nMESSAGE FROM THE PRESIDENT\\n\\nDELIVERING ON OUR COMMITMENTS REQUIRES US TO DEVELOP NEW AND BETTER WAYS OF WORKING. IN FISCAL 2024, WE DID JUST THAT.\\n\\nAJAY BANGA\\n\\nIn fiscal 2024, the World Bank Group adopted a bold new vision of a world free of poverty on a livable planet. To achieve this, the Bank Group is enacting reforms to become a better partner to governments, the private sector, and, ultimately, the people we serve. Rarely in our 80-year history has our work been more urgent: We face declining progress in our fight against poverty, an existential climate crisis, mounting public debt, food insecurity, an unequal pandemic recovery, and the effects of geopolitical conflict.\\n\\nResponding to these intertwined challenges requires a faster, simpler, and more efficient World Bank Group. We are refocusing to confront these challenges not just through funding, but with knowledge. Our Knowledge Compact for Action, published in fiscal 2024, details how we will empower all Bank Group clients, public and private, by making our wealth of development knowledge more accessible. And we have reorganized the World Bank’s global practices into five Vice Presidency units—People, Prosperity, Planet, Infrastructure, and Digital—for more flexible and faster engagements with clients. Each of these units reached important milestones in fiscal 2024.\\n\\nWe are supporting countries in delivering quality, affordable health services to 1.5 billion people by 2030 so our children and grandchildren will lead healthier, better lives. This is part of our larger global effort to address a basic standard of care through every stage of a person’s life—infancy, childhood, adolescence, and adulthood. To help people withstand food-affected shocks and crises, we are strengthening social protection services to support half a billion people by the end of 2030—aiming for half of these beneficiaries to be women.\\n\\nWe are helping developing countries create jobs and employment, the surest enablers of prosperity. In the next 10 years, 1.2 billion young people across the Global South will become working-age adults. Yet, in the same period and the same countries, only 424 million jobs are expected to be created. The cost of hundreds of millions of young people with no hope for a decent job or future is unimaginable, and we are working urgently to create opportunity for all.\\n\\nIn response to climate change—arguably the greatest challenge of our generation—we’re channeling 45 percent of annual financing to climate action by 2025, deployed equally between mitigation and adaptation. Among other efforts, we intend to launch at least 15 country-led methane-reduction programs by fiscal 2026, and our Forest Carbon Partnership Facility has helped strengthen high-integrity carbon markets.\\n\\nAccess to electricity is a fundamental human right and foundational to any successful development effort. It will accelerate the digital development of developing countries, strengthen public infrastructure, and prepare people for the jobs of tomorrow. But half the population of Africa—600 million people—lacks access to electricity. In response, we have committed to provide electricity to 300 million people in Sub-Saharan Africa by 2030 in partnership with the African Development Bank.\\n\\nRecognizing that digitalization is the transformational opportunity of our time, we are collaborating with governments in more than 100 developing countries to enable digital economies. Our digital lending portfolio totaled $6.5 billion in commitments as of June 2024, and our new Digital Vice Presidency unit will guide our efforts to establish the foundations of a digital economy. Key measures include building and enhancing digital and data infrastructure, ensuring cybersecurity and data privacy for institutions, businesses, and citizens, and advancing digital government services.\\n\\nDelivering on our commitments requires us to develop new and better ways of working. In fiscal 2024, we did just that. We are squeezing our balance sheet and finding new opportunities to take more risk and boost our lending. Our new crisis preparedness and response tools, Global Challenge Programs, and Livable Planet Fund demonstrate how we are modernizing our approach to better thrive and meet outcomes. Our new Scorecard radically changes how we track results.\\n\\nBut we cannot deliver alone; we depend on our own. We need partners from both the public and private sectors to join our efforts. That’s why we are working closely with other multilateral development banks to improve the lives of people in developing countries in tangible, measurable ways. Our deepening relationship with the private sector is evidenced by our Private Sector Investment Lab, which is working to address the barriers preventing private sector investment in emerging markets. The Lab’s core group of 15 Chief Executive Officers and Chairs meets regularly, and already has informed our work—most notably with the development of the World Bank Group Guarantee Platform.\\n\\nThe impact and innovations we delivered this year will allow us to move forward with a raised ambition and a greater sense of urgency to improve people’s lives. I would like to recognize the remarkable efforts of our staff and Executive Directors, as well as the unwavering support of our clients and partners. Together, we head into fiscal 2025 with a great sense of optimism—and determination to create a better Bank for a better world.\\n\\nAJAY BANGA  \\nPresident of the World Bank Group  \\nand Chairman of the Board of Executive Directors\\n\\n**DESCRIPTION OF THE IMAGE OR CHART:**\\n\\nThe image shows a group of people engaged in agriculture. One person is holding a tomato, and others are observing. It reflects collaboration or assistance in agricultural practices, possibly in a developing country.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Convert the DataFrame to an HTML table and display top 5 rows \n",
    "display(HTML(df.head().to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb52367eef3222",
   "metadata": {},
   "source": [
    "サンプルページとして、埋め込まれたグラフィックとテキストを含むページ21を見てみましょう。視覚モダリティが視覚情報を効果的に抽出し、記述していることが観察できます。例えば、このページの円グラフは以下のように正確に記述されています：\n",
    "\n",
    "`\"FIGURE 6: MIDDLE EAST AND NORTH AFRICA IBRD AND IDA LENDING BY SECTOR - FISCAL 2024 SHARE OF TOTAL OF $4.6 BILLION\" is a circular chart, resembling a pie chart, illustrating the percentage distribution of funds among different sectors. The sectors include:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a0a3ca3f9012fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T18:26:55.161781Z",
     "start_time": "2024-11-13T18:26:55.156699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**TRANSCRIPTION OF THE TEXT:**\n",
      "\n",
      "We also committed $35 million in grants to support emergency relief in Gaza. Working with the World Food Programme, the World Health Organization, and the UN Children’s Fund, the grants supported the delivery of emergency food, water, and medical supplies. In the West Bank, we approved a $200 million program for the continuation of education for children, $22 million to support municipal services, and $45 million to strengthen healthcare and hospital services.\n",
      "\n",
      "**Enabling green and resilient growth**\n",
      "To help policymakers in the region advance their climate change and development goals, we published Country Climate and Development Reports for the West Bank and Gaza, Lebanon, and Tunisia. In Libya, the catastrophic flooding in September 2023 devastated eastern localities, particularly the city of Derna. The World Bank, together with the UN and the European Union, produced a Rapid Damage and Needs Assessment to inform recovery and reconstruction efforts.\n",
      "\n",
      "We signed a new Memorandum of Understanding (MoU) with the Islamic Development Bank to promote further collaboration between our institutions. The MoU focuses on joint knowledge and operational engagements around the energy, food, and water nexus, climate impact, empowering women and youth to engage with the private sector, and advancing the digital transition and regional integration. The MoU aims to achieve a co-financing value of $6 billion through 2026, 45 percent of which has already been met.\n",
      "\n",
      "**Expanding economic opportunities for women**\n",
      "The World Bank has drawn on a variety of instruments to support Jordan’s commitment to increase female labor force participation, including through the recently approved Country Partnership Framework. Through operations, technical assistance (such as Mashreq Gender Facility; Women Entrepreneurs Finance Initiative; and the Women, Business and the Law report), and policy dialogue, we have contributed to legal reforms in Jordan that removed job restrictions on women, prohibited gender-based discrimination in the workplace, and criminalized sexual harassment in the workplace. In fiscal 2024, we approved the first women-focused Bank project in the region: the Enhancing Women’s Economic Opportunities Program for Results aims to improve workplace conditions, increase financial inclusion and entrepreneurship, make public transport safer, and increase access to affordable, quality childcare services.\n",
      "\n",
      "**Analyzing critical infrastructure needs**\n",
      "We published an Interim Damage Assessment for Gaza in partnership with the UN and with financial support from the EU. This found that a preliminary estimate of the cost of damages to critical infrastructure from the conflict in Gaza between October 2023 and the end of January 2024 was around $18.5 billion—equivalent to 97 percent of the 2022 GDP of the West Bank and Gaza combined. When the situation allows, a full-fledged Rapid Damage and Needs Assessment will be conducted.\n",
      "\n",
      "**COUNTRY IMPACT**\n",
      "\n",
      "Egypt: The Bank-supported Takaful and Karama social protection program has reached 4.7 million vulnerable households, benefitting approximately 20 million individuals, 75 percent of them women.\n",
      "\n",
      "Lebanon: A roads project has rehabilitated over 500 km of roads in 25 districts across the country and generated 1.3 million labor days for Lebanese workers and Syrian refugees.\n",
      "\n",
      "Morocco: Our programs have benefited more than 400,000 people directly and more than 33 million people indirectly, through more than 230 disaster risk reduction projects.\n",
      "\n",
      "**DESCRIPTION OF THE IMAGE OR CHART:**\n",
      "\n",
      "The image is a pie chart titled \"FIGURE 6: MIDDLE EAST AND NORTH AFRICA IBRD AND IDA LENDING BY SECTOR - FISCAL 2024 SHARE OF TOTAL OF $4.6 BILLION.\" The chart breaks down the sectors as follows:\n",
      "- Public Administration: 24%\n",
      "- Social Protection: 13%\n",
      "- Health: 13%\n",
      "- Education: 17%\n",
      "- Agriculture, Fishing, and Forestry: 8%\n",
      "- Water, Sanitation, and Waste Management: 8%\n",
      "- Transportation: 5%\n",
      "- Energy and Extractives: 3%\n",
      "- Financial Sector: 1%\n",
      "- Industry, Trade, and Services: 2%\n",
      "- Information and Communications Technologies: 6%\n",
      "\n",
      "**TRANSCRIPTION OF THE TABLE:**\n",
      "\n",
      "TABLE 13: MIDDLE EAST AND NORTH AFRICA REGIONAL SNAPSHOT\n",
      "\n",
      "| INDICATOR                                                | 2000   | 2012     | CURRENT DATA* |\n",
      "|----------------------------------------------------------|--------|----------|---------------|\n",
      "| Total population (millions)                              | 283.9  | 356.2    | 430.9         |\n",
      "| Population growth (annual %)                             | 2.0    | 1.8      | 1.5           |\n",
      "| GNI per capita (Atlas method, current US$)               | 1,595.5| 4,600.4  | 3,968.1       |\n",
      "| GDP per capita growth (annual %)                         | 4.0    | 1.7      | 1.2           |\n",
      "| Population living below $2.15 a day (millions)           | 9.7    | 8.2      | 19.1          |\n",
      "| Life expectancy at birth, females (years)                | 70.8   | 73.9     | 74.8          |\n",
      "| Life expectancy at birth, males (years)                  | 66.5   | 69.6     | 69.9          |\n",
      "| Carbon dioxide emissions (megatons)                      | 813.2  | 1,297.7  | 1,370.9       |\n",
      "| Extreme poverty (% of population below $2.15 a day, 2017 PPP)| 3.4 | 2.3      | 4.7           |\n",
      "| Debt service as a proportion of exports of goods, services, and primary income | 15.1   | 5.2      | 12.4   |\n",
      "| Ratio of female to male labor force participation rate (%) (modeled ILO estimate) | 24.5   | 26.2     | 23.2   |\n",
      "| Vulnerable employment, total (% of total employment) (modeled ILO estimate) | 35.4   | 31.7     | 31.4   |\n",
      "| Under-5 mortality rate per 1,000 live births             | 46.7   | 29.0     | 20.9          |\n",
      "| Primary completion rate (% of relevant age group)        | 81.4   | 88.9     | 86.7          |\n",
      "| Individuals using the Internet (% of population)         | 0.9    | 26.0     | 73.4          |\n",
      "| Access to electricity (% of population)                  | 91.4   | 94.7     | 96.9          |\n",
      "| Renewable energy consumption (% of total final energy consumption) | 3.0 | 3.6      | 2.9    |\n",
      "| People using at least basic drinking water services (% of population) | 86.5   | 90.6     | 93.7   |\n",
      "| People using at least basic sanitation services (% of population) | 79.4   | 86.2     | 90.4   |\n",
      "\n",
      "*Note: ILO = International Labour Organization. PPP = purchasing power parity. a. The most current data available between 2018 and 2023; visit [https://data.worldbank.org](https://data.worldbank.org) for data updates.\n",
      "\n",
      "For more information, visit [www.worldbank.org/mena](http://www.worldbank.org/mena).\n"
     ]
    }
   ],
   "source": [
    "# Filter and print rows where pageNumber is 21\n",
    "filtered_rows = df[df['PageNumber'] == 21]\n",
    "for text in filtered_rows.PageText:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33919ba1ef16f993",
   "metadata": {},
   "source": [
    "### ステップ3: エンベディングの生成\n",
    "\n",
    "このセクションでは、文書の各ページから抽出されたテキストコンテンツをベクトルエンベディングに変換することに焦点を当てます。これらのエンベディングはテキストの意味的な意味を捉え、効率的な類似性検索や様々な自然言語処理（NLP）タスクを可能にします。また、画像、グラフィック、表などの視覚的要素を含むページを特定し、特別な処理のためにフラグを立てます。\n",
    "\n",
    "**ステップの詳細:**\n",
    "\n",
    "**1. 視覚的コンテンツのフラグ追加**   \n",
    "  \n",
    "視覚的情報を含むページを処理するため、ステップ2では視覚モダリティを使用してチャート、表、画像からコンテンツを抽出しました。プロンプトに特定の指示を含めることで、視覚的コンテンツを説明する際にモデルが`DESCRIPTION OF THE IMAGE OR CHART`や`TRANSCRIPTION OF THE TABLE`などのマーカーを追加することを確実にします。このステップでは、そのようなマーカーが検出された場合、Visual_Input_Processedフラグを'Y'に設定し、そうでなければ'N'のままにします。\n",
    "\n",
    "視覚モダリティは大部分の視覚的情報を効果的に捉えますが、特に工学図面のような複雑な視覚要素では、翻訳過程で一部の詳細が失われる可能性があります。ステップ6では、このフラグを使用して、ページの画像を追加のコンテキストとしてGPT-4 Visionに渡すタイミングを決定します。これはRAGソリューションの効果を大幅に向上させることができるオプションの機能強化です。\n",
    "\n",
    "**2. OpenAIのエンベディングモデルによるエンベディング生成**  \n",
    "\n",
    "各ページの意味的コンテンツを表現する高次元エンベディングを生成するために、OpenAIのエンベディングモデル`text-embedding-3-large`を使用します。\n",
    "\n",
    "注意: 使用するエンベディングモデルの次元数が、Pineconeベクトルストアの設定と一致していることを確認することが重要です。今回の場合、`text-embedding-3-large`のデフォルト次元数に合わせて、Pineconeデータベースを3072次元で設定しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "209bf18a8849b61d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T19:21:34.638823Z",
     "start_time": "2024-11-13T19:21:15.844635Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 49/49 [00:18<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add a column to flag pages with visual content\n",
    "df['Visual_Input_Processed'] = df['PageText'].apply(\n",
    "    lambda x: 'Y' if 'DESCRIPTION OF THE IMAGE OR CHART' in x or 'TRANSCRIPTION OF THE TABLE' in x else 'N'\n",
    ")\n",
    "\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text_input):\n",
    "    response = oai_client.embeddings.create(\n",
    "        input=text_input,\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "# Generate embeddings with a progress bar\n",
    "embeddings = []\n",
    "for text in tqdm(df['PageText'], desc='Generating Embeddings'):\n",
    "    embedding = get_embedding(text)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Add the embeddings to the DataFrame\n",
    "df['Embeddings'] = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1357b68b8cc62a",
   "metadata": {},
   "source": [
    "私たちのロジックが視覚的入力を必要とするページを正しくフラグ付けしたことを確認できます。例えば、以前に調べたページ21では、Visual_Input_Neededフラグが\"Y\"に設定されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "952c187d77892913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T19:21:51.844793Z",
     "start_time": "2024-11-13T19:21:51.840204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20    Y\n",
      "Name: Visual_Input_Processed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Display the flag for page 21 \n",
    "filtered_rows = df[df['PageNumber'] == 21]\n",
    "print(filtered_rows.Visual_Input_Processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb055f21c11d7098",
   "metadata": {},
   "source": [
    "#### ステップ4: Pineconeへの埋め込みのアップロード:\n",
    "\n",
    "このセクションでは、ドキュメントの各ページに対して生成した埋め込みをPineconeにアップロードします。埋め込みと併せて、ページ番号、テキストコンテンツ、画像パス、ページにグラフィックが含まれているかどうかなど、各ページを説明する関連するメタデータタグも含めます。\n",
    "\n",
    "**ステップの詳細:**\n",
    "\n",
    "**1. メタデータフィールドの作成:**\n",
    "メタデータにより、より細かい検索の実行、ベクトルに関連するテキストや画像の検索、ベクトルデータベース内でのフィルタリングが可能になります。\n",
    "* pageId: document_idとpageNumberを組み合わせて、各ページの一意の識別子を作成します。これを埋め込みの一意の識別子として使用します。\n",
    "* pageNumber: ドキュメント内の数値ページ番号。\n",
    "* text: ページから抽出されたテキストコンテンツ。\n",
    "* ImagePath: ページに関連する画像のファイルパス。\n",
    "* GraphicIncluded: ページに視覚的処理が必要なグラフィック要素が含まれているかどうかを示すブール値またはフラグ。\n",
    "\n",
    "**2. 埋め込みのアップロード:**\n",
    "Pinecone APIを使用して、`upsert_vector`関数で値を「upsert」します -\n",
    "\n",
    "* 一意の識別子\n",
    "* 埋め込み\n",
    "* 上記で定義されたメタデータ\n",
    "\n",
    "注意: 「Upsert」は「update」と「insert」の単語を組み合わせたものです。データベース操作において、upsertは既存のレコードが存在する場合は更新し、存在しない場合は新しいレコードを挿入するアトミック操作です。これは、挿入や更新のための個別のチェックを実行することなく、データベースが最新のデータを持つことを確実にしたい場合に特に有用です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "af65e9d39caf979f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T19:22:01.697982Z",
     "start_time": "2024-11-13T19:21:53.418457Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to Pinecone: 100%|██████████| 49/49 [00:08<00:00,  5.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# reload the index from Pinecone \n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Create a document ID prefix \n",
    "document_id = 'WB_Report'\n",
    "\n",
    "\n",
    "# Define the async function correctly\n",
    "def upsert_vector(identifier, embedding, metadata):\n",
    "    try:\n",
    "        index.upsert([\n",
    "            {\n",
    "                'id': identifier,\n",
    "                'values': embedding,\n",
    "                'metadata': metadata\n",
    "            }\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting vector with ID {identifier}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc='Uploading to Pinecone'):\n",
    "    pageNumber = row['PageNumber']\n",
    "\n",
    "    # Create meta-data tags to be added to Pinecone \n",
    "    metadata = {\n",
    "        'pageId': f\"{document_id}-{pageNumber}\",\n",
    "        'pageNumber': pageNumber,\n",
    "        'text': row['PageText'],\n",
    "        'ImagePath': row['ImagePath'],\n",
    "        'GraphicIncluded': row['Visual_Input_Processed']\n",
    "    }\n",
    "\n",
    "    upsert_vector(metadata['pageId'], row['Embeddings'], metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ce7b0e4a8c5a7",
   "metadata": {},
   "source": [
    "[Pinecone](https://app.pinecone.io/)のIndexesリストに移動すると、メタデータと共にデータベースにアップサートされたベクトルを確認できるはずです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8d84f316a85a",
   "metadata": {},
   "source": [
    "### ステップ5: 関連ページのセマンティック検索の実行：\n",
    "このセクションでは、ユーザーの質問に答える最も関連性の高いページを文書内から見つけるためのセマンティック検索を実装します。このアプローチは、Pineconeベクトルデータベースに保存された埋め込みを使用して、ユーザーのクエリとコンテンツのセマンティック類似性に基づいてページを取得します。これにより、テキストコンテンツを効果的に検索し、ユーザーの質問に答えるためのコンテキストとしてGPT-4oに提供することができます。\n",
    "\n",
    "**ステップの詳細：**  \n",
    "\n",
    "**1. ユーザーの質問に対する埋め込みの生成** \n",
    "\n",
    "* OpenAIの埋め込みモデルを使用して、ユーザーの質問の高次元ベクトル表現を生成します。\n",
    "* このベクトルは質問のセマンティックな意味を捉え、保存された埋め込みに対して効率的な類似性検索を実行できるようにします。\n",
    "* 埋め込みは、正確な単語が一致しない場合でも、検索クエリが文書のコンテンツとセマンティックに整合することを保証するために重要です。\n",
    "\n",
    "**2. 関連ページのPineconeインデックスへのクエリ** \n",
    "\n",
    "* 生成された埋め込みを使用して、Pineconeインデックスにクエリを実行し、最も関連性の高いページを見つけます。\n",
    "* Pineconeは、質問の埋め込みとベクトルデータベースに保存された埋め込みを`cosine`類似性を使用して比較することで類似性検索を実行します。思い出していただければ、ステップ1でPineconeデータベースを作成した際に、これを`metric`パラメータとして設定しました。\n",
    "* 取得する上位マッチの数を指定します。これは通常、カバレッジと関連性のバランスに基づいて決定されます。例えば、上位3-5ページを取得することで、モデルに過度なコンテキストを与えることなく包括的な回答を提供するのに十分な場合が多いです。\n",
    "\n",
    "**3. マッチしたページのメタデータをコンパイルしてコンテキストを提供** \n",
    "\n",
    "* 関連する埋め込みが特定されると、抽出されたテキストやページ番号を含む関連メタデータを収集します。\n",
    "* このメタデータは、GPT-4oに提供するコンテキストを構造化するために不可欠です。\n",
    "* また、コンパイルされた情報をJSONとしてフォーマットし、LLMが解釈しやすくします。\n",
    "\n",
    "**4. GPT-4oモデルを使用した回答の生成** \n",
    "\n",
    "* 最後に、コンパイルされたコンテキストをGPT-4oに渡します。\n",
    "* モデルはコンテキストを使用して、ユーザーの質問に対する情報豊富で一貫性があり、文脈的に関連性の高い回答を生成します。\n",
    "* 取得されたコンテキストは、文書から関連情報にアクセスできるため、LLMがより高い精度で質問に答えるのに役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ff6ec1bf85570568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T19:22:04.349921Z",
     "start_time": "2024-11-13T19:22:04.345790Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Function to get response to a user's question \n",
    "def get_response_to_question(user_question, pc_index):\n",
    "    # Get embedding of the question to find the relevant page with the information \n",
    "    question_embedding = get_embedding(user_question)\n",
    "\n",
    "    # get response vector embeddings \n",
    "    response = pc_index.query(\n",
    "        vector=question_embedding,\n",
    "        top_k=2,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    # Collect the metadata from the matches\n",
    "    context_metadata = [match['metadata'] for match in response['matches']]\n",
    "\n",
    "    # Convert the list of metadata dictionaries to prompt a JSON string\n",
    "    context_json = json.dumps(context_metadata, indent=3)\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use the following context and images to answer the question. In the answer, include the reference to the document, and page number you found the information on between <source></source> tags. If you don't find the information, you can say \"I couldn't find the information\"\n",
    "\n",
    "    question: {user_question}\n",
    "    \n",
    "    <SOURCES>\n",
    "    {context_json}\n",
    "    </SOURCES>\n",
    "    \"\"\"\n",
    "\n",
    "    # Call completions end point with the prompt \n",
    "    completion = oai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f768cacad77b07",
   "metadata": {},
   "source": [
    "では、図表からの情報を必要とする質問を提示してみましょう。この場合、関連する詳細は円グラフ内に見つけることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8f2152d5a6e152d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T19:22:07.106600Z",
     "start_time": "2024-11-13T19:22:05.762624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social protection was allocated 8% of the total lending in Western and Central Africa in fiscal 2024. <source>WB_Report-13, page 13</source>\n"
     ]
    }
   ],
   "source": [
    "question = \"What percentage was allocated to social protections in Western and Central Africa?\"\n",
    "answer = get_response_to_question(question, index)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ee5875a9d9d8b",
   "metadata": {},
   "source": [
    "より挑戦的にするために、表に示された情報の解釈を必要とする質問をしてみましょう。ステップ2では、GPT-4oのビジョンモダリティを使用してこの情報を抽出しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3852345cdc10b063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T19:22:11.753237Z",
     "start_time": "2024-11-13T19:22:09.792274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The increase in access to electricity between 2000 and 2012 in Western and Central Africa was from 34.1% to 44.1%, which is an increase of 10 percentage points. \n",
      "\n",
      "<source>WB_Report-13, page 13</source>\n"
     ]
    }
   ],
   "source": [
    "question = \"What was the increase in access to electricity between 2000 and 2012 in Western and Central Africa?\"\n",
    "answer = get_response_to_question(question, index)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981b499322cee5c",
   "metadata": {},
   "source": [
    "このアプローチはうまく機能しました。しかし、複雑な工学図面などのように、画像やグラフィックに埋め込まれた情報がテキストに変換される際に忠実性を失う場合があります。\n",
    "\n",
    "GPT-4o Visionモダリティを使用することで、ページの画像を直接モデルのコンテキストとして渡すことができます。次のセクションでは、画像入力を使用してモデルの応答精度を向上させる方法を探ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6bbe66e284e12",
   "metadata": {},
   "source": [
    "### ステップ6: 視覚的コンテンツを含むページの処理（オプションステップ）：\n",
    "メタデータが画像、グラフィック、または表の存在を示している場合、抽出されたテキストの代わりに画像をGPT-4oのコンテキストとして渡すことができます。このアプローチは、視覚的情報のテキスト記述だけでは十分にコンテキストを伝えられない場合に有用です。これは、エンジニアリング図面や複雑な図表などの複雑なグラフィックの場合に該当することがあります。\n",
    "\n",
    "**ステップの詳細：**\n",
    "\n",
    "このステップとステップ5の違いは、埋め込みに対して`Visual_Input_Processed`フラグが設定されているかを識別する追加のロジックを追加したことです。その場合、テキストをコンテキストとして渡す代わりに、GPT-4oのビジョンモダリティを使用してページの画像をコンテキストとして渡します。\n",
    "\n",
    "注意：このアプローチは、画像入力の処理がより多くのリソースを必要とし、コストも高いため、レイテンシとコストの両方を増加させます。したがって、上記のステップ5で説明したテキストのみのモダリティでは望ましい結果を得られない場合にのみ使用すべきです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1d2c19cf61fdf198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T19:23:06.302611Z",
     "start_time": "2024-11-13T19:23:06.296695Z"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "\n",
    "def get_response_to_question_with_images(user_question, pc_index):\n",
    "    # Get embedding of the question to find the relevant page with the information \n",
    "    question_embedding = get_embedding(user_question)\n",
    "\n",
    "    # Get response vector embeddings \n",
    "    response = pc_index.query(\n",
    "        vector=question_embedding,\n",
    "        top_k=3,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    # Collect the metadata from the matches\n",
    "    context_metadata = [match['metadata'] for match in response['matches']]\n",
    "\n",
    "    # Build the message content\n",
    "    message_content = []\n",
    "\n",
    "    # Add the initial prompt\n",
    "    initial_prompt = f\"\"\"You are a helpful assistant. Use the text and images provided by the user to answer the question. You must include the reference to the page number or title of the section you the answer where you found the information. If you don't find the information, you can say \"I couldn't find the information\"\n",
    "\n",
    "    question: {user_question}\n",
    "    \"\"\"\n",
    "    \n",
    "    message_content.append({\"role\": \"system\", \"content\": initial_prompt})\n",
    "    \n",
    "    context_messages = []\n",
    "\n",
    "    # Process each metadata item to include text or images based on 'Visual_Input_Processed'\n",
    "    for metadata in context_metadata:\n",
    "        visual_flag = metadata.get('GraphicIncluded')\n",
    "        page_number = metadata.get('pageNumber')\n",
    "        page_text = metadata.get('text')\n",
    "        message =\"\"\n",
    "\n",
    "        if visual_flag =='Y':\n",
    "            # Include the image\n",
    "            print(f\"Adding page number {page_number} as an image to context\")\n",
    "            image_path = metadata.get('ImagePath', None)\n",
    "            try:\n",
    "                base64_image = encode_image(image_path)\n",
    "                image_type = 'jpeg'\n",
    "                # Prepare the messages for the API call\n",
    "                context_messages.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/{image_type};base64,{base64_image}\"\n",
    "                    },\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding image at {image_path}: {e}\")\n",
    "        else:\n",
    "            # Include the text\n",
    "            print(f\"Adding page number {page_number} as text to context\")\n",
    "            context_messages.append({\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Page {page_number} - {page_text}\",\n",
    "                })\n",
    "        \n",
    "                # Prepare the messages for the API call\n",
    "        messages =  {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": context_messages\n",
    "        }\n",
    "    \n",
    "    message_content.append(messages)\n",
    "\n",
    "    completion = oai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=message_content\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a4f85fd3364b0",
   "metadata": {},
   "source": [
    "ステップ5でテキストのみのセマンティック検索について尋ねたのと同じ質問を検証してみましょう。GPT-4oモデルが質問に答えるための関連情報を含む図表を特定できることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "adccf5e17c33766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T19:23:15.154233Z",
     "start_time": "2024-11-13T19:23:06.939605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding page number 13.0 as an image to context\n",
      "Adding page number 12.0 as an image to context\n",
      "Adding page number 11.0 as an image to context\n",
      "The percentage allocated to social protection in Western and Central Africa is 8% (Figure 2: Western and Central Africa; IBRD and IDA Lending by Sector).\n"
     ]
    }
   ],
   "source": [
    "question = \"What percentage was allocated to social protections in Western and Central Africa?\"\n",
    "answer = get_response_to_question_with_images(question, index)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a5562311fcb05",
   "metadata": {},
   "source": [
    "それでは、テキストのみのモダリティでは答えることができない可能性のある質問をしてみましょう。例えば、文書内の関連する画像を見つけて、その画像を説明するような質問です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7120e7ff370e30f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T19:28:07.292614Z",
     "start_time": "2024-11-13T19:27:52.964579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding page number 32.0 as an image to context\n",
      "Adding page number 10.0 as an image to context\n",
      "Adding page number 4.0 as an image to context\n",
      "### Image Descriptions\n",
      "\n",
      "1. **Page 60-61 (Digital Section)**:\n",
      "   - **Left Side**: A person is sitting and working on a laptop, holding a smartphone. The setting seems informal, possibly in a small office or a cafe.\n",
      "   - **Text**: Discussion on scaling digital development, thought leadership, partnerships, and establishment of a Digital Vice Presidency unit for digital transformation efforts.\n",
      "\n",
      "2. **Page 16-17 (Eastern and Southern Africa Section)**:\n",
      "   - **Right Side**: A group of people standing on a paved street, some using mobile phones. It seems to be a casual, evening setting.\n",
      "   - **Text**: Information about improving access to electricity in Rwanda and efforts for education and other services in Eastern and Southern Africa.\n",
      "\n",
      "3. **Page 4-5 (Driving Action, Measuring Results)**:\n",
      "   - **Images**: Various circular images and icons accompany text highlights such as feeding people, providing schooling, access to clean water, transport, and energy.\n",
      "   - **Text**: Summary of key development results achieved by the World Bank Group in fiscal 2024.\n",
      "\n",
      "These images illustrate the initiatives and impacts of the World Bank's projects and activities in various sectors.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you find the image associated with digital improvements and describe what you see in the images?\"\n",
    "answer = get_response_to_question_with_images(question, index)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcfbb6a47322a21",
   "metadata": {},
   "source": [
    "### まとめ\n",
    "\n",
    "このクックブックでは、画像、グラフィック、表が豊富に含まれた文書に対するRetrieval-Augmented Generation（RAG）システムの強化に取り組みました。従来のRAGモデルは、テキストデータには習熟していますが、視覚的要素を通じて伝達される豊富な情報を見落とすことがよくあります。ビジョンモデルを統合し、メタデータタギングを活用することで、このギャップを埋め、AIが視覚的コンテンツを効果的に解釈し活用できるようにしました。\n",
    "\n",
    "まず、Pineconeを使用してベクトルストアを設定し、ベクトル埋め込みの効率的な保存と検索の基盤を確立しました。PDFを解析し、GPT-4oのビジョンモダリティを使用して視覚情報を抽出することで、文書ページを関連するテキストに変換できました。埋め込みを生成し、視覚的コンテンツを含むページにフラグを立てることで、ベクトルストア内に堅牢なメタデータフィルタリングシステムを作成しました。\n",
    "\n",
    "これらの埋め込みをPineconeにアップロードすることで、RAG処理ワークフローとのシームレスな統合が促進されました。セマンティック検索を通じて、ユーザークエリに一致する関連ページを取得し、テキストと視覚情報の両方が考慮されることを確保しました。視覚的コンテンツを含むページをビジョンモデルに渡すことで処理することにより、特に画像や表に依存するクエリに対する回答の精度と深度が向上しました。\n",
    "\n",
    "世界銀行の**A Better Bank for a Better World: Annual Report 2024**を指針となる例として使用し、これらの技術がどのように組み合わさって複雑な文書を処理し解釈するかを実証しました。このアプローチは、ユーザーに提供される情報を豊かにするだけでなく、より包括的で正確な回答を提供することで、ユーザーの満足度とエンゲージメントを大幅に向上させます。\n",
    "\n",
    "このクックブックで概説された概念に従うことで、複雑な視覚的要素を含む文書を処理し解釈できるRAGシステムを構築する準備が整いました。この進歩により、視覚データが重要な役割を果たすさまざまな領域でのAIアプリケーションの新たな可能性が開かれます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
