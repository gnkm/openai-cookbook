{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChromaとOpenAIを使った堅牢な質問応答システム\n",
    "\n",
    "このノートブックでは、オープンソースの埋め込みデータベースである[Chroma](https://trychroma.com)と、OpenAIの[テキスト埋め込み](https://platform.openai.com/docs/guides/embeddings/use-cases)および[チャット補完](https://platform.openai.com/docs/guides/chat) APIを使用して、データコレクションに関する質問に答える方法をステップバイステップで説明します。\n",
    "\n",
    "さらに、このノートブックでは質問応答システムをより堅牢にするためのトレードオフについても説明します。これから見るように、*単純なクエリが常に最良の結果を生み出すとは限りません*！\n",
    "\n",
    "## LLMを使った質問応答\n",
    "\n",
    "OpenAIのChatGPTのような大規模言語モデル（LLM）は、モデルが訓練されていない、またはアクセスできないデータに関する質問に答えるために使用できます。例えば：\n",
    "\n",
    "- メールやメモなどの個人データ\n",
    "- アーカイブや法的文書などの高度に専門化されたデータ\n",
    "- 最近のニュース記事などの新しく作成されたデータ\n",
    "\n",
    "この制限を克服するために、LLM自体と同様に自然言語でのクエリに適したデータストアを使用できます。Chromaのような埋め込みストアは、文書自体と並んで文書を[埋め込み](https://openai.com/blog/introducing-text-and-code-embeddings)として表現します。\n",
    "\n",
    "テキストクエリを埋め込むことで、Chromaは関連する文書を見つけることができ、それをLLMに渡して質問に答えることができます。このアプローチの詳細な例とバリエーションを紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# セットアップと準備\n",
    "\n",
    "まず、必要なPython依存関係がインストールされていることを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU openai chromadb pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブック全体でOpenAIのAPIを使用します。APIキーは[https://beta.openai.com/account/api-keys](https://beta.openai.com/account/api-keys)から取得できます。\n",
    "\n",
    "ターミナルで`export OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`コマンドを実行することで、APIキーを環境変数として追加できます。環境変数がまだ設定されていない場合は、ノートブックを再読み込みする必要があることに注意してください。または、以下に示すようにノートブック内で設定することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client is ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Uncomment the following line to set the environment variable in the notebook\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    print(\"OpenAI client is ready\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY environment variable not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model for all API calls\n",
    "OPENAI_MODEL = \"gpt-4o\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセット\n",
    "\n",
    "このノートブック全体を通して、[SciFact dataset](https://github.com/allenai/scifact)を使用します。これは専門家によって注釈付けされた科学的主張のキュレートされたデータセットで、論文のタイトルと要約のテキストコーパスが付属しています。各主張は、コーパス内の文書に従って、支持される、矛盾する、またはいずれの方向にも十分な証拠がない、のいずれかに分類されます。\n",
    "\n",
    "コーパスが正解として利用可能であることにより、LLMの質問応答に対する以下のアプローチがどの程度うまく機能するかを調査することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>cited_doc_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0-dimensional biomaterials show inductive prop...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[31715818]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1,000 genomes project enables mapping of genet...</td>\n",
       "      <td>{'14717500': [{'sentences': [2, 5], 'label': '...</td>\n",
       "      <td>[14717500]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1/2000 in UK have abnormal PrP positivity.</td>\n",
       "      <td>{'13734012': [{'sentences': [4], 'label': 'SUP...</td>\n",
       "      <td>[13734012]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>5% of perinatal mortality is due to low birth ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[1606628]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>A deficiency of vitamin B12 increases blood le...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[5152028, 11705328]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              claim  \\\n",
       "0   1  0-dimensional biomaterials show inductive prop...   \n",
       "1   3  1,000 genomes project enables mapping of genet...   \n",
       "2   5         1/2000 in UK have abnormal PrP positivity.   \n",
       "3  13  5% of perinatal mortality is due to low birth ...   \n",
       "4  36  A deficiency of vitamin B12 increases blood le...   \n",
       "\n",
       "                                            evidence        cited_doc_ids  \n",
       "0                                                 {}           [31715818]  \n",
       "1  {'14717500': [{'sentences': [2, 5], 'label': '...           [14717500]  \n",
       "2  {'13734012': [{'sentences': [4], 'label': 'SUP...           [13734012]  \n",
       "3                                                 {}            [1606628]  \n",
       "4                                                 {}  [5152028, 11705328]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the claim dataset\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '../../data'\n",
    "\n",
    "claim_df = pd.read_json(f'{data_path}/scifact_claims.jsonl', lines=True)\n",
    "claim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルに直接質問する\n",
    "\n",
    "ChatGPTは大量の科学的情報で訓練されています。ベースラインとして、追加のコンテキストなしでモデルがすでに何を知っているかを理解したいと思います。これにより、全体的なパフォーマンスを較正することができます。\n",
    "\n",
    "いくつかの事実例を含む適切なプロンプトを構築し、データセット内の各主張についてモデルに問い合わせます。モデルには、主張を「True」、「False」、または証拠が不十分な場合は「NEE」として評価するよう求めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(claim):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"I will ask you to assess a scientific claim. Output only the text 'True' if the claim is true, 'False' if the claim is false, or 'NEE' if there's not enough evidence.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Example:\n",
    "\n",
    "Claim:\n",
    "0-dimensional biomaterials show inductive properties.\n",
    "\n",
    "Assessment:\n",
    "False\n",
    "\n",
    "Claim:\n",
    "1/2000 in UK have abnormal PrP positivity.\n",
    "\n",
    "Assessment:\n",
    "True\n",
    "\n",
    "Claim:\n",
    "Aspirin inhibits the production of PGE2.\n",
    "\n",
    "Assessment:\n",
    "False\n",
    "\n",
    "End of examples. Assess the following claim:\n",
    "\n",
    "Claim:\n",
    "{claim}\n",
    "\n",
    "Assessment:\n",
    "\"\"\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "def assess_claims(claims):\n",
    "    responses = []\n",
    "    # Query the OpenAI API\n",
    "    for claim in claims:\n",
    "        response = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=build_prompt(claim),\n",
    "            max_tokens=3,\n",
    "        )\n",
    "        # Strip any punctuation or whitespace from the response\n",
    "        responses.append(response.choices[0].message.content.strip('., '))\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットから50件のクレームをサンプリングします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at 50 claims\n",
    "samples = claim_df.sample(50)\n",
    "\n",
    "claims = samples['claim'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットに従って正解データを評価します。データセットの説明によると、各主張は証拠によって支持されるか反駁されるか、あるいはどちらの方向にも十分な証拠が存在しないかのいずれかです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groundtruth(evidence):\n",
    "    groundtruth = []\n",
    "    for e in evidence:\n",
    "        # Evidence is empty\n",
    "        if len(e) == 0:\n",
    "            groundtruth.append('NEE')\n",
    "        else:\n",
    "            # In this dataset, all evidence for a given claim is consistent, either SUPPORT or CONTRADICT\n",
    "            if list(e.values())[0][0]['label'] == 'SUPPORT':\n",
    "                groundtruth.append('True')\n",
    "            else:\n",
    "                groundtruth.append('False')\n",
    "    return groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence = samples['evidence'].tolist()\n",
    "groundtruth = get_groundtruth(evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、モデルの評価と正解データを比較した混同行列を、読みやすい表形式で出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(inferred, groundtruth):\n",
    "    assert len(inferred) == len(groundtruth)\n",
    "    confusion = {\n",
    "        'True': {'True': 0, 'False': 0, 'NEE': 0},\n",
    "        'False': {'True': 0, 'False': 0, 'NEE': 0},\n",
    "        'NEE': {'True': 0, 'False': 0, 'NEE': 0},\n",
    "    }\n",
    "    for i, g in zip(inferred, groundtruth):\n",
    "        confusion[i][g] += 1\n",
    "\n",
    "    # Pretty print the confusion matrix\n",
    "    print('\\tGroundtruth')\n",
    "    print('\\tTrue\\tFalse\\tNEE')\n",
    "    for i in confusion:\n",
    "        print(i, end='\\t')\n",
    "        for g in confusion[i]:\n",
    "            print(confusion[i][g], end='\\t')\n",
    "        print()\n",
    "\n",
    "    return confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルに対して、追加のコンテキストなしに、主張を直接評価するよう求めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tGroundtruth\n",
      "\tTrue\tFalse\tNEE\n",
      "True\t9\t3\t15\t\n",
      "False\t0\t3\t2\t\n",
      "NEE\t8\t6\t4\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'True': {'True': 9, 'False': 3, 'NEE': 15},\n",
       " 'False': {'True': 0, 'False': 3, 'NEE': 2},\n",
       " 'NEE': {'True': 8, 'False': 6, 'NEE': 4}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_inferred = assess_claims(claims)\n",
    "confusion_matrix(gpt_inferred, groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果\n",
    "\n",
    "これらの結果から、LLMは主張を真実として評価する強いバイアスを持っており、それらが偽である場合でも真実と評価し、また偽の主張を証拠不十分として評価する傾向があることがわかります。なお、「証拠不十分」とは、追加の文脈なしに、真空状態での主張に対するモデルの評価に関するものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コンテキストの追加\n",
    "\n",
    "ここで、論文タイトルと要約のコーパスから利用可能な追加のコンテキストを追加します。このセクションでは、OpenAIのテキスト埋め込みを使用して、テキストコーパスをChromaに読み込む方法を示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、テキストコーパスを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>structured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4983</td>\n",
       "      <td>Microstructural development of human newborn c...</td>\n",
       "      <td>[Alterations of the architecture of cerebral w...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5836</td>\n",
       "      <td>Induction of myelodysplasia by myeloid-derived...</td>\n",
       "      <td>[Myelodysplastic syndromes (MDS) are age-depen...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7912</td>\n",
       "      <td>BC1 RNA, the transcript from a master gene for...</td>\n",
       "      <td>[ID elements are short interspersed elements (...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18670</td>\n",
       "      <td>The DNA Methylome of Human Peripheral Blood Mo...</td>\n",
       "      <td>[DNA methylation plays an important role in bi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19238</td>\n",
       "      <td>The human myelin basic protein gene is include...</td>\n",
       "      <td>[Two human Golli (for gene expressed in the ol...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title  \\\n",
       "0    4983  Microstructural development of human newborn c...   \n",
       "1    5836  Induction of myelodysplasia by myeloid-derived...   \n",
       "2    7912  BC1 RNA, the transcript from a master gene for...   \n",
       "3   18670  The DNA Methylome of Human Peripheral Blood Mo...   \n",
       "4   19238  The human myelin basic protein gene is include...   \n",
       "\n",
       "                                            abstract  structured  \n",
       "0  [Alterations of the architecture of cerebral w...       False  \n",
       "1  [Myelodysplastic syndromes (MDS) are age-depen...       False  \n",
       "2  [ID elements are short interspersed elements (...       False  \n",
       "3  [DNA methylation plays an important role in bi...       False  \n",
       "4  [Two human Golli (for gene expressed in the ol...       False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the corpus into a dataframe\n",
    "corpus_df = pd.read_json(f'{data_path}/scifact_corpus.jsonl', lines=True)\n",
    "corpus_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コーパスをChromaに読み込む\n",
    "\n",
    "次のステップは、コーパスをChromaに読み込むことです。埋め込み関数が与えられると、Chromaは各ドキュメントの埋め込みを自動的に処理し、そのテキストとメタデータと一緒に保存するため、クエリを簡単に実行できます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "私たちは（一時的な）Chromaクライアントをインスタンス化し、SciFact のタイトルと要約コーパス用のコレクションを作成します。\n",
    "Chromaは永続化設定でもインスタンス化できます。詳細は[Chroma docs](https://docs.trychroma.com/usage-guide?lang=py)で学習してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "# We initialize an embedding function, and provide it to the collection.\n",
    "embedding_function = OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "chroma_client = chromadb.Client() # Ephemeral by default\n",
    "scifact_corpus_collection = chroma_client.create_collection(name='scifact_corpus', embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、コーパスをChromaに読み込みます。このデータ読み込みはメモリ集約的であるため、50-1000件のバッチに分けたバッチ読み込み方式を使用することをお勧めします。この例では、コーパス全体の処理に1分強かかるはずです。先ほど指定した`embedding_function`を使用して、バックグラウンドで自動的に埋め込み処理が行われています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "for i in range(0, len(corpus_df), batch_size):\n",
    "    batch_df = corpus_df[i:i+batch_size]\n",
    "    scifact_corpus_collection.add(\n",
    "        ids=batch_df['doc_id'].apply(lambda x: str(x)).tolist(), # Chroma takes string IDs.\n",
    "        documents=(batch_df['title'] + '. ' + batch_df['abstract'].apply(lambda x: ' '.join(x))).to_list(), # We concatenate the title and abstract.\n",
    "        metadatas=[{\"structured\": structured} for structured in batch_df['structured'].to_list()] # We also store the metadata, though we don't use it in this example.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コンテキストの取得\n",
    "\n",
    "次に、サンプル内の各クレームに関連する可能性のある文書をコーパスから取得します。これらをLLMがクレームを評価する際のコンテキストとして提供したいと考えています。埋め込み距離に基づいて、各クレームに対して最も関連性の高い3つの文書を取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_query_result = scifact_corpus_collection.query(query_texts=claims, include=['documents', 'distances'], n_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスから取得した追加のコンテキストを考慮して、新しいプロンプトを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_context(claim, context):\n",
    "    return [{'role': 'system', 'content': \"I will ask you to assess whether a particular scientific claim, based on evidence provided. Output only the text 'True' if the claim is true, 'False' if the claim is false, or 'NEE' if there's not enough evidence.\"},\n",
    "            {'role': 'user', 'content': f\"\"\"\"\n",
    "The evidence is the following:\n",
    "\n",
    "{' '.join(context)}\n",
    "\n",
    "Assess the following claim on the basis of the evidence. Output only the text 'True' if the claim is true, 'False' if the claim is false, or 'NEE' if there's not enough evidence. Do not output any other text.\n",
    "\n",
    "Claim:\n",
    "{claim}\n",
    "\n",
    "Assessment:\n",
    "\"\"\"}]\n",
    "\n",
    "\n",
    "def assess_claims_with_context(claims, contexts):\n",
    "    responses = []\n",
    "    # Query the OpenAI API\n",
    "    for claim, context in zip(claims, contexts):\n",
    "        # If no evidence is provided, return NEE\n",
    "        if len(context) == 0:\n",
    "            responses.append('NEE')\n",
    "            continue\n",
    "        response = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=build_prompt_with_context(claim=claim, context=context),\n",
    "            max_tokens=3,\n",
    "        )\n",
    "        # Strip any punctuation or whitespace from the response\n",
    "        responses.append(response.choices[0].message.content.strip('., '))\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、取得したコンテキストを使用して主張を評価するようにモデルに依頼します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tGroundtruth\n",
      "\tTrue\tFalse\tNEE\n",
      "True\t13\t1\t4\t\n",
      "False\t1\t10\t2\t\n",
      "NEE\t3\t1\t15\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'True': {'True': 13, 'False': 1, 'NEE': 4},\n",
       " 'False': {'True': 1, 'False': 10, 'NEE': 2},\n",
       " 'NEE': {'True': 3, 'False': 1, 'NEE': 15}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_with_context_evaluation = assess_claims_with_context(claims, claim_query_result['documents'])\n",
    "confusion_matrix(gpt_with_context_evaluation, groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果\n",
    "\n",
    "モデル全体的により良いパフォーマンスを示し、偽の主張を正しく識別することが大幅に改善されました。さらに、ほとんどのNEEケースも正しく識別されるようになりました。\n",
    "\n",
    "取得された文書を見ると、時々主張に関連していないことがあります。これにより、モデルは余分な情報に混乱し、情報が無関係であっても十分な証拠が存在すると判断してしまう可能性があります。これは、常に「最も」関連性の高い3つの文書を要求するためですが、これらの文書は特定の時点を超えると全く関連性がない可能性があります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関連性に基づくコンテキストのフィルタリング\n",
    "\n",
    "Chromaは文書自体とともに距離スコアも返します。距離に対して閾値を設定することで、モデルに提供するコンテキストに含まれる無関係な文書を減らすことができます。\n",
    "\n",
    "閾値でフィルタリングした後にコンテキスト文書が残らない場合は、モデルをバイパスして、十分な証拠がないことを単純に返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_query_result(query_result, distance_threshold=0.25):\n",
    "# For each query result, retain only the documents whose distance is below the threshold\n",
    "    for ids, docs, distances in zip(query_result['ids'], query_result['documents'], query_result['distances']):\n",
    "        for i in range(len(ids)-1, -1, -1):\n",
    "            if distances[i] > distance_threshold:\n",
    "                ids.pop(i)\n",
    "                docs.pop(i)\n",
    "                distances.pop(i)\n",
    "    return query_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_claim_query_result = filter_query_result(claim_query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今度は、このより明確なコンテキストを使用して主張を評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tGroundtruth\n",
      "\tTrue\tFalse\tNEE\n",
      "True\t9\t0\t1\t\n",
      "False\t0\t7\t0\t\n",
      "NEE\t8\t5\t20\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'True': {'True': 9, 'False': 0, 'NEE': 1},\n",
       " 'False': {'True': 0, 'False': 7, 'NEE': 0},\n",
       " 'NEE': {'True': 8, 'False': 5, 'NEE': 20}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_with_filtered_context_evaluation = assess_claims_with_context(claims, filtered_claim_query_result['documents'])\n",
    "confusion_matrix(gpt_with_filtered_context_evaluation, groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果\n",
    "\n",
    "モデルは現在、十分な証拠が存在しない場合に、TrueまたはFalseと評価する主張の数を大幅に減らしています。しかし、モデルは非常に慎重になり、ほとんどの項目を証拠不十分とラベル付けする傾向があり、確実性から遠ざかる方向にバイアスがかかっています。現在、ほとんどの主張が証拠不十分と評価されています。これは、それらの大部分が距離閾値によってフィルタリングされているためです。最適な動作点を見つけるために距離閾値を調整することは可能ですが、これは困難であり、データセットと埋め込みモデルに依存します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仮想文書埋め込み：幻覚を生産的に活用する\n",
    "\n",
    "私たちは、モデルを混乱させる可能性のある関連性の低い文書を取得することなく、関連する文書を取得できるようになりたいと考えています。これを実現する一つの方法は、検索クエリを改善することです。\n",
    "\n",
    "これまで、私たちは科学論文を説明する_要約_を含むコーパスに対して、単一の文による陳述である_主張_を使用してデータセットにクエリを実行してきました。直感的には、これらは関連している可能性がありますが、その構造と意味には大きな違いがあります。これらの違いは埋め込みモデルによってエンコードされ、クエリと最も関連性の高い結果との間の距離に影響を与えます。\n",
    "\n",
    "私たちは、LLMの力を活用して関連するテキストを生成することで、この問題を克服できます。事実は幻覚かもしれませんが、モデルが生成する文書の内容と構造は、クエリよりも私たちのコーパス内の文書により類似しています。これにより、より良いクエリが得られ、ひいてはより良い結果が得られる可能性があります。\n",
    "\n",
    "このアプローチは[仮想文書埋め込み（HyDE）](https://arxiv.org/abs/2212.10496)と呼ばれ、検索タスクにおいて非常に優れていることが示されています。これにより、コンテキストを汚染することなく、より関連性の高い情報をコンテキストに取り込むことができるはずです。\n",
    "\n",
    "要約：\n",
    "- 単一の文ではなく完全な要約を埋め込む場合、はるかに良いマッチングが得られる\n",
    "- しかし、主張は通常単一の文である\n",
    "- そこでHyDEは、GPT3を使用して主張を幻覚的な要約に拡張し、それらの要約に基づいて検索することが（主張 -> 要約 -> 結果）、直接検索する（主張 -> 結果）よりも効果的であることを示している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、評価したい各主張について、コーパス内の文書と類似した文書を生成するよう、文脈内の例を使用してモデルにプロンプトを与えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hallucination_prompt(claim):\n",
    "    return [{'role': 'system', 'content': \"\"\"I will ask you to write an abstract for a scientific paper which supports or refutes a given claim. It should be written in scientific language, include a title. Output only one abstract, then stop.\n",
    "\n",
    "    An Example:\n",
    "\n",
    "    Claim:\n",
    "    A high microerythrocyte count raises vulnerability to severe anemia in homozygous alpha (+)- thalassemia trait subjects.\n",
    "\n",
    "    Abstract:\n",
    "    BACKGROUND The heritable haemoglobinopathy alpha(+)-thalassaemia is caused by the reduced synthesis of alpha-globin chains that form part of normal adult haemoglobin (Hb). Individuals homozygous for alpha(+)-thalassaemia have microcytosis and an increased erythrocyte count. Alpha(+)-thalassaemia homozygosity confers considerable protection against severe malaria, including severe malarial anaemia (SMA) (Hb concentration < 50 g/l), but does not influence parasite count. We tested the hypothesis that the erythrocyte indices associated with alpha(+)-thalassaemia homozygosity provide a haematological benefit during acute malaria.\n",
    "    METHODS AND FINDINGS Data from children living on the north coast of Papua New Guinea who had participated in a case-control study of the protection afforded by alpha(+)-thalassaemia against severe malaria were reanalysed to assess the genotype-specific reduction in erythrocyte count and Hb levels associated with acute malarial disease. We observed a reduction in median erythrocyte count of approximately 1.5 x 10(12)/l in all children with acute falciparum malaria relative to values in community children (p < 0.001). We developed a simple mathematical model of the linear relationship between Hb concentration and erythrocyte count. This model predicted that children homozygous for alpha(+)-thalassaemia lose less Hb than children of normal genotype for a reduction in erythrocyte count of >1.1 x 10(12)/l as a result of the reduced mean cell Hb in homozygous alpha(+)-thalassaemia. In addition, children homozygous for alpha(+)-thalassaemia require a 10% greater reduction in erythrocyte count than children of normal genotype (p = 0.02) for Hb concentration to fall to 50 g/l, the cutoff for SMA. We estimated that the haematological profile in children homozygous for alpha(+)-thalassaemia reduces the risk of SMA during acute malaria compared to children of normal genotype (relative risk 0.52; 95% confidence interval [CI] 0.24-1.12, p = 0.09).\n",
    "    CONCLUSIONS The increased erythrocyte count and microcytosis in children homozygous for alpha(+)-thalassaemia may contribute substantially to their protection against SMA. A lower concentration of Hb per erythrocyte and a larger population of erythrocytes may be a biologically advantageous strategy against the significant reduction in erythrocyte count that occurs during acute infection with the malaria parasite Plasmodium falciparum. This haematological profile may reduce the risk of anaemia by other Plasmodium species, as well as other causes of anaemia. Other host polymorphisms that induce an increased erythrocyte count and microcytosis may confer a similar advantage.\n",
    "\n",
    "    End of example.\n",
    "\n",
    "    \"\"\"}, {'role': 'user', 'content': f\"\"\"\"\n",
    "    Perform the task for the following claim.\n",
    "\n",
    "    Claim:\n",
    "    {claim}\n",
    "\n",
    "    Abstract:\n",
    "    \"\"\"}]\n",
    "\n",
    "\n",
    "def hallucinate_evidence(claims):\n",
    "    responses = []\n",
    "    # Query the OpenAI API\n",
    "    for claim in claims:\n",
    "        response = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=build_hallucination_prompt(claim),\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各クレームに対してドキュメントを幻覚生成します。\n",
    "\n",
    "*注意：これには時間がかかる場合があります。100件のクレームで約7分程度*。より迅速に結果を得るために、評価したいクレーム数を減らすことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucinated_evidence = hallucinate_evidence(claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "幻覚された文書をコーパスへのクエリとして使用し、同じ距離閾値を使って結果をフィルタリングします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucinated_query_result = scifact_corpus_collection.query(query_texts=hallucinated_evidence, include=['documents', 'distances'], n_results=3)\n",
    "filtered_hallucinated_query_result = filter_query_result(hallucinated_query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "その後、新しいコンテキストを使用して、モデルに主張を評価するよう求めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tGroundtruth\n",
      "\tTrue\tFalse\tNEE\n",
      "True\t13\t0\t3\t\n",
      "False\t1\t10\t1\t\n",
      "NEE\t3\t2\t17\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'True': {'True': 13, 'False': 0, 'NEE': 3},\n",
       " 'False': {'True': 1, 'False': 10, 'NEE': 1},\n",
       " 'NEE': {'True': 3, 'False': 2, 'NEE': 17}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_with_hallucinated_context_evaluation = assess_claims_with_context(claims, filtered_hallucinated_query_result['documents'])\n",
    "confusion_matrix(gpt_with_hallucinated_context_evaluation, groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果\n",
    "\n",
    "HyDEをシンプルな距離閾値と組み合わせることで、大幅な改善が得られます。モデルはもはや主張をTrueと評価することに偏ることもなく、証拠が不十分であると判断することに偏ることもありません。また、証拠が不十分な場合をより正確に評価できるようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結論\n",
    "\n",
    "文書のコーパスに基づくコンテキストをLLMに装備することは、LLMの一般的な推論と自然言語インタラクションを独自のデータに適用するための強力な技術です。しかし、単純なクエリと検索では最良の結果が得られない可能性があることを理解しておくことが重要です！最終的に、データを理解することが、検索ベースの質問応答アプローチから最大限の効果を得るのに役立ちます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
