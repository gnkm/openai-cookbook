{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9w5JBaUL-lO"
   },
   "source": [
    "# CLIPエンベディングとGPT-4 Visionを使用したマルチモーダルRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CCjcFSiMbvf"
   },
   "source": [
    "マルチモーダルRAGは、従来のテキストベースのRAGに追加のモダリティを統合し、追加のコンテキストを提供してテキストデータをグラウンディングすることで、LLMの質問応答を強化し、理解を向上させます。\n",
    "\n",
    "[clothing matchmaker cookbook](https://cookbook.openai.com/examples/how_to_combine_gpt4v_with_rag_outfit_assistant)のアプローチを採用し、テキストキャプション化という情報損失を伴うプロセスを回避して画像を直接埋め込み、類似性検索を行うことで、検索精度を向上させています。\n",
    "\n",
    "CLIPベースの埋め込みを使用することで、特定のデータでのファインチューニングや、未見の画像での更新がさらに可能になります。\n",
    "\n",
    "この技術は、ユーザーが提供した技術画像を使用して企業のナレッジベースを検索し、関連情報を提供することを通じて実演されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-Mpdxit4x49"
   },
   "source": [
    "# インストール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbt3evfHUJTZ"
   },
   "source": [
    "まず、関連するパッケージをインストールしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hgrcVEl0Ma1"
   },
   "outputs": [],
   "source": [
    "#installations\n",
    "%pip install clip\n",
    "%pip install torch\n",
    "%pip install pillow\n",
    "%pip install faiss-cpu\n",
    "%pip install numpy\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgrlBLTpT0si"
   },
   "source": [
    "それでは、必要なパッケージをすべてインポートしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pN1cWF-iyLUg"
   },
   "outputs": [],
   "source": [
    "# model imports\n",
    "import faiss\n",
    "import json\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import clip\n",
    "client = OpenAI()\n",
    "\n",
    "# helper imports\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List, Union, Tuple\n",
    "\n",
    "# visualisation imports\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fONcWxRqll8"
   },
   "source": [
    "それでは、CLIPモデルを読み込みましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_Ua9y98NRk70"
   },
   "outputs": [],
   "source": [
    "#load model on device. The device you are running inference/training on is either a CPU or GPU if you have.\n",
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dev-zjfJ774W"
   },
   "source": [
    "以下を実行します：\n",
    "1. 画像埋め込みデータベースを作成する\n",
    "2. ビジョンモデルへのクエリを設定する\n",
    "3. セマンティック検索を実行する\n",
    "4. ユーザークエリを画像に渡す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Y1v2jkS42TS"
   },
   "source": [
    "# 画像埋め込みデータベースの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVBAMyhesAyi"
   },
   "source": [
    "次に、画像のディレクトリから画像埋め込みナレッジベースを作成します。これは、ユーザーがアップロードした画像に関する情報を提供するために検索する技術のナレッジベースとなります。\n",
    "\n",
    "画像を保存するディレクトリ（JPEG形式）を渡し、それぞれをループして埋め込みを作成します。\n",
    "\n",
    "また、`description.json`も用意しています。これには、ナレッジベース内のすべての画像に対するエントリが含まれています。このファイルには`'image_path'`と`'description'`の2つのキーがあり、各画像をユーザーの質問に答える際に役立つ画像の説明にマッピングしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDCz76gr8yAu"
   },
   "source": [
    "まず、指定されたディレクトリ内のすべての画像パスを取得する関数を書きましょう。その後、'image_database'というディレクトリからすべてのjpegファイルを取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vE9i3zLuRk5c"
   },
   "outputs": [],
   "source": [
    "def get_image_paths(directory: str, number: int = None) -> List[str]:\n",
    "    image_paths = []\n",
    "    count = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.jpeg'):\n",
    "            image_paths.append(os.path.join(directory, filename))\n",
    "            if number is not None and count == number:\n",
    "                return [image_paths[-1]]\n",
    "            count += 1\n",
    "    return image_paths\n",
    "direc = 'image_database/'\n",
    "image_paths = get_image_paths(direc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMldfjn189vC"
   },
   "source": [
    "次に、一連のパスが与えられたときにCLIPモデルから画像埋め込みを取得する関数を書きます。\n",
    "\n",
    "まず、先ほど取得した前処理関数を使用して画像を前処理します。これにより、CLIPモデルへの入力が適切な形式と次元になるよう、リサイズ、正規化、色チャンネル調整などのいくつかの処理が実行されます。\n",
    "\n",
    "次に、これらの前処理済み画像をスタックして、ループではなく一度にモデルに渡せるようにします。そして最後に、埋め込みの配列であるモデル出力を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fd3I_fPh8qvi"
   },
   "outputs": [],
   "source": [
    "def get_features_from_image_path(image_paths):\n",
    "  images = [preprocess(Image.open(image_path).convert(\"RGB\")) for image_path in image_paths]\n",
    "  image_input = torch.tensor(np.stack(images))\n",
    "  with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "  return image_features\n",
    "image_features = get_features_from_image_path(image_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UH_kyZAE-kHe"
   },
   "source": [
    "これで、ベクトルデータベースを作成できるようになりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TIeqpndF8tZk"
   },
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(image_features.shape[1])\n",
    "index.add(image_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swDe1c4v-mbz"
   },
   "source": [
    "また、画像と説明のマッピング用のjsonを取り込み、jsonのリストを作成します。さらに、指定した画像を検索するためのヘルパー関数も作成し、その画像の説明を取得できるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tdjlXQqC8uNE"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "image_path = 'train1.jpeg'\n",
    "with open('description.json', 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "def find_entry(data, key, value):\n",
    "    for entry in data:\n",
    "        if entry.get(key) == value:\n",
    "            return entry\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJXfCtPD5_63"
   },
   "source": [
    "ユーザーがアップロードした画像の例を表示してみましょう。これは2024年のCESで発表された技術製品です。DELTA Pro Ultra Whole House Battery Generatorです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtkZ7W3g5sED"
   },
   "outputs": [],
   "source": [
    "im = Image.open(image_path)\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ivECCKSdbBy"
   },
   "source": [
    "![Delta Pro](../images/train1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sidjylki7Kye"
   },
   "source": [
    "# ビジョンモデルのクエリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8O7X6ml7t38"
   },
   "source": [
    "それでは、GPT-4 Vision（この技術を以前に見たことがないはず）が、これを何とラベル付けするかを見てみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4uDjS-gQAqm"
   },
   "source": [
    "まず、画像をbase64形式でエンコードする関数を作成する必要があります。これは、ビジョンモデルに渡す形式だからです。次に、画像入力でLLMにクエリを送信できるように、汎用的な`image_query`関数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "87gf6_xO8Y4i",
    "outputId": "99be865f-12e8-4ef0-c2f5-5fd6e5c787f3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Autonomous Delivery Robot'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read())\n",
    "        return encoded_image.decode('utf-8')\n",
    "\n",
    "def image_query(query, image_path):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4-vision-preview',\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": query,\n",
    "                },\n",
    "                {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{encode_image(image_path)}\",\n",
    "                },\n",
    "                }\n",
    "            ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=300,\n",
    "    )\n",
    "    # Extract relevant features from the response\n",
    "    return response.choices[0].message.content\n",
    "image_query('Write a short label of what is show in this image?', image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfG_7c-jQAqm"
   },
   "source": [
    "ご覧のとおり、AIは訓練されたデータの情報から最善を尽くそうとしますが、訓練データで類似したものを見たことがないため、間違いを犯してしまいます。これは、曖昧な画像であるため、推定や推論が困難になるからです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szWZqTqf7SrA"
   },
   "source": [
    "# セマンティック検索の実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV8LaOncGH3j"
   },
   "source": [
    "それでは、類似度検索を実行して、ナレッジベース内で最も類似している2つの画像を見つけてみましょう。これは、ユーザーが入力した`image_path`の埋め込みを取得し、データベース内の類似画像のインデックスと距離を取得することで行います。距離は類似度の代理指標となり、距離が小さいほどより類似していることを意味します。その後、距離に基づいて降順でソートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GzNEhKJ04D-F"
   },
   "outputs": [],
   "source": [
    "image_search_embedding = get_features_from_image_path([image_path])\n",
    "distances, indices = index.search(image_search_embedding.reshape(1, -1), 2) #2 signifies the number of topmost similar images to bring back\n",
    "distances = distances[0]\n",
    "indices = indices[0]\n",
    "indices_distances = list(zip(indices, distances))\n",
    "indices_distances.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O-GYQ-1QAqm"
   },
   "source": [
    "インデックスが必要です。これを使用して`image_directory`を検索し、インデックスの位置にある画像を選択してRAG用のビジョンモデルに入力するためです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-6SVzwSJVuT"
   },
   "source": [
    "そして、何が返されたかを見てみましょう（類似度の順に表示しています）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt1ZYuKDFeww"
   },
   "outputs": [],
   "source": [
    "#display similar images\n",
    "for idx, distance in indices_distances:\n",
    "    print(idx)\n",
    "    path = get_image_paths(direc, idx)[0]\n",
    "    im = Image.open(path)\n",
    "    plt.imshow(im)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPTvKIUJ2tgz"
   },
   "source": [
    "![Delta Pro2](../images/train2.jpeg)\n",
    "\n",
    "![Delta Pro3](../images/train17.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4kF2-MJQAqm"
   },
   "source": [
    "ここでは、DELTA Pro Ultra Whole House Battery Generatorが含まれている2つの画像が返されていることがわかります。画像の1つには気を散らす可能性のある背景も含まれていますが、適切な画像を見つけることができています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qc2sOKzY7yv3"
   },
   "source": [
    "# 最も類似した画像をクエリするユーザー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Sio6OR4MDjI"
   },
   "source": [
    "最も類似した画像について、その画像と説明をユーザーのクエリと共にgpt-vに渡して、購入した可能性のある技術について問い合わせできるようにします。これがビジョンモデルの力が発揮される部分で、モデルが明示的に訓練されていない一般的なクエリを投げかけても、高い精度で応答してくれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPzsRk66QAqn"
   },
   "source": [
    "以下の例では、問題となっているアイテムの容量について問い合わせを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "-_5W_xwitbr3",
    "outputId": "99a40617-0153-492a-d8b0-6782b8421e40"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The portable home battery DELTA Pro has a base capacity of 3.6kWh. This capacity can be expanded up to 25kWh with additional batteries. The image showcases the DELTA Pro, which has an impressive 3600W power capacity for AC output as well.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_path = get_image_paths(direc, indices_distances[0][0])[0]\n",
    "element = find_entry(data, 'image_path', similar_path)\n",
    "\n",
    "user_query = 'What is the capacity of this item?'\n",
    "prompt = f\"\"\"\n",
    "Below is a user query, I want you to answer the query using the description and image provided.\n",
    "\n",
    "user query:\n",
    "{user_query}\n",
    "\n",
    "description:\n",
    "{element['description']}\n",
    "\"\"\"\n",
    "image_query(prompt, similar_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIInamGaAG9L"
   },
   "source": [
    "そして、質問に答えることができていることがわかります。これは、画像を直接マッチングし、そこから関連する説明をコンテキストとして収集することによってのみ可能でした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ljrf0VKR_2q9"
   },
   "source": [
    "# 結論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PexvxTF5_7ay"
   },
   "source": [
    "このノートブックでは、CLIPモデルの使用方法、CLIPモデルを使用した画像埋め込みデータベースの作成例、セマンティック検索の実行、そして最終的にユーザークエリに対して質問に答える方法について説明しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOgRBeh6eMiq"
   },
   "source": [
    "この使用パターンの応用は多くの異なるアプリケーションドメインに広がっており、この技術をさらに向上させるために容易に改良することができます。例えば、CLIPをファインチューニングしたり、RAGと同様に検索プロセスを改善したり、GPT-Vをプロンプトエンジニアリングしたりすることができます。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
