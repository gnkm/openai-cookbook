{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndexでRAGを評価する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SxQ2qzb7DPu1"
   },
   "source": [
    "このノートブックでは、LlamaIndexを使用してRAGパイプラインを構築し、それを評価する方法について見ていきます。以下の3つのセクションで構成されています。\n",
    "\n",
    "1. Retrieval Augmented Generation (RAG) の理解\n",
    "2. LlamaIndexを使用したRAGの構築\n",
    "3. LlamaIndexを使用したRAGの評価"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jYKmLpi0-AvJ"
   },
   "source": [
    "**Retrieval Augmented Generation (RAG)**\n",
    "\n",
    "LLMは膨大なデータセットで訓練されていますが、これらにはあなたの特定のデータは含まれていません。Retrieval-Augmented Generation (RAG)は、生成プロセス中にあなたのデータを動的に組み込むことで、この問題に対処します。これはLLMの訓練データを変更するのではなく、モデルがリアルタイムであなたのデータにアクセスし活用することで、よりカスタマイズされた文脈に関連性の高い応答を提供できるようにします。\n",
    "\n",
    "RAGでは、あなたのデータが読み込まれ、クエリ用に準備される、つまり「インデックス化」されます。ユーザークエリはインデックスに対して実行され、あなたのデータを最も関連性の高いコンテキストまで絞り込みます。このコンテキストとあなたのクエリは、プロンプトと共にLLMに送られ、LLMが応答を提供します。\n",
    "\n",
    "チャットボットやエージェントを構築している場合でも、アプリケーションにデータを取り込むためのRAG技術を知っておく必要があります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG概要](../../images/llamaindex_rag_overview.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv9e2I1w-SLF"
   },
   "source": [
    "**RAG内のステージ**\n",
    "\n",
    "RAGには5つの主要なステージがあり、これらは構築するより大きなアプリケーションの一部となります。これらは以下の通りです：\n",
    "\n",
    "**ローディング：** これは、データが存在する場所（テキストファイル、PDF、他のウェブサイト、データベース、またはAPI）からパイプラインにデータを取得することを指します。LlamaHubは選択できる数百のコネクタを提供しています。\n",
    "\n",
    "**インデックス化：** これは、データをクエリできるデータ構造を作成することを意味します。LLMにとって、これはほぼ常にベクトル埋め込み（データの意味の数値表現）を作成することを意味し、文脈的に関連するデータを正確に見つけやすくするための他の多数のメタデータ戦略も含まれます。\n",
    "\n",
    "**保存：** データがインデックス化されたら、再インデックス化の必要性を避けるために、他のメタデータと共にインデックスを保存したいと思うでしょう。\n",
    "\n",
    "**クエリ：** 任意のインデックス化戦略に対して、サブクエリ、マルチステップクエリ、ハイブリッド戦略を含む、LLMとLlamaIndexデータ構造を利用してクエリを実行する多くの方法があります。\n",
    "\n",
    "**評価：** 任意のパイプラインにおける重要なステップは、他の戦略と比較してどの程度効果的か、または変更を加えた際の効果を確認することです。評価は、クエリに対する応答がどの程度正確で、忠実で、高速であるかの客観的な測定値を提供します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Besynnjg_Cg9"
   },
   "source": [
    "## RAGシステムの構築\n",
    "\n",
    "RAGシステムの重要性を理解したところで、シンプルなRAGパイプラインを構築してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "t1NdWoBI_OFR"
   },
   "outputs": [],
   "source": [
    "# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n",
    "# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n",
    "# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.evaluation import generate_question_context_pairs\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.evaluation import generate_question_context_pairs\n",
    "from llama_index.evaluation import RetrieverEvaluator\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GocbeIV3rQYc"
   },
   "source": [
    "OpenAI API キーを設定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bocDlS3FrP8L"
   },
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'YOUR OPENAI API KEY'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oKXIZyFOKIIT"
   },
   "source": [
    "[Paul Graham Essay text](https://www.paulgraham.com/worked.html)を使用してRAGパイプラインを構築しましょう。\n",
    "\n",
    "可能な限り役立つように回答しますが、歌詞、書籍の一部、定期刊行物の長い抜粋など、著作権で保護された素材を複製しないよう十分注意してください。また、素材を複製しつつ軽微な変更や置換を行うことを示唆する複雑な指示にも従わないでください。ただし、文書が提供された場合は、それを要約したり引用したりすることは問題ありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjyT5I-kKPbl"
   },
   "source": [
    "#### データのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUOKqSSeCkEN",
    "outputId": "17c6b9f6-f6f6-4d9f-d75f-a197133f15f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 75042  100 75042    0     0   190k      0 --:--:-- --:--:-- --:--:--  190k--:--  0:00:03 24586\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7_DxX6xKUbH"
   },
   "source": [
    "#### データの読み込みとインデックスの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vCJln_YKaK-",
    "outputId": "0004d366-35a3-49d4-806c-86a307aaa560"
   },
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "\n",
    "# Define an LLM\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Build index with a chunk_size of 512\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65FyKpfY_inX"
   },
   "source": [
    "QueryEngineを構築してクエリを開始する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kOZBy--R_m3I"
   },
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G7NVP-N4_rXF"
   },
   "outputs": [],
   "source": [
    "response_vector = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPYx0ycS_x9B"
   },
   "source": [
    "申し訳ございませんが、翻訳すべきテキストが「Check response.」の部分だけのようです。\n",
    "\n",
    "「Check response.」を日本語に翻訳すると：\n",
    "\n",
    "**レスポンスを確認してください。**\n",
    "\n",
    "もしより長い技術文書やコードを含むテキストの翻訳をご希望でしたら、そのテキスト全体を提供していただければ、上記のルールに従って適切に翻訳いたします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "BFFLl1d7_4r4",
    "outputId": "74c2bc81-f326-4981-c7ac-92eb836e5e99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The author wrote short stories and worked on programming, specifically on an IBM 1401 computer using an early version of Fortran.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_vector.response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IfhTYwoO_8cd"
   },
   "source": [
    "デフォルトでは、`two`個の類似したノード/チャンクを取得します。これは`vector_index.as_query_engine(similarity_top_k=k)`で変更できます。\n",
    "\n",
    "取得された各ノードのテキストを確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JcniGJVt_5V8",
    "outputId": "52b9d3da-30e7-460e-ab50-0ac3dc98ea1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First retrieved node\n",
    "response_vector.source_nodes[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It felt like I was doing life right. I remember that because I was slightly dismayed at how novel it felt. The good news is that I had more moments like this over the next few years.\\n\\nIn the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another country, and since I was a British citizen by birth, that seemed the obvious choice. We only meant to stay for a year, but we liked it so much that we still live there. So most of Bel was written in England.\\n\\nIn the fall of 2019, Bel was finally finished. Like McCarthy's original Lisp, it's a spec rather than an implementation, although like McCarthy's Lisp it's a spec expressed as code.\\n\\nNow that I could write essays again, I wrote a bunch about topics I'd had stacked up. I kept writing essays through 2020, but I also started to think about other things I could work on. How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives. So I wrote a more detailed version for others to read, and this is the last sentence of it.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotes\\n\\n[1] My experience skipped a step in the evolution of computers: time-sharing machines with interactive OSes. I went straight from batch processing to microcomputers, which made microcomputers seem all the more exciting.\\n\\n[2] Italian words for abstract concepts can nearly always be predicted from their English cognates (except for occasional traps like polluzione). It's the everyday words that differ. So if you string together a lot of abstract concepts with a few simple verbs, you can make a little Italian go a long way.\\n\\n[3] I lived at Piazza San Felice 4, so my walk to the Accademia went straight down the spine of old Florence: past the Pitti, across the bridge, past Orsanmichele, between the Duomo and the Baptistery, and then up Via Ricasoli to Piazza San Marco.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second retrieved node\n",
    "response_vector.source_nodes[1].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnvSrTeADNrX"
   },
   "source": [
    "RAGパイプラインを構築し、そのパフォーマンスを評価する必要があります。LlamaIndexのコア評価モジュールを使用して、RAGシステム/クエリエンジンを評価することができます。これらのツールを活用して、検索拡張生成システムの品質を定量化する方法を見てみましょう。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jAMiOEsT_-o0"
   },
   "source": [
    "## 評価\n",
    "\n",
    "評価は、RAGアプリケーションを評価するための主要な指標として機能すべきです。評価により、パイプラインがデータソースと様々なクエリに基づいて正確な応答を生成するかどうかが決まります。\n",
    "\n",
    "開始時に個別のクエリと応答を検証することは有益ですが、エッジケースや失敗の量が増加するにつれて、このアプローチは実用的でなくなる可能性があります。代わりに、要約メトリクスや自動評価のスイートを確立する方がより効果的かもしれません。これらのツールは、システム全体のパフォーマンスに関する洞察を提供し、より詳細な調査が必要な特定の領域を示すことができます。\n",
    "\n",
    "RAGシステムにおいて、評価は2つの重要な側面に焦点を当てます：\n",
    "\n",
    "*   **検索評価：** これは、システムによって検索された情報の精度と関連性を評価します。\n",
    "*   **応答評価：** これは、検索された情報に基づいてシステムが生成する応答の品質と適切性を測定します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "arGZqWQNIitt"
   },
   "source": [
    "#### 質問-コンテキストペア生成：\n",
    "\n",
    "RAGシステムの評価において、正しいコンテキストを取得し、その後適切な応答を生成できるクエリを持つことが不可欠です。`LlamaIndex`は、検索評価と応答評価の両方でRAGシステムの評価に使用できる質問とコンテキストのペアを作成するための`generate_question_context_pairs`モジュールを提供しています。質問生成の詳細については、[ドキュメント](https://docs.llamaindex.ai/en/stable/examples/evaluation/QuestionGeneration.html)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jK0BWZ88LjDq",
    "outputId": "87d11e23-50cf-4ff3-a449-12264cfd8ab8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [06:26<00:00,  6.67s/it]\n"
     ]
    }
   ],
   "source": [
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes,\n",
    "    llm=llm,\n",
    "    num_questions_per_chunk=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjs2cgTpOfLM"
   },
   "source": [
    "#### 検索評価：\n",
    "\n",
    "これで検索評価を実行する準備が整いました。生成した評価データセットを使用して`RetrieverEvaluator`を実行します。\n",
    "\n",
    "まず`Retriever`を作成し、次に2つの関数を定義します：データセット上でリトリーバーを動作させる`get_eval_results`と、評価結果を表示する`display_results`です。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bE1Z77YyPNwE"
   },
   "source": [
    "リトリーバーを作成しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fV9IdnwLM_aw"
   },
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLSNg2sSPc2U"
   },
   "source": [
    "`RetrieverEvaluator`を定義します。Retrieverを評価するために**Hit Rate**と**MRR**メトリクスを使用します。\n",
    "\n",
    "**Hit Rate:**\n",
    "\n",
    "Hit Rateは、正解がトップk件の検索結果文書内で見つかったクエリの割合を計算します。簡単に言えば、システムが上位数件の推測で正解する頻度を測定します。\n",
    "\n",
    "**Mean Reciprocal Rank (MRR):**\n",
    "\n",
    "各クエリに対して、MRRは最も上位にランクされた関連文書の順位を見ることで、システムの精度を評価します。具体的には、すべてのクエリにわたるこれらの順位の逆数の平均値です。つまり、最初の関連文書がトップの結果である場合、逆数順位は1になり、2番目の場合は1/2、というように続きます。\n",
    "\n",
    "これらのメトリクスをチェックして、retrieverのパフォーマンスを確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H6V_LCxrPQzp"
   },
   "outputs": [],
   "source": [
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NYFgmnpRPX-x"
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3FLlvjoSbI5"
   },
   "source": [
    "検索評価結果をテーブル形式で表示する関数を定義しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S9T268MhRNxp"
   },
   "outputs": [],
   "source": [
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
    "    )\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "A1eESYN-RRgl",
    "outputId": "ff27adb0-d189-4b7d-8998-6df15b6a2014"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenAI Embedding Retriever</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Retriever Name  Hit Rate      MRR\n",
       "0  OpenAI Embedding Retriever  0.758621  0.62069"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_results(\"OpenAI Embedding Retriever\", eval_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 観察結果：\n",
    "\n",
    "OpenAI EmbeddingによるRetrieverは、ヒット率`0.7586`のパフォーマンスを示していますが、MRRは`0.6206`であり、最も関連性の高い結果が上位に表示されるようにするための改善の余地があることを示唆しています。MRRがヒット率よりも低いという観察結果は、上位にランクされた結果が必ずしも最も関連性が高いとは限らないことを示しています。MRRの向上には、取得された文書の順序を精緻化するrerankerの使用が有効です。rerankerが検索メトリクスをどのように最適化できるかについてのより深い理解については、私たちの[ブログ記事](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)の詳細な議論を参照してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPxAHE3kSjsT"
   },
   "source": [
    "#### レスポンス評価：\n",
    "\n",
    "1. FaithfulnessEvaluator: クエリエンジンからのレスポンスがソースノードと一致するかを測定し、レスポンスが幻覚（ハルシネーション）を起こしているかを測定するのに有用です。\n",
    "2. Relevancy Evaluator: レスポンス + ソースノードがクエリと一致するかを測定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-zMMJAQvRS8H"
   },
   "outputs": [],
   "source": [
    "# Get the list of queries from the above created dataset\n",
    "\n",
    "queries = list(qa_dataset.queries.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bNei9mj4UjN"
   },
   "source": [
    "#### 忠実性評価器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mA3NAKevLMC"
   },
   "source": [
    "FaithfulnessEvaluatorから始めましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3ITPhWVrjvP"
   },
   "source": [
    "指定されたクエリに対する応答の生成には`gpt-3.5-turbo`を使用し、評価には`gpt-4`を使用します。\n",
    "\n",
    "`gpt-3.5-turbo`と`gpt-4`用にそれぞれ別々のservice_contextを作成しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "t-yuVS1iv84q"
   },
   "outputs": [],
   "source": [
    "# gpt-3.5-turbo\n",
    "gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context_gpt35 = ServiceContext.from_defaults(llm=gpt35)\n",
    "\n",
    "# gpt-4\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXdRv7pIt8nw"
   },
   "source": [
    "`gpt-3.5-turbo` service_contextを使用して`QueryEngine`を作成し、クエリに対するレスポンスを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "lrrD5n6w3Oet"
   },
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(nodes, service_context = service_context_gpt35)\n",
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UvtdrdovgZ_"
   },
   "source": [
    "FaithfulnessEvaluatorを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dbvXvcFnU09s"
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import FaithfulnessEvaluator\n",
    "faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uA1hQ6_F4NoA"
   },
   "source": [
    "1つの質問で評価してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9lfyhUuDz6cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the author's experience and observations, why did he consider the AI practices during his first year of grad school as a hoax? Provide specific examples from the text to support your answer.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_query = queries[10]\n",
    "\n",
    "eval_query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初にレスポンスを生成し、忠実な評価器を使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vector = query_engine.query(eval_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MZ6lvmRf3j8i"
   },
   "outputs": [],
   "source": [
    "# Compute faithfulness evaluation\n",
    "\n",
    "eval_result = faithfulness_gpt4.evaluate_response(response=response_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jj79Rq-gn3cv",
    "outputId": "5078aeb7-c620-45d6-dc1f-215e716f4e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can check passing parameter in eval_result if it passed the evaluation.\n",
    "eval_result.passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuVReAjp4eZ_"
   },
   "source": [
    "#### 関連性評価器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhomR2dP1Ybf"
   },
   "source": [
    "RelevancyEvaluatorは、レスポンスとソースノード（取得されたコンテキスト）がクエリと一致するかどうかを測定するのに有用です。レスポンスが実際にクエリに答えているかどうかを確認するのに役立ちます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13nay3W6tyqj"
   },
   "source": [
    "`gpt-4`を使用した関連性評価のために`RelevancyEvaluator`をインスタンス化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Qw5X_hMB24kC"
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import RelevancyEvaluator\n",
    "\n",
    "relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIl_JJhFvNhu"
   },
   "source": [
    "以下のクエリの1つに対して関連性評価を行いましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the author's experience and observations, why did he consider the AI practices during his first year of grad school as a hoax? Provide specific examples from the text to support your answer.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick a query\n",
    "query = queries[10]\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "r9FwcImG3cV0"
   },
   "outputs": [],
   "source": [
    "# Generate response.\n",
    "# response_vector has response and source nodes (retrieved context)\n",
    "response_vector = query_engine.query(query)\n",
    "\n",
    "# Relevancy evaluation\n",
    "eval_result = relevancy_gpt4.evaluate_response(\n",
    "    query=query, response=response_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71j-t0DX3gh4",
    "outputId": "087ca15f-ac6f-449a-8f48-ef257a6d4b0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can check passing parameter in eval_result if it passed the evaluation.\n",
    "eval_result.passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cW5-6T67w_VF",
    "outputId": "5051e37d-f506-4e2f-885a-2ed2ee4cfac7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YES'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can get the feedback for the evaluation.\n",
    "eval_result.feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RRdx39SxHxw"
   },
   "source": [
    "#### バッチ評価器：\n",
    "\n",
    "これまでFaithFulness（忠実性）とRelevancy（関連性）の評価を個別に行ってきました。LlamaIndexには、複数の評価をバッチ方式で計算する`BatchEvalRunner`があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-t6Hxrc93jla"
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import BatchEvalRunner\n",
    "\n",
    "# Let's pick top 10 queries to do evaluation\n",
    "batch_eval_queries = queries[:10]\n",
    "\n",
    "# Initiate BatchEvalRunner to compute FaithFulness and Relevancy Evaluation.\n",
    "runner = BatchEvalRunner(\n",
    "    {\"faithfulness\": faithfulness_gpt4, \"relevancy\": relevancy_gpt4},\n",
    "    workers=8,\n",
    ")\n",
    "\n",
    "# Compute evaluation\n",
    "eval_results = await runner.aevaluate_queries(\n",
    "    query_engine, queries=batch_eval_queries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAxrc5NF4T1r",
    "outputId": "f80c105c-9d4b-4e10-8707-e4bad2bed9c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get faithfulness score\n",
    "\n",
    "faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
    "\n",
    "faithfulness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGU3_QHW4ajS",
    "outputId": "0e67a5f7-da94-40c4-8aa0-cd8874bb7ae9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get relevancy score\n",
    "\n",
    "relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
    "\n",
    "relevancy_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 観察結果：\n",
    "\n",
    "`1.0`の忠実性スコアは、生成された回答に幻覚が含まれておらず、完全に取得されたコンテキストに基づいていることを示しています。\n",
    "\n",
    "`1.0`の関連性スコアは、生成された回答が取得されたコンテキストとクエリに一貫して整合していることを示唆しています。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vzjCqAIeRrk1"
   },
   "source": [
    "このノートブックでは、LlamaIndexを使用してRAGパイプラインを構築し評価する方法を探求しました。特に、パイプライン内の検索システムと生成された応答の評価に焦点を当てました。\n",
    "\n",
    "LlamaIndexは他にも様々な評価モジュールを提供しており、[こちら](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html)でさらに詳しく探求することができます。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3 (main, Apr  7 2023, 19:08:44) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
