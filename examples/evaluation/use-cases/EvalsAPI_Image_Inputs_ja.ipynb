{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals API: 画像入力\n",
    "\n",
    "このクックブックでは、画像ベースのタスクに対してOpenAIのEvalsフレームワークを使用する方法を説明します。Evals APIを活用して、**サンプリング**によってモデルが生成した応答を生成し、**モデル評価**（LLM as a Judge）を使用して、画像、プロンプト、参照回答に対するモデル応答を採点することで、画像とプロンプトに対するモデル生成応答を評価します。\n",
    "\n",
    "この例では、モデルが以下のことをどの程度うまく実行できるかを評価します：\n",
    "1. **画像に関するユーザープロンプトに対して適切な応答を生成する**\n",
    "3. **高品質な応答を表す参照回答と一致する**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依存関係のインストール + セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai datasets pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセット準備\n",
    "\n",
    "Hugging Faceでホストされている[VibeEval](https://huggingface.co/datasets/RekaAI/VibeEval)データセットを使用します。このデータセットには、ユーザープロンプト、付随する画像、および参照回答データのコレクションが含まれています。まず、データセットを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"RekaAI/VibeEval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関連するフィールドを抽出し、Evals APIでデータソースとして渡すためにjson形式で整理します。入力画像データは、WebのURLまたはbase64エンコードされた文字列の形式で提供できます。ここでは、提供されたWebのURLを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_data_source = []\n",
    "\n",
    "# select the first 3 examples in the dataset to use for this cookbook\n",
    "for example in dataset[\"test\"].select(range(3)):\n",
    "    evals_data_source.append({\n",
    "        \"item\": {\n",
    "            \"media_url\": example[\"media_url\"], # image web URL\n",
    "            \"reference\": example[\"reference\"], # reference answer\n",
    "            \"prompt\": example[\"prompt\"] # prompt\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データソースリストを出力すると、各アイテムは以下のような形式になります：\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"item\": {\n",
    "    \"media_url\": \"https://storage.googleapis.com/reka-annotate.appspot.com/vibe-eval/difficulty-normal_food1_7e5c2cb9c8200d70.jpg\"\n",
    "    \"reference\": \"This appears to be a classic Margherita pizza, which has the following ingredients...\"\n",
    "    \"prompt\": \"What ingredients do I need to make this?\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval設定\n",
    "\n",
    "データソースとタスクが準備できたので、evalを作成します。OpenAI Evals APIのドキュメントについては、[APIドキュメント](https://platform.openai.com/docs/evals/overview)をご覧ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalsには「Eval」と「Run」の2つの部分があります。「Eval」では、データの期待される構造とテスト基準（grader）を定義します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データソース設定\n",
    "\n",
    "私たちが収集したデータに基づいて、データソース設定は以下の通りです："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"item_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"media_url\": { \"type\": \"string\" },\n",
    "          \"reference\": { \"type\": \"string\" },\n",
    "          \"prompt\": { \"type\": \"string\" }\n",
    "        },\n",
    "        \"required\": [\"media_url\", \"reference\", \"prompt\"]\n",
    "      },\n",
    "    \"include_sample_schema\": True, # enables sampling\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テスト基準"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト基準として、グレーダー設定を行います。この例では、画像、参考回答、およびサンプルモデル応答（`sample`名前空間内）を受け取り、モデル応答が参考回答とどの程度一致しているか、および会話に対する一般的な適合性に基づいて0から1の間のスコアを出力するモデルグレーダーです。モデルグレーダーの詳細については、[API Grader docs](https://platform.openai.com/docs/api-reference/graders)をご覧ください。\n",
    "\n",
    "効果的な評価を行うためには、データとグレーダーの両方を適切に設定することが重要です。そのため、グレーダーのプロンプトを反復的に改良していくことが必要になるでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**: 画像URLフィールド / テンプレートは、画像として解釈されるために入力画像オブジェクトに配置する必要があります。そうでなければ、画像はテキスト文字列として解釈されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_config = {\n",
    "\t    \"type\": \"score_model\",\n",
    "        \"name\": \"Score Model Grader\",\n",
    "        \"input\":[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "\t\t        \"content\": \"You are an expert grader. Judge how well the model response suits the image and prompt as well as matches the meaniing of the reference answer. Output a score of 1 if great. If it's somewhat compatible, output a score around 0.5. Otherwise, give a score of 0.\"\n",
    "\t        },\n",
    "\t        {\n",
    "\t\t        \"role\": \"user\",\n",
    "\t\t        \"content\": [{ \"type\": \"input_text\", \"text\": \"Prompt: {{ item.prompt }}.\"},\n",
    "\t\t\t\t\t\t\t{ \"type\": \"input_image\", \"image_url\": \"{{ item.media_url }}\", \"detail\": \"auto\" },\n",
    "\t\t\t\t\t\t\t{ \"type\": \"input_text\", \"text\": \"Reference answer: {{ item.reference }}. Model response: {{ sample.output_text }}.\"}\n",
    "\t\t\t\t]\n",
    "\t        }\n",
    "\t\t],\n",
    "\t\t\"pass_threshold\": 0.9,\n",
    "\t    \"range\": [0, 1],\n",
    "\t    \"model\": \"o4-mini\" # model for grading; check that the model you use supports image inputs\n",
    "\t}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、evalオブジェクトを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_object = client.evals.create(\n",
    "        name=\"Image Grading\",\n",
    "        data_source_config=data_source_config,\n",
    "        testing_criteria=[grader_config],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行を作成するために、eval オブジェクト ID、データソース（つまり、先ほどコンパイルしたデータ）、およびモデルレスポンスを生成するためのサンプリングに使用するチャットメッセージ入力を渡します。なお、EvalsAPI は保存された補完や画像を含むレスポンスもデータソースとしてサポートしています。詳細については、[追加情報：ログデータソース](#additional-info-logs-data-source)セクションを参照してください。\n",
    "\n",
    "この例で使用するサンプリングメッセージ入力は以下の通りです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"type\": \"message\",\n",
    "    \"content\": {\n",
    "        \"type\": \"input_text\",\n",
    "        \"text\": \"{{ item.prompt }}\"\n",
    "      }\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"type\": \"message\",\n",
    "    \"content\": {\n",
    "        \"type\": \"input_image\",\n",
    "        \"image_url\": \"{{ item.media_url }}\",\n",
    "        \"detail\": \"auto\"\n",
    "    }\n",
    "  }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価実行を開始します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_run = client.evals.runs.create(\n",
    "        name=\"Image Input Eval Run\",\n",
    "        eval_id=eval_object.id,\n",
    "        data_source={\n",
    "            \"type\": \"responses\", # sample using responses API\n",
    "            \"source\": {\n",
    "                \"type\": \"file_content\",\n",
    "                \"content\": evals_data_source\n",
    "            },\n",
    "            \"model\": \"gpt-4o-mini\", # model used to generate the response; check that the model you use supports image inputs\n",
    "            \"input_messages\": {\n",
    "                \"type\": \"template\", \n",
    "                \"template\": sampling_messages}\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 投票結果の取得と表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行が完了したら、結果を確認できます。また、組織のOpenAI evalsダッシュボードで進行状況と結果を確認することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>reference</th>\n",
       "      <th>model_response</th>\n",
       "      <th>grading_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please provide latex code to replicate this table</td>\n",
       "      <td>Below is the latex code for your table:\\n```te...</td>\n",
       "      <td>Certainly! Below is the LaTeX code to replicat...</td>\n",
       "      <td>{\"steps\":[{\"description\":\"Assess if the provid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What ingredients do I need to make this?</td>\n",
       "      <td>This appears to be a classic Margherita pizza,...</td>\n",
       "      <td>To make a classic Margherita pizza like the on...</td>\n",
       "      <td>{\"steps\":[{\"description\":\"Check if model ident...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is this safe for a vegan to eat?</td>\n",
       "      <td>Based on the image, this dish appears to be a ...</td>\n",
       "      <td>To determine if the dish is safe for a vegan t...</td>\n",
       "      <td>{\"steps\":[{\"description\":\"Compare model respon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Please provide latex code to replicate this table   \n",
       "1           What ingredients do I need to make this?   \n",
       "2                   Is this safe for a vegan to eat?   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Below is the latex code for your table:\\n```te...   \n",
       "1  This appears to be a classic Margherita pizza,...   \n",
       "2  Based on the image, this dish appears to be a ...   \n",
       "\n",
       "                                      model_response  \\\n",
       "0  Certainly! Below is the LaTeX code to replicat...   \n",
       "1  To make a classic Margherita pizza like the on...   \n",
       "2  To determine if the dish is safe for a vegan t...   \n",
       "\n",
       "                                     grading_results  \n",
       "0  {\"steps\":[{\"description\":\"Assess if the provid...  \n",
       "1  {\"steps\":[{\"description\":\"Check if model ident...  \n",
       "2  {\"steps\":[{\"description\":\"Compare model respon...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "while True:\n",
    "    run = client.evals.runs.retrieve(run_id=eval_run.id, eval_id=eval_object.id)\n",
    "    if run.status == \"completed\" or run.status == \"failed\": # check if the run is finished\n",
    "        output_items = list(client.evals.runs.output_items.list(\n",
    "            run_id=run.id, eval_id=eval_object.id\n",
    "        ))\n",
    "        df = pd.DataFrame({\n",
    "                \"prompt\": [item.datasource_item[\"prompt\"]for item in output_items],\n",
    "                \"reference\": [item.datasource_item[\"reference\"] for item in output_items],\n",
    "                \"model_response\": [item.sample.output[0].content for item in output_items],\n",
    "                \"grading_results\": [item.results[0][\"sample\"][\"output\"][0][\"content\"]\n",
    "                                    for item in output_items]\n",
    "            })\n",
    "        display(df)\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 個別の出力項目の表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完全な出力項目を確認するには、以下のようにします。出力項目の構造は、APIドキュメントの[こちら](https://platform.openai.com/docs/api-reference/evals/run-output-item-object)で指定されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"outputitem_687833f102ec8191a6e53a5461b970c2\",\n",
      "  \"created_at\": 1752708081,\n",
      "  \"datasource_item\": {\n",
      "    \"prompt\": \"Please provide latex code to replicate this table\",\n",
      "    \"media_url\": \"https://storage.googleapis.com/reka-annotate.appspot.com/vibe-eval/difficulty-normal_table0_b312eea68bcd0de6.png\",\n",
      "    \"reference\": \"Below is the latex code for your table:\\n```tex\\n\\\\begin{table}\\n\\\\begin{tabular}{c c c c} \\\\hline  & \\\\(S2\\\\) & Expert & Layman & PoelM \\\\\\\\ \\\\cline{2-4} \\\\(S1\\\\) & Expert & \\u2013 & 54.0 & 62.7 \\\\\\\\  & Layman & 46.0 & \\u2013 & 60.7 \\\\\\\\  &,PoelM,LM,LM,LM,LM,LM,,L,M,,L,M,,L,M,,L,M,,,\\u2013&39.3 \\\\\\\\\\n[-1ex] \\\\end{tabular}\\n\\\\end{table}\\n```.\"\n",
      "  },\n",
      "  \"datasource_item_id\": 1,\n",
      "  \"eval_id\": \"eval_687833d68e888191bc4bd8b965368f22\",\n",
      "  \"object\": \"eval.run.output_item\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"name\": \"Score Model Grader-73fe48a0-8090-46eb-aa8e-d426ad074eb3\",\n",
      "      \"sample\": {\n",
      "        \"input\": [\n",
      "          {\n",
      "            \"role\": \"system\",\n",
      "            \"content\": \"You are an expert grader. Judge how well the model response suits the image and prompt as well as matches the meaniing of the reference answer. Output a score of 1 if great. If it's somewhat compatible, output a score around 0.5. Otherwise, give a score of 0.\"\n",
      "          },\n",
      "          {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": \"Prompt: Please provide latex code to replicate this table. <image>https://storage.googleapis.com/reka-annotate.appspot.com/vibe-eval/difficulty-normal_table0_b312eea68bcd0de6.png</image> Reference answer: Below is the latex code for your table:\\n```tex\\n\\\\begin{table}\\n\\\\begin{tabular}{c c c c} \\\\hline  & \\\\(S2\\\\) & Expert & Layman & PoelM \\\\\\\\ \\\\cline{2-4} \\\\(S1\\\\) & Expert & \\u2013 & 54.0 & 62.7 \\\\\\\\  & Layman & 46.0 & \\u2013 & 60.7 \\\\\\\\  &,PoelM,LM,LM,LM,LM,LM,,L,M,,L,M,,L,M,,L,M,,,\\u2013&39.3 \\\\\\\\\\n[-1ex] \\\\end{tabular}\\n\\\\end{table}\\n```.. Model response: Certainly! Below is the LaTeX code to replicate the table you provided:\\n\\n```latex\\n\\\\documentclass{article}\\n\\\\usepackage{array}\\n\\\\usepackage{multirow}\\n\\\\usepackage{booktabs}\\n\\n\\\\begin{document}\\n\\n\\\\begin{table}[ht]\\n    \\\\centering\\n    \\\\begin{tabular}{c|c|c|c}\\n        \\\\multirow{2}{*}{S1} & \\\\multirow{2}{*}{S2} & \\\\multicolumn{3}{c}{Methods} \\\\\\\\ \\n        \\\\cline{3-5}\\n        & & Expert & Layman & PoeLM \\\\\\\\\\n        \\\\hline\\n        Expert & & - & 54.0 & 62.7 \\\\\\\\\\n        Layman & & 46.0 & - & 60.7 \\\\\\\\\\n        PoeLM & & 37.3 & 39.3 & - \\\\\\\\\\n    \\\\end{tabular}\\n    \\\\caption{Comparison of different methods}\\n    \\\\label{tab:methods_comparison}\\n\\\\end{table}\\n\\n\\\\end{document}\\n```\\n\\n### Explanation:\\n- The `multirow` package is used to create the multi-row header for `S1` and `S2`.\\n- The `booktabs` package is used for improved table formatting (with `\\\\hline` for horizontal lines).\\n- Adjust the table's caption and label as needed..\"\n",
      "          }\n",
      "        ],\n",
      "        \"output\": [\n",
      "          {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"{\\\"steps\\\":[{\\\"description\\\":\\\"Assess if the provided LaTeX code correctly matches the structure of the target table, including the diagonal header, column counts, and alignment.\\\",\\\"conclusion\\\":\\\"The code fails to create the diagonal split between S1 and S2 and mismatches column counts (defines 4 columns but uses 5).\\\"},{\\\"description\\\":\\\"Check the header layout: the target table has a single diagonal cell spanning two axes and three following columns labeled Expert, Layman, PoeLM. The model uses \\\\\\\\multirow and a \\\\\\\\multicolumn block named 'Methods', which does not replicate the diagonal or correct labeling.\\\",\\\"conclusion\\\":\\\"Header structure is incorrect and does not match the prompt's table.\\\"},{\\\"description\\\":\\\"Verify the data rows: the model code includes two empty cells after S1 and before the data, misaligning all data entries relative to the intended columns.\\\",\\\"conclusion\\\":\\\"Data rows are misaligned due to incorrect column definitions.\\\"},{\\\"description\\\":\\\"Overall compatibility: the code is syntactically flawed for the target table and conceptually does not replicate the diagonal header or correct column count.\\\",\\\"conclusion\\\":\\\"The response does not satisfy the prompt.\\\"}],\\\"result\\\":0.0}\"\n",
      "          }\n",
      "        ],\n",
      "        \"finish_reason\": \"stop\",\n",
      "        \"model\": \"o4-mini-2025-04-16\",\n",
      "        \"usage\": {\n",
      "          \"total_tokens\": 2185,\n",
      "          \"completion_tokens\": 712,\n",
      "          \"prompt_tokens\": 1473,\n",
      "          \"cached_tokens\": 0\n",
      "        },\n",
      "        \"error\": null,\n",
      "        \"seed\": null,\n",
      "        \"temperature\": 1.0,\n",
      "        \"top_p\": 1.0,\n",
      "        \"reasoning_effort\": null,\n",
      "        \"max_completions_tokens\": 4096\n",
      "      },\n",
      "      \"passed\": false,\n",
      "      \"score\": 0.0\n",
      "    }\n",
      "  ],\n",
      "  \"run_id\": \"evalrun_687833dbadd081919a0f9fbfb817baf4\",\n",
      "  \"sample\": \"Sample(error=None, finish_reason='stop', input=[SampleInput(content='Please provide latex code to replicate this table', role='user'), SampleInput(content='<image>https://storage.googleapis.com/reka-annotate.appspot.com/vibe-eval/difficulty-normal_table0_b312eea68bcd0de6.png</image>', role='user')], max_completion_tokens=None, model='gpt-4o-mini-2024-07-18', output=[SampleOutput(content=\\\"Certainly! Below is the LaTeX code to replicate the table you provided:\\\\n\\\\n```latex\\\\n\\\\\\\\documentclass{article}\\\\n\\\\\\\\usepackage{array}\\\\n\\\\\\\\usepackage{multirow}\\\\n\\\\\\\\usepackage{booktabs}\\\\n\\\\n\\\\\\\\begin{document}\\\\n\\\\n\\\\\\\\begin{table}[ht]\\\\n    \\\\\\\\centering\\\\n    \\\\\\\\begin{tabular}{c|c|c|c}\\\\n        \\\\\\\\multirow{2}{*}{S1} & \\\\\\\\multirow{2}{*}{S2} & \\\\\\\\multicolumn{3}{c}{Methods} \\\\\\\\\\\\\\\\ \\\\n        \\\\\\\\cline{3-5}\\\\n        & & Expert & Layman & PoeLM \\\\\\\\\\\\\\\\\\\\n        \\\\\\\\hline\\\\n        Expert & & - & 54.0 & 62.7 \\\\\\\\\\\\\\\\\\\\n        Layman & & 46.0 & - & 60.7 \\\\\\\\\\\\\\\\\\\\n        PoeLM & & 37.3 & 39.3 & - \\\\\\\\\\\\\\\\\\\\n    \\\\\\\\end{tabular}\\\\n    \\\\\\\\caption{Comparison of different methods}\\\\n    \\\\\\\\label{tab:methods_comparison}\\\\n\\\\\\\\end{table}\\\\n\\\\n\\\\\\\\end{document}\\\\n```\\\\n\\\\n### Explanation:\\\\n- The `multirow` package is used to create the multi-row header for `S1` and `S2`.\\\\n- The `booktabs` package is used for improved table formatting (with `\\\\\\\\hline` for horizontal lines).\\\\n- Adjust the table's caption and label as needed.\\\", role='assistant')], seed=None, temperature=1.0, top_p=1.0, usage=SampleUsage(cached_tokens=0, completion_tokens=295, prompt_tokens=14187, total_tokens=14482), max_completions_tokens=4096)\",\n",
      "  \"status\": \"fail\",\n",
      "  \"_datasource_item_content_hash\": \"bb2090df47ea2ca0aa67337709ce2ff7382d639118d3358068b0cc7031c12f82\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "first_item = output_items[0]\n",
    "\n",
    "print(json.dumps(dict(first_item), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 追加情報: ログデータソース\n",
    "\n",
    "前述したように、EvalsAPIは画像を含むログ（つまり、保存された補完や応答）をデータソースとしてサポートしています。この機能を使用するには、評価設定を以下のように変更してください：\n",
    "\n",
    "評価の作成\n",
    "  - `data_source_config = { \"type\": \"logs\" }` を設定する\n",
    "  - `grader_config` のテンプレート化を修正して、ログの入力と出力を示す `{{item.input}}` および/または `{{sample.output_text}}` を使用する\n",
    "\n",
    "評価実行の作成\n",
    "  - 評価実行に対応するログを取得するために使用される `data_source` フィールドにフィルターを指定する（詳細については[ドキュメント](https://platform.openai.com/docs/api-reference/evals/createRun)を参照）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このクックブックでは、OpenAI Evals APIを使用して画像ベースのタスクを評価するワークフローについて説明しました。サンプリングとモデル採点の両方で画像入力機能を使用することで、このタスクの評価プロセスを効率化することができました。\n",
    "\n",
    "OCR精度、画像生成の採点など、皆さん独自の画像ベースのユースケースにこの手法を拡張していただけることを楽しみにしています！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
