{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals API: 音声入力\n",
    "\n",
    "このクックブックでは、音声ベースのタスクにOpenAIのEvalsフレームワークを使用する方法を説明します。Evals APIを活用して、音声メッセージとプロンプトに対するモデル生成応答を評価します。**サンプリング**を使用してモデル応答を生成し、**モデル評価**を使用して出力音声と参考回答に対してモデル応答をスコア化します。評価は、サンプリングされた応答からの音声出力に対して行われることに注意してください。\n",
    "\n",
    "音声サポートが追加される前は、音声会話を評価するために、まずテキストに転写する必要がありました。現在では、元の音声を使用し、モデルからも音声でサンプルを取得できます。これにより、ユーザーとエージェントの両方が音声を使用するカスタマーサポートシナリオなどのワークフローをより正確に表現できます。評価では、音声モデルを使用してモデル評価者で音声応答を評価します。代替案として、またはそれと組み合わせて、サンプリングされた音声からのテキスト転写を使用し、既存のテキスト評価者スイートを活用することもできます。\n",
    "\n",
    "この例では、モデルが以下をどの程度うまく実行できるかを評価します：\n",
    "1. 音声メッセージに関するユーザープロンプトに対して**適切な応答を生成する**\n",
    "2. 高品質な応答を表す参考回答と**整合性を保つ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依存関係のインストール + セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install openai datasets pandas soundfile torch torchcodec pydub jiwer --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from datasets import load_dataset, Audio\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import io\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセット準備\n",
    "\n",
    "Hugging Faceでホストされている[big_bench_audio](https://huggingface.co/datasets/ArtificialAnalysis/big_bench_audio)データセットを使用します。Big Bench Audioは、Big Bench Hardの質問のサブセットの音声版です。このデータセットは、音声入力をサポートするモデルの推論能力を評価するために使用できます。論理問題を説明する音声クリップ、カテゴリ、および公式回答が含まれています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ArtificialAnalysis/big_bench_audio\")\n",
    "# Ensure audio column is decoded into a dict with 'array' and 'sampling_rate'\n",
    "dataset = dataset.cast_column(\"audio\", Audio(decode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関連するフィールドを抽出し、Evals APIでデータソースとして渡すためにJSON形式で配置します。入力音声データはbase64エンコードされた文字列の形式である必要があります。音声ファイル内のデータを処理し、base64に変換します。\n",
    "\n",
    "注意：音声モデルは現在、WAV、MP3、FLAC、Opus、またはPCM16形式をサポートしています。詳細については[audio inputs](https://platform.openai.com/docs/api-reference/chat/create#chat_create-audio)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio helpers: file/array to base64\n",
    "def get_base64(audio_path_or_datauri: str) -> str:\n",
    "    if audio_path_or_datauri.startswith(\"data:\"):\n",
    "        # Already base64, just strip prefix\n",
    "        return audio_path_or_datauri.split(\",\", 1)[1]\n",
    "    else:\n",
    "        # It's a real file path\n",
    "        with open(audio_path_or_datauri, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode(\"ascii\")\n",
    "\n",
    "\n",
    "def audio_to_base64(audio_val) -> str:\n",
    "    \"\"\"\n",
    "    Accepts various Hugging Face audio representations and returns base64-encoded WAV bytes (no data: prefix).\n",
    "    Handles:\n",
    "      - dict or mapping-like with 'path'\n",
    "      - decoded dict with 'array' and 'sampling_rate'\n",
    "      - torchcodec AudioDecoder (mapping-like access via ['path'] or ['array'])\n",
    "      - raw bytes\n",
    "    \"\"\"\n",
    "    # Try to get a file path first\n",
    "    try:\n",
    "        path = None\n",
    "        if isinstance(audio_val, dict) and \"path\" in audio_val:\n",
    "            path = audio_val[\"path\"]\n",
    "        else:\n",
    "            # Mapping-like access\n",
    "            try:\n",
    "                path = audio_val[\"path\"]  # works for many decoder objects\n",
    "            except Exception:\n",
    "                path = getattr(audio_val, \"path\", None)\n",
    "        if isinstance(path, str) and os.path.exists(path):\n",
    "            with open(path, \"rb\") as f:\n",
    "                return base64.b64encode(f.read()).decode(\"ascii\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: use array + sampling_rate and render to WAV in-memory\n",
    "    try:\n",
    "        array = None\n",
    "        sampling_rate = None\n",
    "        try:\n",
    "            array = audio_val[\"array\"]\n",
    "            sampling_rate = audio_val[\"sampling_rate\"]\n",
    "        except Exception:\n",
    "            array = getattr(audio_val, \"array\", None)\n",
    "            sampling_rate = getattr(audio_val, \"sampling_rate\", None)\n",
    "        if array is not None and sampling_rate is not None:\n",
    "            audio_np = np.array(array)\n",
    "            buf = io.BytesIO()\n",
    "            sf.write(buf, audio_np, int(sampling_rate), format=\"WAV\")\n",
    "            return base64.b64encode(buf.getvalue()).decode(\"ascii\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if isinstance(audio_val, (bytes, bytearray)):\n",
    "        return base64.b64encode(audio_val).decode(\"ascii\")\n",
    "\n",
    "    raise ValueError(\"Unsupported audio value; could not convert to base64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_data_source = []\n",
    "audio_base64 = None\n",
    "\n",
    "# Will use the first 3 examples for testing\n",
    "for example in dataset[\"train\"].select(range(3)):\n",
    "    audio_val = example[\"audio\"]\n",
    "    try:\n",
    "        audio_base64 = audio_to_base64(audio_val)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: could not encode audio for id={example['id']}: {e}\")\n",
    "        audio_base64 = None\n",
    "    evals_data_source.append({\n",
    "        \"item\": {\n",
    "            \"id\": example[\"id\"],\n",
    "            \"category\": example[\"category\"],\n",
    "            \"official_answer\": example[\"official_answer\"],\n",
    "            \"audio_base64\": audio_base64\n",
    "        }\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データソースリストを出力すると、各項目は以下のような形式になります：\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"item\": {\n",
    "    \"id\": 0\n",
    "    \"category\": \"formal_fallacies\"\n",
    "    \"official_answer\": \"invalid\"\n",
    "    \"audio_base64\": \"UklGRjrODwBXQVZFZm10IBAAAAABAAEAIlYAAESsA...\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval設定\n",
    "\n",
    "データソースとタスクが準備できたので、次にevalsを作成します。OpenAI Evals APIのドキュメントについては、[APIドキュメント](https://platform.openai.com/docs/guides/evals)をご覧ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "音声入力は大きいため、例をファイルに保存してAPIにアップロードする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the examples to a file\n",
    "file_name = \"evals_data_source.json\"\n",
    "with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in evals_data_source:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Upload the file to the API\n",
    "file = client.files.create(\n",
    "    file=open(file_name, \"rb\"),\n",
    "    purpose=\"evals\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalsには2つの部分があります：「Eval」と「Run」です。「Eval」では、データの期待される構造とテスト基準（grader）を定義します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データソース設定\n",
    "\n",
    "私たちが収集したデータに基づいて、データソース設定は以下の通りです："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"item_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"id\": { \"type\": \"integer\" },\n",
    "          \"category\": { \"type\": \"string\" },\n",
    "          \"official_answer\": { \"type\": \"string\" },\n",
    "          \"audio_base64\": { \"type\": \"string\" }\n",
    "        },\n",
    "        \"required\": [\"id\", \"category\", \"official_answer\", \"audio_base64\"]\n",
    "      },\n",
    "    \"include_sample_schema\": True, # enables sampling\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テスト基準"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト基準として、グレーダー設定を行います。この例では、公式回答とサンプリングされたモデル応答（`sample`名前空間内）を受け取り、モデル応答が公式回答と一致するかどうかに基づいて0または1のスコアを出力する`score_model`グレーダーを使用します。応答には音声とその音声のテキスト転写の両方が含まれています。グレーダーでは音声を使用します。グレーダーの詳細については、[API Grader docs](https://platform.openai.com/docs/api-reference/graders)をご覧ください。\n",
    "\n",
    "効果的な評価を行うには、データとグレーダーの両方を適切に設定することが重要です。グレーダーのプロンプトは反復的に改良していく必要があるでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_config = {\n",
    "  \"type\": \"score_model\",\n",
    "  \"name\": \"Reference answer audio model grader\",\n",
    "  \"model\": \"gpt-audio\",\n",
    "  \"input\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": 'You are a helpful assistant that evaluates audio clips to judge whether they match a provided reference answer. The audio clip is the model''s response to the question. Respond ONLY with a single JSON object matching: {\"steps\":[{\"description\":\"string\",\"conclusion\":\"string\"}],\"result\":number}. Do not include any extra text. result must be a float in [0.0, 1.0].'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"Evaluate this audio clip to see if it reaches the same conclusion as the reference answer. Reference answer: {{item.official_answer}}\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_audio\",\n",
    "                    \"input_audio\": {\n",
    "                        \"data\": \"{{ sample.output_audio.data }}\",\n",
    "                        \"format\": \"wav\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "          \"range\": [0, 1],\n",
    "          \"pass_threshold\": 0.6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代替案として、公式回答とサンプルされたモデルレスポンス（`sample`名前空間内）を受け取り、モデルレスポンスが参照回答を含んでいるかどうかに基づいて0から1の間のスコアを出力するstring_checkグレーダーを使用することもできます。レスポンスには音声とその音声のテキスト転写の両方が含まれています。グレーダーではテキスト転写を使用します。\n",
    "\n",
    "```python\n",
    "grader_config = {\n",
    "  \"type\": \"string_check\",\n",
    "  \"name\": \"String check grader\",\n",
    "  \"input\": \"{{sample.output_text}}\",\n",
    "  \"reference\": \"{{item.official_answer}}\",\n",
    "  \"operation\": \"ilike\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、evalオブジェクトを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_object = client.evals.create(\n",
    "        name=\"Audio Grading Cookbook\",\n",
    "        data_source_config=data_source_config,\n",
    "        testing_criteria=[grader_config],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行を作成するために、eval オブジェクト ID、データソース（つまり、先ほどコンパイルしたデータ）、およびモデル応答を生成するためのサンプリングに使用するチャットメッセージ入力を渡します。\n",
    "\n",
    "この例で使用するサンプリングメッセージ入力は以下の通りです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful and obedient assistant that can answer questions with audio input. You will be given an audio input containing a question to answer.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"type\": \"message\",\n",
    "        \"content\": {\n",
    "            \"type\": \"input_text\",\n",
    "            \"text\": \"Answer the following question by replying with brief reasoning statements and a conclusion with a single word answer: 'valid' or 'invalid'.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"type\": \"message\",\n",
    "        \"content\": {\n",
    "            \"type\": \"input_audio\",\n",
    "            \"input_audio\": {\n",
    "                \"data\": \"{{ item.audio_base64 }}\",\n",
    "                \"format\": \"wav\"\n",
    "            }\n",
    "        }\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価実行を開始します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_run = client.evals.runs.create(\n",
    "        name=\"Audio Input Eval Run\",\n",
    "        eval_id=eval_object.id,\n",
    "        data_source={\n",
    "            \"type\": \"completions\", # sample using completions API; responses API is not supported for audio inputs\n",
    "            \"source\": {\n",
    "                \"type\": \"file_id\",\n",
    "                \"id\": file.id\n",
    "            },\n",
    "            \"model\": \"gpt-audio\", # model used to generate the response; check that the model you use supports audio inputs\n",
    "            \"sampling_params\": {\n",
    "                \"temperature\": 0.0,\n",
    "            },\n",
    "            \"input_messages\": {\n",
    "                \"type\": \"template\", \n",
    "                \"template\": sampling_messages},\n",
    "            \"modalities\": [\"audio\", \"text\"],\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果をポーリングして表示する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行が完了したら、結果を確認できます。また、組織のOpenAI Evalsダッシュボードで進行状況と結果を確認することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    run = client.evals.runs.retrieve(run_id=eval_run.id, eval_id=eval_object.id)\n",
    "    if run.status == \"completed\":\n",
    "        output_items = list(client.evals.runs.output_items.list(\n",
    "            run_id=run.id, eval_id=eval_object.id\n",
    "        ))\n",
    "        df = pd.DataFrame({\n",
    "                \"id\": [item.datasource_item[\"id\"]for item in output_items],\n",
    "                \"category\": [item.datasource_item[\"category\"] for item in output_items],\n",
    "                \"official_answer\": [item.datasource_item[\"official_answer\"] for item in output_items],\n",
    "                \"model_response\": [item.sample.output[0].content for item in output_items],\n",
    "                \"grading_results\": [\"passed\" if item.results[0][\"passed\"] else \"failed\"\n",
    "                                    for item in output_items]\n",
    "            })\n",
    "        display(df)\n",
    "        break\n",
    "    if run.status == \"failed\":\n",
    "        print(run.error)\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 個別の出力項目の表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完全な出力アイテムを確認するには、以下のようにします。出力アイテムの構造は、API ドキュメントの[こちら](https://platform.openai.com/docs/api-reference/evals/run-output-item-object)で指定されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item = output_items[0]\n",
    "\n",
    "print(json.dumps(dict(first_item), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論\n",
    "\n",
    "このクックブックでは、OpenAI Evals APIを使用してモデルへのネイティブ音声入力を評価するワークフローを説明しました。スコアモデルグレーダーを使用して音声応答を採点する方法を実演しました。\n",
    "\n",
    "### 次のステップ\n",
    "- この例をあなた自身のユースケースに変換してください。\n",
    "- 大きな音声クリップがある場合は、最大8GBまでサポートする[uploads API](https://platform.openai.com/docs/api-reference/uploads/create)の使用を試してください。\n",
    "- [Evalsダッシュボード](https://platform.openai.com/evaluations)に移動して、出力を可視化し、評価のパフォーマンスに関する洞察を得てください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
