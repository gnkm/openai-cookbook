{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd71cc8e",
   "metadata": {},
   "source": [
    "# カスタムデータセットを使用したMCPベースの回答の評価"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565afbb",
   "metadata": {},
   "source": [
    "このノートブックでは、OpenAI **Evals** フレームワークとカスタムのインメモリデータセットを使用して、[tiktoken](https://github.com/openai/tiktoken) GitHubリポジトリに関する質問に答えるモデルの能力を評価します。\n",
    "\n",
    "Q&Aペアのカスタムインメモリデータセットを使用し、リポジトリを認識した文脈的に正確な回答のために **MCP** ツールを活用する2つのモデル：`gpt-4.1` と `o4-mini` を比較します。\n",
    "\n",
    "**目標：**\n",
    "- カスタムデータセットを使用したOpenAI Evalsでの評価の設定と実行方法を示す\n",
    "- MCPベースのツールを活用する異なるモデルの性能を比較する\n",
    "- プロフェッショナルで再現可能な評価ワークフローのベストプラクティスを提供する\n",
    "\n",
    "_次：環境を設定し、必要なライブラリをインポートします。_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c3e8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Update OpenAI client\n",
    "%pip install --upgrade openai --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e168b9f",
   "metadata": {},
   "source": [
    "## 環境設定\n",
    "\n",
    "必要なライブラリをインポートし、OpenAIクライアントを設定することから始めます。  \n",
    "この手順により、OpenAI APIと評価に必要なすべてのユーティリティにアクセスできるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31fc4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Instantiate the OpenAI client (no custom base_url).\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\") or os.getenv(\"_OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9053623",
   "metadata": {},
   "source": [
    "## カスタム評価データセットの定義\n",
    "\n",
    "`tiktoken`リポジトリに関する質問と回答のペアからなる小さなインメモリデータセットを定義します。  \n",
    "このデータセットは、MCPツールの支援を受けて、モデルが正確で関連性の高い回答を提供する能力をテストするために使用されます。\n",
    "\n",
    "- 各項目には`query`（ユーザーの質問）と`answer`（期待される正解）が含まれています。\n",
    "- このデータセットは、あなた自身のユースケースやリポジトリに合わせて変更または拡張することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "840a9f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(limit=None):\n",
    "    items = [\n",
    "        {\n",
    "            \"query\": \"What is tiktoken?\",\n",
    "            \"answer\": \"tiktoken is a fast Byte-Pair Encoding (BPE) tokenizer designed for OpenAI models.\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"How do I install the open-source version of tiktoken?\",\n",
    "            \"answer\": \"Install it from PyPI with `pip install tiktoken`.\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"How do I get the tokenizer for a specific OpenAI model?\",\n",
    "            \"answer\": 'Call tiktoken.encoding_for_model(\"<model-name>\"), e.g. tiktoken.encoding_for_model(\"gpt-4o\").',\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"How does tiktoken perform compared to other tokenizers?\",\n",
    "            \"answer\": \"On a 1 GB GPT-2 benchmark, tiktoken runs about 3-6x faster than GPT2TokenizerFast (tokenizers==0.13.2, transformers==4.24.0).\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Why is Byte-Pair Encoding (BPE) useful for language models?\",\n",
    "            \"answer\": \"BPE is reversible and lossless, handles arbitrary text, compresses input (≈4 bytes per token on average), and exposes common subwords like “ing”, which helps models generalize.\",\n",
    "        },\n",
    "    ]\n",
    "    return items[:limit] if limit else items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8482643",
   "metadata": {},
   "source": [
    "### 採点ロジックの定義\n",
    "\n",
    "モデルの回答を評価するために、2つの採点者を使用します：\n",
    "\n",
    "- **合格/不合格採点者（LLMベース）：**  \n",
    "  モデルの回答が期待される回答（正解）と一致するか、または同じ意味を伝えているかをチェックするLLMベースの採点者。\n",
    "- **Python MCP採点者：**  \n",
    "  モデルが実際に応答中にMCPツールを使用したかどうかをチェックするPython関数（ツール使用の監査用）。\n",
    "\n",
    "  > **ベストプラクティス：**  \n",
    "  > LLMベースとプログラマティックな採点者の両方を使用することで、より堅牢で透明性の高い評価が可能になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3812d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based pass/fail grader: instructs the model to grade answers as \"pass\" or \"fail\".\n",
    "pass_fail_grader = \"\"\"\n",
    "You are a helpful assistant that grades the quality of the answer to a query about a GitHub repo.\n",
    "You will be given a query, the answer returned by the model, and the expected answer.\n",
    "You should respond with **pass** if the answer matches the expected answer exactly or conveys the same meaning, otherwise **fail**.\n",
    "\"\"\"\n",
    "\n",
    "# User prompt template for the grader, providing context for grading.\n",
    "pass_fail_grader_user_prompt = \"\"\"\n",
    "<Query>\n",
    "{{item.query}}\n",
    "</Query>\n",
    "\n",
    "<Web Search Result>\n",
    "{{sample.output_text}}\n",
    "</Web Search Result>\n",
    "\n",
    "<Ground Truth>\n",
    "{{item.answer}}\n",
    "</Ground Truth>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Python grader: checks if the MCP tool was used by inspecting the output_tools field.\n",
    "python_mcp_grader = {\n",
    "    \"type\": \"python\",\n",
    "    \"name\": \"Assert MCP was used\",\n",
    "    \"image_tag\": \"2025-05-08\",\n",
    "    \"pass_threshold\": 1.0,\n",
    "    \"source\": \"\"\"\n",
    "def grade(sample: dict, item: dict) -> float:\n",
    "    output = sample.get('output_tools', [])\n",
    "    return 1.0 if len(output) > 0 else 0.0\n",
    "\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d22eb7",
   "metadata": {},
   "source": [
    "## 評価設定の定義\n",
    "\n",
    "OpenAI Evalsフレームワークを使用して評価を設定します。\n",
    "\n",
    "この手順では以下を指定します：\n",
    "- 評価名とデータセット\n",
    "- 各項目のスキーマ（各Q&Aペアに存在するフィールド）\n",
    "- 使用するグレーダー（LLMベースおよび/またはPythonベース）\n",
    "- 合格基準とラベル\n",
    "\n",
    "> **ベストプラクティス：**  \n",
    "> 評価スキーマと採点ロジックを事前に明確に定義することで、再現性と透明性が確保されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluation definition using the OpenAI Evals client.\n",
    "logs_eval = client.evals.create(\n",
    "    name=\"MCP Eval\",\n",
    "    data_source_config={\n",
    "        \"type\": \"custom\",\n",
    "        \"item_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\"type\": \"string\"},\n",
    "                \"answer\": {\"type\": \"string\"},\n",
    "            },\n",
    "        },\n",
    "        \"include_sample_schema\": True,\n",
    "    },\n",
    "    testing_criteria=[\n",
    "        {\n",
    "            \"type\": \"label_model\",\n",
    "            \"name\": \"General Evaluator\",\n",
    "            \"model\": \"o3\",\n",
    "            \"input\": [\n",
    "                {\"role\": \"system\", \"content\": pass_fail_grader},\n",
    "                {\"role\": \"user\", \"content\": pass_fail_grader_user_prompt},\n",
    "            ],\n",
    "            \"passing_labels\": [\"pass\"],\n",
    "            \"labels\": [\"pass\", \"fail\"],\n",
    "        },\n",
    "        python_mcp_grader\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec09dbd",
   "metadata": {},
   "source": [
    "## 各モデルの評価を実行\n",
    "\n",
    "次に、各モデル（`gpt-4.1`と`o4-mini`）の評価を実行します。\n",
    "\n",
    "各実行は以下のように設定されています：\n",
    "- リポジトリ対応の回答にMCPツールを使用する\n",
    "- 公平な比較のために同じデータセットと評価設定を使用する\n",
    "- モデル固有のパラメータ（最大完了トークン数、許可されたツールなど）を指定する\n",
    "\n",
    "> **ベストプラクティス：**  \n",
    "> モデル間で評価設定を一貫して保つことで、結果が比較可能で信頼性の高いものになることが保証されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15838d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1: gpt-4.1 using MCP\n",
    "gpt_4one_responses_run = client.evals.runs.create(\n",
    "    name=\"gpt-4.1\",\n",
    "    eval_id=logs_eval.id,\n",
    "    data_source={\n",
    "        \"type\": \"responses\",\n",
    "        \"source\": {\n",
    "            \"type\": \"file_content\",\n",
    "            \"content\": [{\"item\": item} for item in get_dataset()],\n",
    "        },\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                {\n",
    "                    \"type\": \"message\",\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": \"You are a helpful assistant that searches the web and gives contextually relevant answers. Never use your tools to answer the query.\",\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"message\",\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": \"Search the web for the answer to the query {{item.query}}\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"model\": \"gpt-4.1\",\n",
    "        \"sampling_params\": {\n",
    "            \"seed\": 42,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_completions_tokens\": 10000,\n",
    "            \"top_p\": 0.9,\n",
    "            \"tools\": [\n",
    "                {\n",
    "                    \"type\": \"mcp\",\n",
    "                    \"server_label\": \"gitmcp\",\n",
    "                    \"server_url\": \"https://gitmcp.io/openai/tiktoken\",\n",
    "                    \"allowed_tools\": [\n",
    "                        \"search_tiktoken_documentation\",\n",
    "                        \"fetch_tiktoken_documentation\",\n",
    "                    ],\n",
    "                    \"require_approval\": \"never\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72190fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 2: o4-mini using MCP\n",
    "gpt_o4_mini_responses_run = client.evals.runs.create(\n",
    "    name=\"o4-mini\",\n",
    "    eval_id=logs_eval.id,\n",
    "    data_source={\n",
    "        \"type\": \"responses\",\n",
    "        \"source\": {\n",
    "            \"type\": \"file_content\",\n",
    "            \"content\": [{\"item\": item} for item in get_dataset()],\n",
    "        },\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                {\n",
    "                    \"type\": \"message\",\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": \"You are a helpful assistant that searches the web and gives contextually relevant answers.\",\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"message\",\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": \"Search the web for the answer to the query {{item.query}}\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"model\": \"o4-mini\",\n",
    "        \"sampling_params\": {\n",
    "            \"seed\": 42,\n",
    "            \"max_completions_tokens\": 10000,\n",
    "            \"tools\": [\n",
    "                {\n",
    "                    \"type\": \"mcp\",\n",
    "                    \"server_label\": \"gitmcp\",\n",
    "                    \"server_url\": \"https://gitmcp.io/openai/tiktoken\",\n",
    "                    \"allowed_tools\": [\n",
    "                        \"search_tiktoken_documentation\",\n",
    "                        \"fetch_tiktoken_documentation\",\n",
    "                    ],\n",
    "                    \"require_approval\": \"never\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d1a26",
   "metadata": {},
   "source": [
    "## 完了をポーリングして出力を取得する\n",
    "\n",
    "評価実行を開始した後、実行が完了するまでポーリングできます。\n",
    "\n",
    "このステップにより、すべてのモデル応答が処理された後にのみ結果を分析することが保証されます。\n",
    "\n",
    "> **ベストプラクティス:**  \n",
    "> 遅延を伴うポーリングは過度なAPI呼び出しを避け、効率的なリソース使用を保証します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d439589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evalrun_684769b577488191863b5a51cf4db57a completed ResultCounts(errored=0, failed=5, passed=0, total=5)\n",
      "evalrun_684769c1ad9c8191affea5aa02ef1215 completed ResultCounts(errored=0, failed=3, passed=2, total=5)\n"
     ]
    }
   ],
   "source": [
    "def poll_runs(eval_id, run_ids):\n",
    "    while True:\n",
    "        runs = [client.evals.runs.retrieve(rid, eval_id=eval_id) for rid in run_ids]\n",
    "        for run in runs:\n",
    "            print(run.id, run.status, run.result_counts)\n",
    "        if all(run.status in {\"completed\", \"failed\"} for run in runs):\n",
    "            break\n",
    "        time.sleep(5)\n",
    "    \n",
    "# Start polling both runs.\n",
    "poll_runs(logs_eval.id, [gpt_4one_responses_run.id, gpt_o4_mini_responses_run.id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddf8e0",
   "metadata": {},
   "source": [
    "## モデル出力の表示と解釈\n",
    "\n",
    "最後に、各モデルからの出力を表示して、手動での検査とさらなる分析を行います。\n",
    "\n",
    "- 各モデルの回答がデータセット内の各質問に対して出力されます。\n",
    "- 出力を並べて比較することで、品質、関連性、正確性を評価できます。\n",
    "\n",
    "以下は、両方のモデルの評価出力を示すOpenAI Evalsダッシュボードのスクリーンショットです：\n",
    "\n",
    "![Evaluation Output](../../../images/mcp_eval_output.png)\n",
    "\n",
    "評価メトリクスと結果の包括的な内訳については、ダッシュボードの「Data」タブに移動してください：\n",
    "\n",
    "![Evaluation Data Tab](../../../images/mcp_eval_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f655b",
   "metadata": {},
   "source": [
    "4.1モデルは、クエリに回答するためにツールを使用しないように構築されているため、MCPサーバーを呼び出すことはありませんでした。o4-miniモデルには明示的にツールを使用するよう指示されていませんでしたが、禁止もされていなかったため、MCPサーバーを3回呼び出しました。4.1モデルはo4モデルよりもパフォーマンスが劣っていることがわかります。また注目すべきは、o4-miniモデルが失敗した1つの例は、MCPツールが使用されなかったケースだったということです。\n",
    "\n",
    "また、手動検査とさらなる分析のために、各モデルからの出力の詳細な分析を確認することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e151b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "four_one_output = client.evals.runs.output_items.list(\n",
    "    run_id=gpt_4one_responses_run.id, eval_id=logs_eval.id\n",
    ")\n",
    "\n",
    "o4_mini_output = client.evals.runs.output_items.list(\n",
    "    run_id=gpt_o4_mini_responses_run.id, eval_id=logs_eval.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e68b016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# gpt‑4.1 Output\n",
      "Byte-Pair Encoding (BPE) is useful for language models because it provides an efficient way to handle large vocabularies and rare words. Here’s why it is valuable:\n",
      "\n",
      "1. **Efficient Tokenization:**  \n",
      "   BPE breaks down words into smaller subword units based on the frequency of character pairs in a corpus. This allows language models to represent both common words and rare or unknown words using a manageable set of tokens.\n",
      "\n",
      "2. **Reduces Out-of-Vocabulary (OOV) Issues:**  \n",
      "   Since BPE can split any word into known subword units, it greatly reduces the problem of OOV words—words that the model hasn’t seen during training.\n",
      "\n",
      "3. **Balances Vocabulary Size:**  \n",
      "   By adjusting the number of merge operations, BPE allows control over the size of the vocabulary. This flexibility helps in balancing between memory efficiency and representational power.\n",
      "\n",
      "4. **Improves Generalization:**  \n",
      "   With BPE, language models can better generalize to new words, including misspellings or new terminology, because they can process words as a sequence of subword tokens.\n",
      "\n",
      "5. **Handles Morphologically Rich Languages:**  \n",
      "   BPE is especially useful for languages with complex morphology (e.g., agglutinative languages) where words can have many forms. BPE reduces the need to memorize every possible word form.\n",
      "\n",
      "In summary, Byte-Pair Encoding is effective for language models because it enables efficient, flexible, and robust handling of text, supporting both common and rare words, and improving overall model performance.\n",
      "**Tiktoken**, developed by OpenAI, is a tokenizer specifically optimized for speed and compatibility with OpenAI's language models. Here’s how it generally compares to other popular tokenizers:\n",
      "\n",
      "### Performance\n",
      "- **Speed:** Tiktoken is significantly faster than most other Python-based tokenizers. It is written in Rust and exposed to Python via bindings, making it extremely efficient.\n",
      "- **Memory Efficiency:** Tiktoken is designed to be memory efficient, especially for large text inputs and batch processing.\n",
      "\n",
      "### Accuracy and Compatibility\n",
      "- **Model Alignment:** Tiktoken is tailored to match the tokenization logic used by OpenAI’s GPT-3, GPT-4, and related models. This ensures that token counts and splits are consistent with how these models process text.\n",
      "- **Unicode Handling:** Like other modern tokenizers (e.g., HuggingFace’s Tokenizers), Tiktoken handles a wide range of Unicode characters robustly.\n",
      "\n",
      "### Comparison to Other Tokenizers\n",
      "- **HuggingFace Tokenizers:** HuggingFace’s library is very flexible and supports a wide range of models (BERT, RoBERTa, etc.). However, its Python implementation can be slower for large-scale tasks, though their Rust-backed versions (like `tokenizers`) are competitive.\n",
      "- **NLTK/SpaCy:** These libraries are not optimized for transformer models and are generally slower and less accurate for tokenization tasks required by models like GPT.\n",
      "- **SentencePiece:** Used by models like T5 and ALBERT, SentencePiece is also fast and efficient, but its output is not compatible with OpenAI’s models.\n",
      "\n",
      "### Use Cases\n",
      "- **Best for OpenAI Models:** If you are working with OpenAI’s APIs or models, Tiktoken is the recommended tokenizer due to its speed and alignment.\n",
      "- **General Purpose:** For non-OpenAI models, HuggingFace or SentencePiece might be preferable due to broader support.\n",
      "\n",
      "### Benchmarks & Community Feedback\n",
      "- Multiple [community benchmarks](https://github.com/openai/tiktoken#performance) and [blog posts](https://www.philschmid.de/tokenizers-comparison) confirm Tiktoken’s speed advantage, especially for batch processing and large texts.\n",
      "\n",
      "**Summary:**  \n",
      "Tiktoken outperforms most tokenizers in speed when used with OpenAI models, with robust Unicode support and memory efficiency. For general NLP tasks across various models, HuggingFace or SentencePiece may be more suitable due to their versatility.\n",
      "\n",
      "**References:**  \n",
      "- [Tiktoken GitHub - Performance](https://github.com/openai/tiktoken#performance)\n",
      "- [Tokenizers Comparison Blog](https://www.philschmid.de/tokenizers-comparison)\n",
      "To get the tokenizer for a specific OpenAI model, you typically use the Hugging Face Transformers library, which provides easy access to tokenizers for OpenAI models like GPT-3, GPT-4, and others. Here’s how you can do it:\n",
      "\n",
      "**1. Using Hugging Face Transformers:**\n",
      "\n",
      "Install the library (if you haven’t already):\n",
      "```bash\n",
      "pip install transformers\n",
      "```\n",
      "\n",
      "**Example for GPT-3 (or GPT-4):**\n",
      "```python\n",
      "from transformers import AutoTokenizer\n",
      "\n",
      "# For GPT-3 (davinci), use the corresponding model name\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n",
      "\n",
      "# For GPT-4 (if available)\n",
      "# tokenizer = AutoTokenizer.from_pretrained(\"gpt-4\")\n",
      "```\n",
      "\n",
      "**2. Using OpenAI’s tiktoken library (for OpenAI API models):**\n",
      "\n",
      "Install tiktoken:\n",
      "```bash\n",
      "pip install tiktoken\n",
      "```\n",
      "\n",
      "Example for GPT-3.5-turbo or GPT-4:\n",
      "```python\n",
      "import tiktoken\n",
      "\n",
      "# For 'gpt-3.5-turbo'\n",
      "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "\n",
      "# For 'gpt-4'\n",
      "# tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
      "```\n",
      "\n",
      "**Summary:**\n",
      "- Use `transformers.AutoTokenizer` for Hugging Face models.\n",
      "- Use `tiktoken.encoding_for_model` for OpenAI API models.\n",
      "\n",
      "**References:**\n",
      "- [Hugging Face Tokenizer Documentation](https://huggingface.co/docs/transformers/main_classes/tokenizer)\n",
      "- [tiktoken Documentation](https://github.com/openai/tiktoken)\n",
      "\n",
      "Let me know if you need an example for a specific model!\n",
      "To install the open-source version of **tiktoken**, you can use Python’s package manager, pip. The open-source version is available on [PyPI](https://pypi.org/project/tiktoken/), so you can install it easily with the following command:\n",
      "\n",
      "```bash\n",
      "pip install tiktoken\n",
      "```\n",
      "\n",
      "If you want to install the latest development version directly from the GitHub repository, you can use:\n",
      "\n",
      "```bash\n",
      "pip install git+https://github.com/openai/tiktoken.git\n",
      "```\n",
      "\n",
      "**Requirements:**\n",
      "- Python 3.7 or newer\n",
      "- pip (Python package installer)\n",
      "\n",
      "**Steps:**\n",
      "1. Open your terminal or command prompt.\n",
      "2. Run one of the above commands.\n",
      "3. Once installed, you can import and use `tiktoken` in your Python scripts.\n",
      "\n",
      "**Additional Resources:**\n",
      "- [tiktoken GitHub repository](https://github.com/openai/tiktoken)\n",
      "- [tiktoken documentation](https://github.com/openai/tiktoken#readme)\n",
      "\n",
      "Let me know if you need help with a specific operating system or environment!\n",
      "Tiktoken is a fast and efficient tokenization library developed by OpenAI, primarily used for handling text input and output with language models such as GPT-3 and GPT-4. Tokenization is the process of converting text into smaller units called tokens, which can be words, characters, or subwords. Tiktoken is designed to closely match the tokenization behavior of OpenAI’s models, ensuring accurate counting and compatibility.\n",
      "\n",
      "Key features of tiktoken:\n",
      "- **Speed:** It’s written in Rust for performance and has Python bindings.\n",
      "- **Compatibility:** Matches the exact tokenization used by OpenAI models, which is important for estimating token counts and costs.\n",
      "- **Functionality:** Allows users to encode (convert text to tokens) and decode (convert tokens back to text).\n",
      "\n",
      "Tiktoken is commonly used in applications that need to interact with OpenAI’s APIs, for tasks like counting tokens to avoid exceeding API limits or optimizing prompt length. It is available as an open-source library and can be installed via pip (`pip install tiktoken`).\n",
      "\n",
      "# o4-mini Output\n",
      "Here’s a high-level comparison of OpenAI’s tiktoken vs. some of the other commonly used tokenizers:\n",
      "\n",
      "1. Implementation & Language Support  \n",
      "   • tiktoken  \n",
      "     – Rust core with Python bindings.  \n",
      "     – Implements GPT-2/GPT-3/GPT-4 byte-pair-encoding (BPE) vocabularies.  \n",
      "     – Focused on English-centric BPE; no built-in support for CJK segmentation or languages requiring character-level tokenization.  \n",
      "   • Hugging Face Tokenizers (“tokenizers” library)  \n",
      "     – Also Rust core with Python bindings.  \n",
      "     – Supports BPE, WordPiece, Unigram (SentencePiece), Metaspace, and custom vocabularies.  \n",
      "     – Broader multilingual and subword model support.  \n",
      "   • Python-only Tokenizers (e.g. GPT-2 BPE in pure Python)  \n",
      "     – Much slower, larger memory overhead, not suitable for high-throughput use.  \n",
      "\n",
      "2. Speed & Throughput  \n",
      "   • tiktoken  \n",
      "     – Benchmarks (OpenAI-internal) on a single CPU core: ~1–2 million tokens/second.  \n",
      "     – Roughly 10–20× faster than pure-Python GPT-2 BPE implementations.  \n",
      "     – Roughly 2–4× faster (or on par) with Hugging Face’s Rust tokenizers when using identical BPE models.  \n",
      "   • Hugging Face Tokenizers  \n",
      "     – In the same ballpark as tiktoken for a given BPE vocab (hundreds of thousands to a million tokens/sec).  \n",
      "     – Slightly higher startup overhead when loading models, but offers more tokenization strategies.  \n",
      "   • SentencePiece (C++) / Python bindings  \n",
      "     – Generally slower than Rust-based (tiktoken, tokenizers) – on the order of 100–300 K tokens/sec.  \n",
      "\n",
      "3. Memory & Footprint  \n",
      "   • tiktoken  \n",
      "     – Tiny binary (~1–2 MB) plus vocab files (~50 MB).  \n",
      "     – Low working memory; ideal for lightweight embedding or inference pipelines.  \n",
      "   • Hugging Face Tokenizers  \n",
      "     – Slightly larger binary (~3–5 MB) plus model files.  \n",
      "     – Offers on-disk memory-mapping for very large vocabularies.  \n",
      "   • Python-only  \n",
      "     – Larger RAM footprint during init; slower GC pauses.  \n",
      "\n",
      "4. Feature Set & Flexibility  \n",
      "   • tiktoken  \n",
      "     – “Batteries included” for OpenAI model vocabularies: GPT-2, Codex, GPT-3.5, GPT-4.  \n",
      "     – Simple API: encode/decode, count tokens.  \n",
      "     – No training or custom-vocab routines.  \n",
      "   • Hugging Face Tokenizers  \n",
      "     – Train new tokenizers (BPE, WordPiece, Unigram).  \n",
      "     – Pre- and post-processing pipelines (normalization, special tokens).  \n",
      "     – Easy integration with Transformers.  \n",
      "   • Other libraries (NLTK, spaCy, jieba, etc.)  \n",
      "     – Not directly comparable, since many perform linguistic tokenization, not subword BPE.  \n",
      "     – Far slower for BPE-style byte-pair encoding.  \n",
      "\n",
      "5. When to Use Which  \n",
      "   • tiktoken  \n",
      "     – If you’re targeting OpenAI’s GPT-family models and need maximum raw throughput/count accuracy.  \n",
      "     – You don’t need to train a new tokenizer or handle exotic language scripts.  \n",
      "   • Hugging Face Tokenizers  \n",
      "     – If you need broad language support, multiple subword algorithms, training tools, or tight HF Transformers integration.  \n",
      "   • Python-only / Other  \n",
      "     – Only if you have trivial performance needs or are experimenting in pure-Python teaching/demo settings.  \n",
      "\n",
      "Bottom line: for GPT-style BPE tokenization at scale, tiktoken is one of the fastest and most lightweight options—substantially faster than any pure-Python implementation and roughly on par (or a bit faster) than other Rust-backed libraries, at the cost of supporting only OpenAI’s pre-built vocabularies.\n",
      "Tiktoken is the open-source tokenization library that OpenAI uses to convert between text and the integer “tokens” their models (GPT-3, GPT-4, etc.) actually consume. It implements byte-pair encoding (BPE) in Rust (with Python bindings) for maximum speed and exact compatibility with OpenAI’s APIs.\n",
      "\n",
      "Key points:\n",
      "\n",
      "1. Purpose  \n",
      "   • Language models work on token IDs, not raw text.  \n",
      "   • Tiktoken maps Unicode text ↔ token IDs using the same vocabularies and BPE merges that OpenAI’s models were trained on.  \n",
      "\n",
      "2. Performance  \n",
      "   • Typically 3–6× faster than other BPE tokenizers (e.g. Hugging Face’s GPT2TokenizerFast).  \n",
      "   • Handles gigabytes of text in seconds.\n",
      "\n",
      "3. Installation  \n",
      "   pip install tiktoken\n",
      "\n",
      "4. Basic usage  \n",
      "   ```python\n",
      "   import tiktoken\n",
      "\n",
      "   # Get a specific encoding (vocabulary + merges)\n",
      "   enc = tiktoken.get_encoding(\"cl100k_base\")\n",
      "   tokens = enc.encode(\"Hello, world!\")\n",
      "   text   = enc.decode(tokens)\n",
      "   assert text == \"Hello, world!\"\n",
      "\n",
      "   # Or auto-select by OpenAI model name\n",
      "   enc = tiktoken.encoding_for_model(\"gpt-4o\")  # e.g. returns cl100k_base under the hood\n",
      "   ```\n",
      "\n",
      "5. Why BPE?  \n",
      "   • Reversible and lossless  \n",
      "   • Handles any text (even unseen words) by splitting into subword units  \n",
      "   • Compresses common substrings (e.g. “ing”, “tion”) so the model sees familiar chunks  \n",
      "\n",
      "6. Extras  \n",
      "   • Educational module (tiktoken._educational) to visualize or train simple BPEs  \n",
      "   • Extension mechanism (tiktoken_ext) to register custom encodings  \n",
      "\n",
      "7. Where to learn more  \n",
      "   • GitHub: https://github.com/openai/tiktoken  \n",
      "   • PyPI: https://pypi.org/project/tiktoken  \n",
      "   • OpenAI Cookbook example: How to count tokens with tiktoken  \n",
      "\n",
      "In short, if you’re building or billing on token usage with OpenAI’s models, tiktoken is the official, fast, and exact way to go from text ↔ tokens.\n",
      "Here are the two easiest ways to get the open-source tiktoken up and running:\n",
      "\n",
      "1. Install the released package from PyPI  \n",
      "   • (no Rust toolchain needed—prebuilt wheels for most platforms)  \n",
      "   ```bash\n",
      "   pip install tiktoken\n",
      "   ```  \n",
      "   Then in Python:  \n",
      "   ```python\n",
      "   import tiktoken\n",
      "   enc = tiktoken.get_encoding(\"cl100k_base\")\n",
      "   print(enc.encode(\"Hello, world!\"))\n",
      "   ```\n",
      "\n",
      "2. Install the bleeding-edge version straight from GitHub  \n",
      "   • (you’ll need a Rust toolchain—on macOS `brew install rust`, on Ubuntu `sudo apt install cargo`)  \n",
      "   ```bash\n",
      "   pip install git+https://github.com/openai/tiktoken.git@main\n",
      "   ```  \n",
      "   Or, if you prefer to clone & develop locally:  \n",
      "   ```bash\n",
      "   git clone https://github.com/openai/tiktoken.git\n",
      "   cd tiktoken\n",
      "   pip install -e .\n",
      "   ```\n",
      "\n",
      "That’s it! Once installed, you can use `tiktoken.get_encoding(...)` to load any of the supported tokenizers.\n",
      "To get the exact tokenizer (BPE encoding) that an OpenAI model uses, you can use the open-source tiktoken library. It provides a helper that maps model names to their correct tokenizers:\n",
      "\n",
      "1. Install tiktoken  \n",
      "   ```bash\n",
      "   pip install tiktoken\n",
      "   ```\n",
      "\n",
      "2. In Python, call encoding_for_model(model_name):  \n",
      "   ```python\n",
      "   import tiktoken\n",
      "\n",
      "   #—for a gpt-3.5-turbo or gpt-4 style model:\n",
      "   enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
      "   print(enc.name)            # e.g. \"cl100k_base\"\n",
      "   print(enc.encode(\"Hello\")) # list of token IDs\n",
      "   ```\n",
      "\n",
      "   If you already know the encoding name (e.g. “cl100k_base” for GPT-3.5/4 or “r50k_base” for GPT-2), you can also do:\n",
      "   ```python\n",
      "   enc = tiktoken.get_encoding(\"cl100k_base\")\n",
      "   ```\n",
      "\n",
      "3. In Node.js / JavaScript, use the tiktoken npm package the same way:\n",
      "   ```js\n",
      "   import { encoding_for_model } from \"tiktoken\";\n",
      "\n",
      "   const enc = await encoding_for_model(\"gpt-3.5-turbo\");\n",
      "   console.log(enc.name);       // \"cl100k_base\"\n",
      "   console.log(enc.encode(\"Hi\")); // array of token IDs\n",
      "   ```\n",
      "\n",
      "Under the hood encoding_for_model knows which BPE schema (“r50k_base”, “cl100k_base”, etc.) each OpenAI model uses and returns the right tokenizer instance.\n",
      "Byte-Pair Encoding (BPE) has become the de-facto subword tokenization method in modern language models because it strikes a practical balance between fixed, closed vocabularies (word-level tokenizers) and open, but very long sequences (character-level tokenizers).  In particular:\n",
      "\n",
      "1. Open-vocabulary coverage  \n",
      "   • Learns subword units from your corpus by iteratively merging the most frequent byte (or character) pairs.  \n",
      "   • Can represent any new or rare word as a sequence of known subwords—no “unknown token” blowups.  \n",
      "\n",
      "2. Compact vocabulary size  \n",
      "   • Vocabulary sizes on the order of 20K–100K tokens capture very common words as single tokens and rare or morphologically complex words as a few subwords.  \n",
      "   • Keeps softmax layers and embedding tables manageable in size.  \n",
      "\n",
      "3. Reduced data sparsity  \n",
      "   • Shares subwords among many words (e.g. “play,” “playing,” “replay”).  \n",
      "   • Provides better statistical estimates (fewer zero‐count tokens) and faster convergence in training.  \n",
      "\n",
      "4. Morphological and cross-lingual adaptability  \n",
      "   • Naturally splits on morpheme or syllable boundaries when those are frequent in the data.  \n",
      "   • Can be trained on multilingual corpora to share subwords across related languages.  \n",
      "\n",
      "5. Speed and simplicity  \n",
      "   • Linear-time, greedy encoding of new text (just look up merges).  \n",
      "   • Deterministic and invertible: you can reconstruct the original byte sequence exactly.\n",
      "\n",
      "In short, BPE tokenization gives you a small, fixed-size vocabulary that still generalizes to unseen words, reduces training and memory costs, and improves statistical efficiency—key ingredients for high-quality, scalable language models.\n"
     ]
    }
   ],
   "source": [
    "print('# gpt‑4.1 Output')\n",
    "for item in four_one_output:\n",
    "    print(item.sample.output[0].content)\n",
    "\n",
    "print('\\n# o4-mini Output')\n",
    "for item in o4_mini_output:\n",
    "    print(item.sample.output[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936def6",
   "metadata": {},
   "source": [
    "## どのように改善できるでしょうか？\n",
    "\n",
    "o4-miniモデルのシステムメッセージに「このタスクで正しい答えを得るためには、常にツールを使用してください。」という文言を追加したら、何が起こると思いますか？（試してみてください）\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "モデルが今度は毎回MCPツールを呼び出し、すべての答えを正しく取得するようになると予想した方は正解です！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf797a91",
   "metadata": {},
   "source": [
    "![評価データタブ](../../../images/mcp_eval_improved_output.png)\n",
    "![評価データタブ](../../../images/mcp_eval_improved_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924619e0",
   "metadata": {},
   "source": [
    "このノートブックでは、MCP ツールを活用した OpenAI Evals フレームワークを使用して、`tiktoken` リポジトリに関する技術的な質問に答える LLM の能力を評価するサンプルワークフローを実演しました。\n",
    "\n",
    "**主要なポイント:**\n",
    "- 評価用の焦点を絞ったカスタムデータセットを定義しました。\n",
    "- 堅牢な評価のために LLM ベースと Python ベースの採点者を設定しました。\n",
    "- 2つのモデル（`gpt-4.1` と `o4-mini`）を再現可能で透明性のある方法で比較しました。\n",
    "- 自動/手動検査のためにモデル出力を取得し表示しました。\n",
    "\n",
    "**次のステップ:**\n",
    "- **データセットの拡張:** より多様で挑戦的な質問を追加して、モデルの能力をより適切に評価する。\n",
    "- **結果の分析:** 合格/不合格率の要約、パフォーマンスの可視化、または強みと弱みを特定するためのエラー分析を実行する。\n",
    "- **モデル/ツールの実験:** 追加のモデルを試す、ツール設定を調整する、または他のリポジトリでテストする。\n",
    "- **レポートの自動化:** より簡単な共有と意思決定のために要約表やプロットを生成する。\n",
    "\n",
    "詳細については、[OpenAI Evals ドキュメント](https://platform.openai.com/docs/guides/evals)をご確認ください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
