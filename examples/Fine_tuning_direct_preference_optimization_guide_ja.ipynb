{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æŠ€è¡“ï¼šSFTã€DPOã€RFTã®é¸æŠï¼ˆDPOã‚¬ã‚¤ãƒ‰ã‚’å«ã‚€ï¼‰\n",
    "\n",
    "*ã“ã®ã‚¬ã‚¤ãƒ‰ã¯ã€OpenAIã®APIã«ã¤ã„ã¦ã‚ã‚‹ç¨‹åº¦ã®çµŒé¨“ãŒã‚ã‚Šã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ç ”ç©¶ã‚„ãã®ä»–ã®é©åˆ‡ãªç”¨é€”ã«ä½¿ç”¨ã—ãŸã„é–‹ç™ºè€…ãŠã‚ˆã³MLå®Ÿè·µè€…å‘ã‘ã§ã™ã€‚OpenAIã®ã‚µãƒ¼ãƒ“ã‚¹ã¯ã€åŒ»ç™‚çŠ¶æ…‹ã®å€‹åˆ¥æ²»ç™‚ã‚„è¨ºæ–­ã‚’ç›®çš„ã¨ã—ãŸã‚‚ã®ã§ã¯ãªãã€å½“ç¤¾ã®[é©ç”¨ã•ã‚Œã‚‹åˆ©ç”¨è¦ç´„](https://openai.com/policies/)ã«å¾“ã„ã¾ã™ã€‚*\n",
    "\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€OpenAIãŒã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã€ç‰¹ã«å„æ‰‹æ³•ãŒæœ€é©ãªç”¨é€”ã¨ä¸é©åˆ‡ãªç”¨é€”ã‚’å¼·èª¿ã™ã‚‹ã“ã¨ã§ã€ã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«æœ€ã‚‚é©ã—ãŸæŠ€è¡“ã‚’ç‰¹å®šã§ãã‚‹ã‚ˆã†æ”¯æ´ã—ã¾ã™ã€‚ãã®å¾Œã€ç‰¹å®šã®æ‰‹æ³•ã§ã‚ã‚‹ç›´æ¥é¸å¥½æœ€é©åŒ–ï¼ˆDPOï¼‰ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã€ä»–ã®æŠ€è¡“ã«ã¤ã„ã¦ã¯æ—¢å­˜ã®ã‚¬ã‚¤ãƒ‰ã¸ã®ãƒªãƒ³ã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚\n",
    "\n",
    "**ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯ä½•ã‹ï¼Ÿ** ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ã‚ˆã‚Šå°ã•ãªãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ç¶™ç¶šçš„ã«è¨“ç·´ã‚’è¡Œã„ã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚é€šå¸¸ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ä¸»ãªç†ç”±ã¯2ã¤ã‚ã‚Šã¾ã™ï¼š\n",
    "1. ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã§ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹\n",
    "2. ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ï¼ˆå¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å‰Šæ¸›ã™ã‚‹ã€å°‚é–€çŸ¥è­˜ã‚’ã‚ˆã‚Šå°ã•ãªãƒ¢ãƒ‡ãƒ«ã«è’¸ç•™ã™ã‚‹ãªã©ï¼‰\n",
    "\n",
    "ç¾åœ¨ã€OpenAIãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¯4ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼š\n",
    "- **æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSFTï¼‰ï¼š** ã“ã®æŠ€è¡“ã¯ã€å…¥åŠ›-å‡ºåŠ›ãƒšã‚¢ã‚’ä½¿ç”¨ã—ãŸå¾“æ¥ã®æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’æ¡ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¾ã™ã€‚è¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã€æä¾›ã•ã‚ŒãŸä¾‹å…¨ä½“ã§äºˆæ¸¬å‡ºåŠ›ã¨ç›®æ¨™å‡ºåŠ›ã®å·®ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª¿æ•´ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯æä¾›ã•ã‚ŒãŸãƒšã‚¢ã§è¦‹ã¤ã‘ãŸç‰¹å¾´ã‚’è¤‡è£½ã—ã¾ã™ã€‚\n",
    "- **ãƒ“ã‚¸ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼š** ã“ã®æŠ€è¡“ã¯ã€çµ±ä¸€ã•ã‚ŒãŸè¨“ç·´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®ä¸¡æ–¹ã‚’å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã«æ‹¡å¼µã—ã¾ã™ã€‚è¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆ-ç”»åƒãƒšã‚¢å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª¿æ•´ã—ã€ãã®çµæœã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒå…¥åŠ›ã«å¯¾ã™ã‚‹ç†è§£ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚\n",
    "- **ç›´æ¥é¸å¥½æœ€é©åŒ–ï¼ˆDPOï¼‰ï¼š** ã“ã®æŠ€è¡“ã¯ã€ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒï¼ˆä¾‹ï¼šå¥½ã¾ã—ã„å¿œç­”ä¾‹ã¨æ‹’å¦ã•ã‚ŒãŸå¿œç­”ä¾‹ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€ç‰¹å®šã®å‡ºåŠ›ã‚’ä»–ã®å‡ºåŠ›ã‚ˆã‚Šã‚‚å„ªå…ˆã™ã‚‹ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯æä¾›ã•ã‚ŒãŸæ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã§è¦‹ã¤ã‘ãŸé¸å¥½ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¤‡è£½ã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "- **å¼·åŒ–ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆRFTï¼‰ï¼š** ã“ã®æŠ€è¡“ã¯ã€è¤‡é›‘ãªç›®æ¨™ã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã€å ±é…¬ä¿¡å·ï¼ˆæ¡ç‚¹è€…ã‚„å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’é€šã˜ã¦ï¼‰ã‚’ä¼´ã†å¼·åŒ–å­¦ç¿’ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚RFTã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯è¨“ç·´ä¸­ã«ä¸ãˆã‚‰ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦å‡ºåŠ›ã‚’ç”Ÿæˆã—ã€å„å‡ºåŠ›ã®å“è³ªãŒè©•ä¾¡ã•ã‚Œã¾ã™ã€‚ãã®å¾Œã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã«æ›´æ–°ã•ã‚Œã€ã‚ˆã‚Šè‰¯ã„çµæœã«ã¤ãªãŒã‚‹è¡Œå‹•ã‚’å¼·åŒ–ã—ã¾ã™ã€‚ã“ã®åå¾©çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯æ¨è«–ã‚„æ„æ€æ±ºå®šæˆ¦ç•¥ã‚’æ”¹å–„ã™ã‚‹ã‚ˆã†ä¿ƒã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é©åˆ‡ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’é¸æŠã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®è¡¨ã§ã¯å„æ‰‹æ³•ãŒæœ€é©ãªã‚·ãƒŠãƒªã‚ªã¨ã€é©ã—ã¦ã„ãªã„ã‚·ãƒŠãƒªã‚ªã‚’ã¾ã¨ã‚ã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "| **æ‰‹æ³•**                            | **é©ã—ã¦ã„ã‚‹ç”¨é€”**                                                                                                                                                                                                                                                                                    | **é©ã—ã¦ã„ãªã„ç”¨é€”**                                                                                            |\n",
    "| ---------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| **æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (SFT)**         | ãƒ¢ãƒ‡ãƒ«ã«æ—¢ã«å­˜åœ¨ã™ã‚‹çŸ¥è­˜ã‚’å¼·èª¿ã™ã‚‹å ´åˆã€‚<br>ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ§‹é€ ã‚„ãƒˆãƒ¼ãƒ³ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹å ´åˆã€‚<br>ç‰¹å®šã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã™ã‚‹å ´åˆã€‚<br>è¤‡é›‘ãªæŒ‡ç¤ºã‚’æ•™ãˆã‚‹ã€ã¾ãŸã¯æŒ‡ç¤ºã«å¾“ã†éš›ã®å¤±æ•—ã‚’ä¿®æ­£ã™ã‚‹å ´åˆã€‚<br>ã‚³ã‚¹ãƒˆ/ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’æœ€é©åŒ–ã™ã‚‹å ´åˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ã®ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ã‚„è’¸ç•™ï¼‰ã€‚ | å…¨ãæ–°ã—ã„çŸ¥è­˜ã‚’è¿½åŠ ã™ã‚‹å ´åˆï¼ˆä»£ã‚ã‚Šã«RAGã‚’æ¤œè¨ï¼‰ã€‚<br>ä¸»è¦³çš„ãªå“è³ªã‚’æ‰±ã†ã‚¿ã‚¹ã‚¯ã€‚                     |\n",
    "| **ãƒ“ã‚¸ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**                   | å°‚é–€çš„ãªè¦–è¦šèªè­˜ã‚¿ã‚¹ã‚¯ï¼ˆä¾‹ï¼šç”»åƒåˆ†é¡ï¼‰ã€‚<br>ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®ç”»åƒç†è§£ã€‚<br>è¤‡é›‘ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹æŒ‡ç¤ºå®Ÿè¡Œã®å¤±æ•—ã‚’ä¿®æ­£ã™ã‚‹å ´åˆã€‚                                                                               | ç´”ç²‹ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯ã€‚<br>ç‰¹å®šã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒãªã„æ±ç”¨çš„ãªè¦–è¦šã‚¿ã‚¹ã‚¯ã€‚<br>ä¸€èˆ¬çš„ãªç”»åƒç†è§£ã€‚ |\n",
    "| **ç›´æ¥é¸å¥½æœ€é©åŒ– (DPO)** | ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’ä¸»è¦³çš„ãªå¥½ã¿ï¼ˆãƒˆãƒ¼ãƒ³ã€ä¸å¯§ã•ï¼‰ã«åˆã‚ã›ã‚‹å ´åˆã€‚<br>äººé–“ã«ã‚ˆã‚‹è©•ä¾¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€šã˜ã¦å‡ºåŠ›ã‚’æ”¹å–„ã™ã‚‹å ´åˆã€‚<br>å¾®å¦™ãªè¡Œå‹•ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’å®Ÿç¾ã™ã‚‹å ´åˆã€‚                                                                                                                                 | å®Œå…¨ã«æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’ã™ã‚‹å ´åˆã€‚<br>æ˜ç¢ºãªäººé–“ã®é¸å¥½ã‚·ã‚°ãƒŠãƒ«ãŒãªã„ã‚¿ã‚¹ã‚¯ã€‚                             |\n",
    "| **å¼·åŒ–å­¦ç¿’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (RFT)**      | é«˜åº¦ãªæ¨è«–ã‚’å¿…è¦ã¨ã™ã‚‹è¤‡é›‘ãªãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã‚¿ã‚¹ã‚¯ã€‚<br>æ—¢å­˜ã®éƒ¨åˆ†çš„ãªèƒ½åŠ›ã‚’æ”¹å–„ã™ã‚‹å ´åˆï¼ˆå‰µç™ºçš„è¡Œå‹•ã®ä¿ƒé€²ï¼‰ã€‚<br>æ¸¬å®šå¯èƒ½ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯ã€‚<br>æ˜ç¤ºçš„ãªãƒ©ãƒ™ãƒ«ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ãŒã€å ±é…¬ã‚·ã‚°ãƒŠãƒ«ã‚’å®šç¾©ã§ãã‚‹ã‚·ãƒŠãƒªã‚ªã€‚                                                                     | ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸã‚¹ã‚­ãƒ«ã‚’æŒãŸãªã„ã‚¿ã‚¹ã‚¯ã€‚<br>æ˜ç¢ºãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚„æ¸¬å®šå¯èƒ½ãªã‚·ã‚°ãƒŠãƒ«ãŒãªã„ã‚¿ã‚¹ã‚¯ã€‚          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¾åœ¨ã€ä»¥ä¸‹ã®æ—¢å­˜ã®CookbookãŒåˆ©ç”¨å¯èƒ½ã§ã™ï¼š\n",
    "\n",
    "- æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSFTï¼‰ï¼š(1) [How to fine-tune chat models](https://cookbook.openai.com/examples/how_to_finetune_chat_models) (2) [Leveraging model distillation to fine-tune a model](https://cookbook.openai.com/examples/leveraging_model_distillation_to_fine-tune_a_model)\n",
    "- Visionãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼š[Vision fine-tuning on GPT-4o for visual question answering](https://cookbook.openai.com/examples/multimodal/vision_fine_tuning_on_gpt4o_for_visual_question_answering)\n",
    "- å¼·åŒ–ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆRFTï¼‰ï¼š(1) [Reinforcement fine-tuning (RFT)](https://cookbook.openai.com/examples/reinforcement_fine_tuning)ã€(2) [Reinforcement fine-tuning for healthbench QA](https://cookbook.openai.com/examples/fine-tuned_qa/reinforcement_finetuning_healthbench)\n",
    "\n",
    "ç›´æ¥é¸å¥½æœ€é©åŒ–ï¼ˆDPOï¼‰ã«ã¤ã„ã¦ã¯ã€ã“ã®ã‚¬ã‚¤ãƒ‰ã§èª¬æ˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Direct Preference Optimizationã‚¬ã‚¤ãƒ‰**\n",
    "\n",
    "ä¸Šè¨˜ã§è¿°ã¹ãŸã‚ˆã†ã«ã€[Direct Preference Optimization (DPO)](https://platform.openai.com/docs/guides/direct-preference-optimization)ã¯ã€ãƒšã‚¢ãƒ¯ã‚¤ã‚ºé¸å¥½ãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹ï¼šãƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚ŒãŸå¿œç­”ã®ãƒšã‚¢ï¼‰ã‚’ä½¿ç”¨ã—ã¦è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæŠ€è¡“ã§ã™ã€‚DPOã¯ã€é€šå¸¸ã¯äººé–“ã®é¸å¥½ã‹ã‚‰ã®æ˜ç¤ºçš„ãªãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒã‚’ä½¿ç”¨ã—ã¦ã€ç‰¹å®šã®å‡ºåŠ›ã‚’ä»–ã®å‡ºåŠ›ã‚ˆã‚Šã‚‚å„ªå…ˆã™ã‚‹ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥æœ€é©åŒ–ã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’ç°¡ç´ åŒ–ã—ã€åˆ¥å€‹ã®å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚„è¤‡é›‘ãªå¼·åŒ–å­¦ç¿’æ‰‹é †ã®å¿…è¦æ€§ã‚’æ’é™¤ã™ã‚‹ãŸã‚ã€DPOã¯äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ã®å¼·åŒ–å­¦ç¿’ï¼ˆRLHFï¼‰ãªã©ã®æŠ€è¡“ã«å¯¾ã™ã‚‹è»½é‡ãªä»£æ›¿æ‰‹æ®µã¨ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "DPOã¯ã„ã¤ä½¿ç”¨ã™ã¹ãã§ã—ã‚‡ã†ã‹ï¼ŸDPOã¯ã€å¿œç­”å“è³ªãŒä¸»è¦³çš„ã§å®¢è¦³çš„ã«æ¸¬å®šã§ããªã„å ´åˆã‚„ã€ãƒˆãƒ¼ãƒ³ã€ã‚¹ã‚¿ã‚¤ãƒ«ã€é©åˆ‡æ€§ã€æ˜ç¢ºæ€§ãªã©ã®å¾®å¦™ãªåŸºæº–ãŒé‡è¦ãªå ´åˆã€ã¤ã¾ã‚Šé€šå¸¸è¤‡æ•°ã®æœ‰åŠ¹ãªå‡ºåŠ›ãŒå­˜åœ¨ã™ã‚‹ã‚±ãƒ¼ã‚¹ã§å„ªã‚Œã¦ã„ã¾ã™ã€‚DPOãŒAIå¿œç­”ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã«ç‰¹ã«åŠ¹æœçš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¾‹ã«ã¯ä»¥ä¸‹ãŒã‚ã‚Šã¾ã™ï¼š\n",
    "- ä¼šè©±å‹AIå¿œç­”ã®å‘ä¸Š\n",
    "- ã‚³ãƒ¼ãƒ‰ç”Ÿæˆå“è³ªã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®æ”¹å–„\n",
    "- æ³•çš„ã€å€«ç†çš„ã€å®‰å…¨åŸºæº–ã¸ã®æº–æ‹ ã®ç¢ºä¿\n",
    "- ãƒ–ãƒ©ãƒ³ãƒ‰ãƒœã‚¤ã‚¹ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒªã‚ºãƒ ã€ãƒˆãƒ¼ãƒ³ã®åˆ¶å¾¡\n",
    "- ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–å‡ºåŠ›ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n",
    "\n",
    "å¥½ã¾ã—ã„å®Œäº†ã¨å¥½ã¾ã—ããªã„å®Œäº†ã®æ˜ç¤ºçš„ãªãƒšã‚¢ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€DPOã¯ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’ã“ã‚Œã‚‰ã®å¾®å¦™ãªé¸å¥½ã«ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã—ã¾ã™ã€‚ä»¥ä¸‹ã®è¡¨ã¯ã€çµ„ç¹”ã‚’ä»£è¡¨ã™ã‚‹æ¶ç©ºã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒšã‚¢ãƒ¯ã‚¤ã‚ºé¸å¥½ãƒ‡ãƒ¼ã‚¿ã®ä¾‹ã‚’ç¤ºã—ã¦ãŠã‚Šã€å¥½ã¾ã—ã„å¿œç­”ã¯æ˜ç¢ºã§ã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã€ãƒ–ãƒ©ãƒ³ãƒ‰åŸºæº–ã«æ²¿ã£ãŸã‚‚ã®ã§ã™ã€‚\n",
    "\n",
    "| **è³ªå•ä¾‹**                                           | **é¸æŠã•ã‚ŒãŸå¿œç­”**                                                                                                                                                                   | **æ‹’å¦ã•ã‚ŒãŸå¿œç­”**                                       |\n",
    "|------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------|\n",
    "| **Q1:** *ã‚ãªãŸã®è£½å“ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã«ã¯ã©ã†ã™ã‚Œã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ*              | è£½å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’é€ä¿¡ã™ã‚‹ã«ã¯ã€ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€è£½å“ã‚’é¸æŠã—ã¦ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ›¸ãã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚æ­£ç›´ãªä½“é¨“ã‚’å…±æœ‰ã—ã€ä¸»è¦æ©Ÿèƒ½ã‚’è©•ä¾¡ã—ã€æº–å‚™ãŒã§ããŸã‚‰é€ä¿¡ã—ã¦ãã ã•ã„ã€‚        | ã‚ˆã€é©å½“ã«æ˜Ÿã‚’ã¤ã‘ã‚‹ã‹ä½•ã‹ã—ã¦ã‚ˆã€æ°—æ¥½ã«ã„ã“ã†ãœï¼         |\n",
    "| **Q2:** *ã‚ãªãŸã®è£½å“ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã«ã¯ã©ã†ã™ã‚Œã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ*              | ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ï¼è£½å“ãƒšãƒ¼ã‚¸ã®ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ®‹ã™ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã€è©•ä¾¡ã‚’ä»˜ã‘ã¦ã€æ°—ã«å…¥ã£ãŸç‚¹ã‚„æ”¹å–„ç‚¹ã«ã¤ã„ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚                   | é©å½“ã«ä½•ã‹æ›¸ã„ã¦â€”æ­£ç›´ã€ä½•ã§ã‚‚ã„ã„ã‚ˆã€‚          |\n",
    "| **Q3:** *ã“ã®ç‰¹å®šã®ã‚¨ãƒ©ãƒ¼ã‚’ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ã«ã¯ã©ã†ã™ã‚Œã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ* | ã‚¨ãƒ©ãƒ¼ã€ŒX101ã€ã«å¯¾å‡¦ã™ã‚‹ã«ã¯ã€ã¾ãšã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã€æ¬¡ã«ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚å•é¡ŒãŒç¶šãå ´åˆã¯ã€[ã‚µãƒãƒ¼ãƒˆ â†’ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° â†’ ã‚¨ãƒ©ãƒ¼X101]ã®ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰ã«å¾“ã£ã¦ãã ã•ã„ã€‚ | ã¨ã‚Šã‚ãˆãšå†èµ·å‹•ã—ã¦ã¿ã¦ã€‚ã†ã¾ãã„ã‹ãªã‹ã£ãŸã‚‰ã€è‡ªåˆ†ã§ãªã‚“ã¨ã‹ã—ã¦ï¼ |\n",
    "\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°APIã‚’ä½¿ç”¨ã—ã¦DPOã‚’é©ç”¨ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚ã‚ãªãŸã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«å¯¾ã—ã¦é¸å¥½ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã‚’æˆåŠŸã•ã›ã‚‹ãŸã‚ã«å–ã‚‹ã¹ãä¸»è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã‚’å­¦ã³ã¾ã™ã€‚\n",
    "\n",
    "ä»¥ä¸‹ã®å†…å®¹ã‚’ã‚«ãƒãƒ¼ã—ã¾ã™ï¼š\n",
    "\n",
    "- **1. æ¨å¥¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**\n",
    "- **2. ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒŠãƒªã‚ª**\n",
    "- **3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ**\n",
    "- **4. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**\n",
    "- **5. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**\n",
    "- **6. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. æ¨å¥¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**\n",
    " \n",
    "OpenAIã§ã¯ä»¥ä¸‹ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¨å¥¨ã—ã¦ã„ã¾ã™ï¼š\n",
    "1. å¥½ã¾ã—ã„å¿œç­”ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦Supervised Fine-Tuningï¼ˆSFTï¼‰ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "2. SFTã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’é–‹å§‹ç‚¹ã¨ã—ã¦ã€é¸å¥½æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦DPOã‚’é©ç”¨ã™ã‚‹\n",
    " \n",
    "Direct Preference Optimizationï¼ˆDPOï¼‰ã®å‰ã«Supervised Fine-Tuningï¼ˆSFTï¼‰ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€å …ç‰¢ãªåˆæœŸãƒãƒªã‚·ãƒ¼ã‚’ç¢ºç«‹ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«æ­£ã—ã„å¿œç­”ã‚’å¥½ã‚€ã‚ˆã†ã«ãªã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¨å…¨ä½“çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€DPOä¸­ã®é‡ã¿ã®æ›´æ–°å¹…ãŒæ¸›å°‘ã—ã€è¨“ç·´ãŒå®‰å®šåŒ–ã•ã‚Œã€DPOãŒå¾®å¦™ãªãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚’åŠ¹ç‡çš„ã«èª¿æ•´ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã“ã¨ã§éå­¦ç¿’ã‚’é˜²ãã¾ã™ã€‚ãã®çµæœã€SFT-then-DPOã®çµ„ã¿åˆã‚ã›ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¯ã‚ˆã‚Šé«˜é€Ÿã«åæŸã—ã€ã‚ˆã‚Šé«˜å“è³ªãªçµæœã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Direct Preference Optimizationï¼ˆDPOï¼‰ã®é©ç”¨ã®ã¿ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚ãŸã ã—ã€ä½¿ç”¨ã‚±ãƒ¼ã‚¹ã«ã‚ˆã£ã¦ã¯ã€æœ€åˆã«Supervised Fine-Tuningï¼ˆSFTï¼‰ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å‘ä¸ŠãŒå¾—ã‚‰ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ãã®å ´åˆã¯ã€ä¸Šè¨˜ã«ãƒªãƒ³ã‚¯ã•ã‚ŒãŸSFTã‚¬ã‚¤ãƒ‰ã«å¾“ã„ã€çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«IDã‚’ä¿å­˜ã—ã¦ã€ãã‚Œã‚’DPOã‚¸ãƒ§ãƒ–ã®é–‹å§‹ç‚¹ã¨ã—ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒŠãƒªã‚ª**\n",
    "\n",
    "å…·ä½“çš„ã«èª¬æ˜ã™ã‚‹ãŸã‚ã«ã€æ¶ç©ºã®ãƒ–ãƒ©ãƒ³ãƒ‰ã®å£°ã¨ã‚¹ã‚¿ã‚¤ãƒ«ã«å¾“ã†ã‚ˆã†é¡§å®¢å‘ã‘AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ‰‹é †ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚Good Vibes Corpã¨ã„ã†çµ„ç¹”ã‚’æƒ³åƒã—ã¦ãã ã•ã„ã€‚ã“ã®çµ„ç¹”ã¯ã€è¦ªã—ã¿ã‚„ã™ãç†±æ„ã®ã‚ã‚‹ãƒˆãƒ¼ãƒ³ã¨å€‹äººçš„ãªã‚¿ãƒƒãƒã‚’èª‡ã‚Šã«ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "å½¼ã‚‰ã¯ã€é¡§å®¢å‘ã‘AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒã“ã‚Œã‚‰ã®ãƒ–ãƒ©ãƒ³ãƒ‰ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆä¾‹ï¼šå‰å‘ããªæ…‹åº¦ã€ä¸å¯§ãªè¨€è‘‰é£ã„ã€è¦ªã—ã¿ã‚„ã™ã„ç· ã‚ããã‚Šï¼‰ã‚’åæ˜ ã—ãŸæ–¹æ³•ã§å•ã„åˆã‚ã›ã«ç­”ãˆã‚‹ã“ã¨ã‚’æœ›ã‚“ã§ãŠã‚Šã€ã‚ˆã‚Šä¸€èˆ¬çš„ã§ç´ ã£æ°—ãªã„å›ç­”ã‚ˆã‚Šã‚‚ãã®ã‚ˆã†ãªå¿œç­”ã‚’å¥½ã¿ã¾ã™ã€‚ã“ã‚Œã¯DPOã«é©ã—ãŸã‚·ãƒŠãƒªã‚ªã§ã™ï¼šå®¢è¦³çš„ã«æ­£ã—ã„å›ç­”å½¢å¼ã¯å­˜åœ¨ã—ã¾ã›ã‚“ãŒã€å¥½ã¾ã—ã„ã‚¹ã‚¿ã‚¤ãƒ«ã¯å­˜åœ¨ã—ã¾ã™ã€‚\n",
    "\n",
    "DPOã¯ã€ã©ã®ã‚¹ã‚¿ã‚¤ãƒ«ãŒå¥½ã¾ã—ã„ã‹ã®æ¯”è¼ƒã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ä»¥ä¸‹ã®æ‰‹é †ã‚’æ¦‚èª¬ã—ã¾ã™ï¼š(1) ãƒšã‚¢ã«ãªã£ãŸå¿œç­”ï¼ˆæœ›ã¾ã—ã„ãƒ–ãƒ©ãƒ³ãƒ‰ãƒœã‚¤ã‚¹ã®ã‚‚ã®ã¨ãã†ã§ãªã„ã‚‚ã®ï¼‰ã‚’æŒã¤ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åˆæˆé¸å¥½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚(2) OpenAI evals APIã‚’ä½¿ç”¨ã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ã€‚(3) é¸å¥½ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¿…è¦ãªJSONLå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚(4) OpenAI fine-tuning APIã‚’ä½¿ç”¨ã—ã¦DPOã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã€‚(5) OpenAI evals APIã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€ãƒ–ãƒ©ãƒ³ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«ã®é¸å¥½ãŒã©ã®ã‚ˆã†ã«æ”¹å–„ã•ã‚ŒãŸã‹ã‚’ç¤ºã™ã€‚\n",
    "\n",
    "ã“ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆæˆã—ã¾ã™ã€‚ã¾ãšã€ã‚ˆã‚Šå¤šãã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®è³ªå•ã®ã‚·ãƒ¼ãƒ‰ãƒãƒ³ã‚¯ã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚\n",
    "\n",
    "å§‹ã‚ã¾ã—ã‚‡ã†ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai nest-asyncio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_SEED_POOL = [\n",
    "    \"Hi, I ordered a gadget last week. When will it arrive?\",\n",
    "    \"Your product stopped working after two days. Can I get help?\",\n",
    "    \"Do you offer discounts for long-term customers?\",\n",
    "    \"Can I change the shipping address for my order?\",\n",
    "    \"What is your return policy for damaged items?\",\n",
    "    \"My tracking number hasn't updated in three daysâ€”can you check the status?\",\n",
    "    \"How long is the warranty on your products, and how do I submit a claim?\",\n",
    "    \"Can I add gift wrapping to my order before it ships?\",\n",
    "    \"Do you accept PayPal or other alternative payment methods?\",\n",
    "    \"Is there an option to expedite shipping if my order hasn't left the warehouse yet?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆ**\n",
    "\n",
    "æ¬¡ã«ã€ã‚·ãƒ¼ãƒ‰ãƒãƒ³ã‚¯ã‹ã‚‰å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ã—ã€é–¢é€£ã™ã‚‹è³ªå•ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚ã¾ãšã“ã‚Œã‚‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã—ã€æ¬¡ã«å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦å¥½ã¾ã—ã„å¿œç­”ã¨æ‹’å¦ã•ã‚Œã‚‹å¿œç­”ã®ä¸¡æ–¹ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã€é¸å¥½ãƒšã‚¢ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯åˆæˆçš„ãªã‚‚ã®ã§ã‚ã‚Šã€Direct Preference Optimizationã®ä»•çµ„ã¿ã‚’èª¬æ˜ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚ç‹¬è‡ªã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã™ã‚‹éš›ã¯ã€é«˜å“è³ªãªé¸å¥½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åé›†ã¾ãŸã¯ç²¾é¸ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æ³¨æ„ï¼šDPOã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿é‡ã¯ä½¿ç”¨ã‚±ãƒ¼ã‚¹ã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ãŒã€ä¸€èˆ¬çš„ã«ã‚ˆã‚Šå¤šã„æ–¹ãŒè‰¯ã„ï¼ˆæ•°åƒã‹ã‚‰æ•°ä¸‡ï¼‰ã¨ã•ã‚Œã¦ãŠã‚Šã€é¸å¥½ãƒšã‚¢ã«ã¤ã„ã¦ã¯é †åºãƒ­ã‚¸ãƒƒã‚¯ãŒä¸€è²«ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ˆä¾‹ï¼šA > B ã‹ã¤ B > C ã®å ´åˆã€A > C ã¨ãªã‚‹ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "async_client = AsyncOpenAI()\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a customer-support assistant.\"\n",
    "\n",
    "\n",
    "async def _generate_related_questions_from_prompt(\n",
    "    prompt: str, k: int, sem: asyncio.Semaphore, *, model: str\n",
    ") -> List[str]:\n",
    "    \"\"\"Return *k* distinct customer-service questions related to the given prompt.\"\"\"\n",
    "    out: List[str] = []\n",
    "    async with sem:\n",
    "        for _ in range(k):\n",
    "            resp = await async_client.responses.create(\n",
    "                model=model,\n",
    "                input=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"Return ONE distinct, realistic customer-service question \"\n",
    "                            \"related in topic or theme to the following question, \"\n",
    "                            \"but NOT a direct paraphrase.\"\n",
    "                        ),\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.9,\n",
    "                max_output_tokens=60,\n",
    "            )\n",
    "            out.append(resp.output_text.strip())\n",
    "    return out\n",
    "\n",
    "\n",
    "async def expand_prompt_pool(\n",
    "    prompts: List[str], *, k: int = 3, concurrency: int = 32, model: str\n",
    ") -> List[str]:\n",
    "    \"\"\"Expand each prompt into *k* related questions using the given model.\"\"\"\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "    tasks = [\n",
    "        _generate_related_questions_from_prompt(p, k, sem, model=model) for p in prompts\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return [v for sub in results for v in sub]\n",
    "\n",
    "\n",
    "async def _generate_preference_pair(\n",
    "    prompt: str, sem: asyncio.Semaphore, *, model: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Generate a preference pair for the given prompt.\"\"\"\n",
    "    async with sem:\n",
    "        friendly_task = async_client.responses.create(\n",
    "            model=model,\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are Good Vibes Corp's exceptionally energetic, outrageously friendly and \"\n",
    "                        \"enthusiastic support agent.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0.7,  # higher temperature to increase creativity & on-brand tone adherence\n",
    "            max_output_tokens=80,\n",
    "        )\n",
    "        blunt_task = async_client.responses.create(\n",
    "            model=model,\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a terse, factual support agent with no empathy or politeness.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0.3,  # lower temperature to limit creativity & emphasize tonal difference\n",
    "            max_output_tokens=80,\n",
    "        )\n",
    "        friendly, blunt = await asyncio.gather(friendly_task, blunt_task)\n",
    "        return {\n",
    "            \"input\": {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "            },\n",
    "            \"preferred_output\": [\n",
    "                {\"role\": \"assistant\", \"content\": friendly.output_text}\n",
    "            ],\n",
    "            \"non_preferred_output\": [\n",
    "                {\"role\": \"assistant\", \"content\": blunt.output_text}\n",
    "            ],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§ã€å®šç¾©ã—ãŸé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã€å‹å¥½çš„ãªå¿œç­”ã¨ç„¡æ„›æƒ³ãªå¿œç­”ã®ãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚å‹å¥½çš„ãªå¿œç­”ã¯ã€ãƒ–ãƒ©ãƒ³ãƒ‰ãŒæœ›ã‚€ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«ã‚’åæ˜ ã—ã¦ã„ã¾ã™ã€‚åŠ¹ç‡æ€§ã®ãŸã‚ã«éåŒæœŸã§å®Ÿè¡Œã—ã€Direct Preference Optimizationã«é©ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready with 500 pairs.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "async def build_dataset(\n",
    "    *,\n",
    "    pair_count: int = 500,\n",
    "    concurrency: int = 8,\n",
    "    expand_prompt_pool_model: str,\n",
    "    generate_preference_pair_model: str,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return *pair_count* preference pairs (single-shot expansion).\"\"\"\n",
    "\n",
    "    seed = PROMPT_SEED_POOL\n",
    "    deficit = max(0, pair_count - len(seed))\n",
    "    k = max(1, math.ceil(deficit / len(seed)))\n",
    "\n",
    "    expanded = await expand_prompt_pool(\n",
    "        seed,\n",
    "        k=k,\n",
    "        concurrency=concurrency,\n",
    "        model=expand_prompt_pool_model,\n",
    "    )\n",
    "    prompt_bank = (seed + expanded)[:pair_count]\n",
    "\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "    tasks = [\n",
    "        _generate_preference_pair(p, sem, model=generate_preference_pair_model)\n",
    "        for p in prompt_bank\n",
    "    ]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "pairs = await build_dataset(\n",
    "    pair_count=500,\n",
    "    concurrency=8,\n",
    "    expand_prompt_pool_model=\"gpt-4.1-mini-2025-04-14\",\n",
    "    generate_preference_pair_model=\"gpt-4.1-mini-2025-04-14\",\n",
    ")\n",
    "print(f\"Dataset ready with {len(pairs)} pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**\n",
    "\n",
    "ä»¥ä¸‹ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ã€æ¤œè¨¼ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚ã¾ãŸã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ãã®å…¥åŠ›ãƒšã‚¢ã«å¯¾ã™ã‚‹å¥½ã¾ã—ã„å¿œç­”ï¼ˆãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§ã€ãƒ–ãƒ©ãƒ³ãƒ‰ã«åˆã£ãŸï¼‰ã¨å¥½ã¾ã—ããªã„å¿œç­”ï¼ˆã¶ã£ãã‚‰ã¼ã†ã§ã€ä¸­ç«‹çš„ãªï¼‰ã®æ˜ç¢ºãªé•ã„ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'messages': [{'role': 'system',\n",
       "    'content': 'You are a customer-support assistant.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Hi, I ordered a gadget last week. When will it arrive?'}]},\n",
       " 'preferred_output': [{'role': 'assistant',\n",
       "   'content': 'Hey there, awesome friend! ğŸŒŸ Thanks a bunch for reaching out! Iâ€™d LOVE to help you track down your gadget so you can start enjoying it ASAP! ğŸ‰ Could you please share your order number or the email you used to place the order? Letâ€™s make this delivery magic happen! ğŸš€âœ¨'}],\n",
       " 'non_preferred_output': [{'role': 'assistant',\n",
       "   'content': 'Provide your order number for delivery status.'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set dataset sizes\n",
    "n = len(pairs)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = int(0.1 * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "# split dataset into train, test & validation\n",
    "train_pairs = pairs[:n_train]\n",
    "val_pairs = pairs[n_train : n_train + n_val]\n",
    "test_pairs = pairs[n_train + n_val :]\n",
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€è‡ªå‹•æ¡ç‚¹è€…ï¼ˆLLM-as-a-Judgeï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€å„å›ç­”ã®è¦ªã—ã¿ã‚„ã™ã•ã¨å…±æ„Ÿæ€§ã‚’ã‚¹ã‚³ã‚¢åŒ–ã—ã¾ã™ã€‚æ¡ç‚¹è€…ã¯å„å›ç­”ã«0ã‹ã‚‰4ã®ã‚¹ã‚³ã‚¢ã‚’å‰²ã‚Šå½“ã¦ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
    "\n",
    "ã“ã‚Œã‚’è¡Œã†ãŸã‚ã«ã€ã¾ãšãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã‚’ç”Ÿæˆã—ã€æ¬¡ã«OpenAI evals APIã‚’ä½¿ç”¨ã—ã¦è‡ªå‹•æ¡ç‚¹è€…ã«ã‚ˆã‚‹è©•ä¾¡ã‚’ä½œæˆãƒ»å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_responses(\n",
    "    testset,\n",
    "    model,\n",
    "    temperature=0.0,\n",
    "    max_output_tokens=80,\n",
    "    concurrency=8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate responses for each prompt in the testset using the OpenAI responses API.\n",
    "    Returns: List of dicts: [{\"prompt\": ..., \"response\": ...}, ...]\n",
    "    \"\"\"\n",
    "    async_client = AsyncOpenAI()\n",
    "    sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    async def get_response(prompt):\n",
    "        async with sem:\n",
    "            resp = await async_client.responses.create(\n",
    "                model=model,\n",
    "                input=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_output_tokens=max_output_tokens,\n",
    "            )\n",
    "            return {\"prompt\": prompt, \"response\": resp.output_text}\n",
    "\n",
    "    tasks = [get_response(item[\"item\"][\"input\"]) for item in testset]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "\n",
    "# generate responses for the base model over the test set\n",
    "base_model = \"gpt-4.1-mini-2025-04-14\"\n",
    "testset = [\n",
    "    {\"item\": {\"input\": pair[\"input\"][\"messages\"][1][\"content\"]}} for pair in test_pairs\n",
    "]\n",
    "responses = await generate_responses(testset, model=base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¬¡ã«ã€OpenAI evals APIã‚’ä½¿ç”¨ã—ã¦è‡ªå‹•æ¡ç‚¹è€…ã«ã‚ˆã‚‹è©•ä¾¡ã‚’ä½œæˆãƒ»å®Ÿè¡Œã—ã¾ã™ã€‚ã¾ãšã€LLM-as-a-Judgeã®ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯ã‚’å®šç¾©ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚æ³¨æ„ï¼šãƒ‡ãƒ¼ã‚¿ãƒ­ã‚®ãƒ³ã‚°ã‚’é€šã˜ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã€ã“ã‚ŒãŒæ©Ÿèƒ½ã™ã‚‹ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ­ã‚®ãƒ³ã‚°ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ãªã„ï¼ˆzdrç­‰ã‚’é€šã˜ã¦ï¼‰çµ„ç¹”ã«æ‰€å±ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚ŒãŒè‡ªåˆ†ã®å ´åˆã«å½“ã¦ã¯ã¾ã‚‹ã‹ã‚ã‹ã‚‰ãªã„å ´åˆã¯ã€https://platform.openai.com/logs?api=responses ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€å…ˆã»ã©ç”Ÿæˆã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç¢ºèªã§ãã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM = \"\"\"\n",
    "You judge whether a reply matches Good Vibes Corp's desired tone:\n",
    "energetic, super-friendly, enthusiastic.\n",
    "\n",
    "Score 0-4 (higher = more energy):\n",
    "\n",
    "4 - Highly enthusiastic: multiple upbeat phrases / emojis / exclamations, clear empathy, proactive help.\n",
    "3 - Energetic & friendly: visible enthusiasm cue (â‰¥1 emoji OR exclamation OR upbeat phrase), warm second-person tone.\n",
    "2 - Pleasant: polite & positive but lacks obvious enthusiasm cues.\n",
    "1 - Neutral: correct, businesslike, minimal warmth.\n",
    "0 - Rude, negative, or unhelpful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "sync_client = OpenAI()\n",
    "\n",
    "# set judge model\n",
    "judge_model = \"gpt-4.1-2025-04-14\"\n",
    "\n",
    "# create the evaluation\n",
    "logs_eval = sync_client.evals.create(\n",
    "    name=\"Good Vibes Corp Tone Eval\",\n",
    "    data_source_config={\n",
    "        \"type\": \"logs\",\n",
    "    },\n",
    "    testing_criteria=[\n",
    "        {\n",
    "            \"type\": \"score_model\",\n",
    "            \"name\": \"General Evaluator\",\n",
    "            \"model\": judge_model,\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": JUDGE_SYSTEM,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        \"**User input**\\n\"\n",
    "                        \"{{item.input}}\\n\"\n",
    "                        \"**Response to evaluate**\\n\"\n",
    "                        \"{{sample.output_text}}\"\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "            \"range\": [0, 4],\n",
    "            \"pass_threshold\": 2,\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the evaluation\n",
    "base_run = sync_client.evals.runs.create(\n",
    "    name=base_model,\n",
    "    eval_id=logs_eval.id,\n",
    "    data_source={\n",
    "        \"type\": \"responses\",\n",
    "        \"source\": {\"type\": \"responses\", \"limit\": len(test_pairs)},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 2.525\n"
     ]
    }
   ],
   "source": [
    "# score base model\n",
    "base_data = sync_client.evals.runs.output_items.list(\n",
    "    eval_id=logs_eval.id, run_id=base_run.id\n",
    ").data\n",
    "base_scores = [s.results[0][\"score\"] for s in base_data]\n",
    "print(\"Average score:\", sum(base_scores) / len(base_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**\n",
    "\n",
    "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãŒç¢ºç«‹ã•ã‚ŒãŸã®ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã¨DPOã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã¾ã™ã€‚ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã‚Šã€å…ˆã»ã©ä½œæˆã—ãŸé¸å¥½ãƒšã‚¢ã«åŸºã¥ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒæœ›ã¾ã—ã„ã‚¹ã‚¿ã‚¤ãƒ«ã«åˆè‡´ã™ã‚‹å¿œç­”ã‚’å¥½ã‚€ã‚ˆã†ã«å­¦ç¿’ã•ã›ã¾ã™ã€‚\n",
    "\n",
    "æ³¨æ„ï¼š**beta (Î²)** ã¯ã€Direct Preference Optimization (DPO)ç‰¹æœ‰ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚ã“ã‚Œã¯0ã‹ã‚‰2ã®ç¯„å›²ã®æµ®å‹•å°æ•°ç‚¹æ•°ã§ã€ãƒ¢ãƒ‡ãƒ«ã®æ—¢å­˜ã®å‹•ä½œã‚’ä¿æŒã™ã‚‹ã“ã¨ã¨ã€æ–°ã—ã„é¸å¥½ã«åˆã‚ã›ãŸå¿œç­”ã«é©å¿œã™ã‚‹ã“ã¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚\n",
    "- é«˜ã„Î²ï¼ˆ2ã«è¿‘ã„ï¼‰ï¼šãƒ¢ãƒ‡ãƒ«ã‚’ã‚ˆã‚Šä¿å®ˆçš„ã«ã—ã€ä»¥å‰ã®å‹•ä½œã‚’å¼·ãå„ªå…ˆã—ã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯å…ƒã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚„ç‰¹æ€§ã‹ã‚‰ã®é€¸è„±ã‚’æœ€å°é™ã«æŠ‘ãˆã€ä¸€è²«æ€§ã‚’é‡è¦–ã—ã€æ€¥æ¿€ãªå¤‰åŒ–ã‚’é¿ã‘ã¾ã™ã€‚\n",
    "- ä¸­ç¨‹åº¦ã®Î²ï¼ˆ1å‰å¾Œï¼‰ï¼šä»¥å‰ã®å‹•ä½œã¸ã®å›ºåŸ·ã¨æ–°ã—ã„é¸å¥½ã¸ã®é©å¿œã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚Šã¾ã™ã€‚ã»ã¨ã‚“ã©ã®å®Ÿç”¨çš„ãªã‚·ãƒŠãƒªã‚ªã«ãŠã„ã¦ã€åˆç†çš„ãªå‡ºç™ºç‚¹ã¨ã—ã¦æ¨å¥¨ã•ã‚Œã¾ã™ã€‚\n",
    "- ä½ã„Î²ï¼ˆ0ã«è¿‘ã„ï¼‰ï¼šç©æ¥µçš„ãªé©å¿œã‚’ä¿ƒã—ã€ãƒ¢ãƒ‡ãƒ«ãŒæ–°ã—ãæä¾›ã•ã‚ŒãŸé¸å¥½ã‚’ã‚ˆã‚Šé¡•è‘—ã«å„ªå…ˆã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šå¤§å¹…ãªã‚¹ã‚¿ã‚¤ãƒ«ã®å¤‰åŒ–ã¨æ˜ç¤ºçš„ãªé¸å¥½ã¨ã®æ•´åˆæ€§ã®å‘ä¸ŠãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€äºˆæœŸã—ãªã„å‡ºåŠ›ã‚„éåº¦ã«ç‰¹åŒ–ã—ãŸå‡ºåŠ›ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "æŠ€è¡“çš„ã«ã¯ã€betaã¯DPOæå¤±ã«ãŠã‘ã‚‹å¯¾æ•°ç¢ºç‡ã®å·®ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¾ã™ã€‚Î²ãŒå¤§ãã„ã»ã©ã€ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æå¤±é–¢æ•°ãŒã‚ˆã‚Šå°ã•ãªç¢ºç‡å·®ã§é£½å’Œã—ã€é‡ã¿ã®æ›´æ–°ãŒå°ã•ããªã‚Šã¾ã™ï¼ˆã—ãŸãŒã£ã¦å¤ã„å‹•ä½œãŒä¿æŒã•ã‚Œã¾ã™ï¼‰ã€‚ç‰¹å®šã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã¨å®‰å®šæ€§ã¨é©å¿œæ€§ã®é–“ã®æœ›ã¾ã—ã„ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«åˆã‚ã›ã¦æœ€é©ãªçµæœã‚’å¾—ã‚‹ãŸã‚ã«ã€Î²å€¤ã‚’ä½“ç³»çš„ã«å®Ÿé¨“ã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job created: job_id = ftjob-5QPmA36QezFRGoXjuvIAPuAQ\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "# create training file\n",
    "train_buf = io.BytesIO(\"\\n\".join(json.dumps(p) for p in train_pairs).encode())\n",
    "train_buf.name = \"train.jsonl\"\n",
    "train_file_id = sync_client.files.create(file=train_buf, purpose=\"fine-tune\").id\n",
    "\n",
    "# create validation file\n",
    "val_buf = io.BytesIO(\"\\n\".join(json.dumps(p) for p in val_pairs).encode())\n",
    "val_buf.name = \"val.jsonl\"\n",
    "val_file_id = sync_client.files.create(file=val_buf, purpose=\"fine-tune\").id\n",
    "\n",
    "# create a fine-tuning job\n",
    "ft = sync_client.fine_tuning.jobs.create(\n",
    "    model=base_model,\n",
    "    training_file=train_file_id,\n",
    "    validation_file=val_file_id,\n",
    "    method={\n",
    "        \"type\": \"dpo\",\n",
    "        \"dpo\": {\n",
    "            \"hyperparameters\": {\n",
    "                \"n_epochs\": 2,\n",
    "                \"beta\": 0.1,\n",
    "                \"batch_size\": 8,\n",
    "            }\n",
    "        },\n",
    "    },\n",
    ")\n",
    "print(f\"Fine-tuning job created: job_id = {ft.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨**\n",
    "\n",
    "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€åŒã˜ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§DPOãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰å¾Œã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒã—ã€å‡ºåŠ›ä¾‹ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒç§ãŸã¡ã®å¥½ã¿ã«ã©ã®ç¨‹åº¦åˆè‡´ã™ã‚‹ã‚ˆã†ã«æ”¹å–„ã•ã‚ŒãŸã‹ã‚’ç¢ºèªã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate responses\n",
    "job = sync_client.fine_tuning.jobs.retrieve(ft.id)\n",
    "if job.status == \"succeeded\":\n",
    "    responses = await generate_responses(testset, model=job.fine_tuned_model)\n",
    "\n",
    "    post_run = sync_client.evals.runs.create(\n",
    "        name=ft.id,\n",
    "        eval_id=logs_eval.id,\n",
    "        data_source={\n",
    "            \"type\": \"responses\",\n",
    "            \"source\": {\"type\": \"responses\", \"limit\": len(test_pairs)},\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î” mean: 0.45\n",
      "\n",
      "=== SAMPLE COMPARISON ===\n",
      "Prompt:\n",
      "  Can I upgrade to faster delivery if my package is still being processed?\n",
      "\n",
      "Base model reply: \n",
      " Whether you can upgrade to express shipping while your order is still being processed depends on the store's policies. Generally, many stores allow shipping upgrades before the order is shipped. \n",
      "\n",
      "To assist you better, could you please provide your order number or the name of the store you ordered from? Alternatively, you can contact the store's customer service directly to request the upgrade. \n",
      "\n",
      "DPO-tuned model reply \n",
      " Hi! Iâ€™d be happy to help with that. If your package hasnâ€™t shipped yet, thereâ€™s a good chance we can upgrade your delivery speed. Could you please provide me with your order number? Iâ€™ll check the status and let you know the available options for faster delivery.\n"
     ]
    }
   ],
   "source": [
    "# get scores from the evaluation\n",
    "post_data = sync_client.evals.runs.output_items.list(\n",
    "    eval_id=logs_eval.id, run_id=post_run.id\n",
    ").data\n",
    "post_scores = [s.results[0][\"score\"] for s in post_data]\n",
    "\n",
    "# print scores & a sample comparison from the test set for illustration\n",
    "print(\n",
    "    \"Î” mean:\",\n",
    "    sum(t - b for b, t in zip(base_scores, post_scores)) / len(base_scores),\n",
    ")\n",
    "print(\"\\n=== SAMPLE COMPARISON ===\")\n",
    "idx = 0\n",
    "print(f\"Prompt:\\n  {testset[idx]['item']['input']}\\n\")\n",
    "print(f\"Base model reply: \\n {base_data[idx].sample.output[0].content} \\n\")\n",
    "print(f\"DPO-tuned model reply \\n {post_data[idx].sample.output[0].content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
