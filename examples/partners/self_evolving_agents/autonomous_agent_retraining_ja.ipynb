{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己進化エージェント：自律エージェントの再訓練のためのクックブック\n",
    "\n",
    "## 概要\n",
    "\n",
    "エージェントシステムは、エッジケースの診断と失敗の修正を人間に依存するため、概念実証の後にしばしば停滞に陥ります。このクックブックでは、これらの問題を捕捉し、フィードバックから学習し、改善を本番環境のようなワークフローに反映させる、反復可能な再訓練ループを紹介します。このアプローチは規制された医療文書作成タスクを基盤としていますが、パターンは正確性、監査可能性、迅速な反復が求められる任意のドメインに一般化できます。\n",
    "\n",
    "### 学習内容\n",
    "- 自律エージェントが本番環境への準備不足に陥る理由を診断し、測定可能なフィードバック信号でそれを計測する方法\n",
    "- 迅速な手動反復から完全自動化ループまで、3つのプロンプト最適化戦略を比較し、それぞれをいつ使用するかを理解する\n",
    "- 人間によるレビュー、LLM-as-judge評価、反復的なプロンプト改良を組み合わせた自己修復ワークフローの構築\n",
    "\n",
    "### 対象読者\n",
    "- おもちゃのデモを超えて進む必要があるML/AIエンジニアとソリューションアーキテクト\n",
    "- 内部ツールや本番パイプラインに適応できる実行可能な成果物を求めるプロダクトチームと配信チーム\n",
    "\n",
    "### このノートブックの進め方\n",
    "1. セクション1から始めて、医療ユースケース、ベースラインエージェント、システムアーキテクチャを理解する\n",
    "2. セクション2を使用してOpenAI Evalsインターフェース内でプロンプト最適化を練習し、構造化されたフィードバックを収集する\n",
    "3. セクション3を実行して、評価者、評価、再訓練ロジックで最適化ループを自動化する\n",
    "4. ワークフローを自分の環境に合わせて調整する際は、付録の再利用可能なプロンプト、設定、評価テンプレートを参照する\n",
    "\n",
    "このノートブックはモジュール式です。再訓練ループを自分のエージェントに適応させる際は、セクションを独立して実行するか、順次実行するかを自由に選択してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ユースケース概要：ヘルスケアにおける自己進化エージェント\n",
    "\n",
    "### 問題の定義\n",
    "\n",
    "このクックブックでは、**実世界のユースケース**に焦点を当てます：製薬会社向けの規制文書の作成です。これらの組織は、新薬の承認を得るために規制当局（例：米国食品医薬品局）に広範囲にわたる文書を準備・提出する必要があります。これらの提出物の正確性と迅速性は重要であり、救命治療が患者に届く速度に直接影響するためです。\n",
    "\n",
    "規制文書の作成は、深い科学的、医学的、コンプライアンス専門知識を必要とする、非常に複雑で反復的、かつ精密性が求められるプロセスです。高度な作成ツールが利用可能であるにもかかわらず、労働集約的で人的エラーが発生しやすいままです。**エージェントシステムは大きな効果を提供**し、研究の統合、コンテンツ生成、文書構造化を支援しますが、事実の正確性と規制遵守を確保するためには人間の専門家が依然として必要です。\n",
    "\n",
    "主要な課題は、これらのエージェントシステムが反復的に学習し、時間の経過とともにモデルの動作を改善できるフィードバックループを設計することです。このようなシステムは、人間の努力を詳細な修正から高レベルの監督へと段階的にシフトさせ、規制提出に必要な厳格な基準を維持しながら効率を向上させることができます。\n",
    "\n",
    "### 自己進化エージェント\n",
    "\n",
    "以下の図は、フィードバック、メタプロンプティング、評価を通じてAIエージェントを継続的に改善する反復プロセスを示しています。このループは、人間の判断またはLLM-as-a-judgeを使用した自動フィードバックを組み合わせて、パフォーマンスを反復的に向上させます。\n",
    "\n",
    "<img src=\"../../../images/baseline_agent.png\" alt=\"Self-evolving loop\" style=\"max-width:50%\"/>\n",
    "<br><em>図1 - 自動エージェント改善のための自己進化ループを示す図</em>\n",
    "\n",
    "プロセスは以下のステップで構成されます：\n",
    "\n",
    "1. **ベースラインエージェント**  \n",
    "   プロセスはベースラインエージェントから始まります。このノートブックでは、反復改善ループを説明するために意図的にシンプルな例（文書のセクションを要約するエージェント）を使用します。実世界や企業環境では、ベースラインエージェントははるかに複雑になる可能性があります。生成される要約は、後続の評価と改善のための初期ベンチマークとして機能します。\n",
    "\n",
    "2. **人間のフィードバック（またはLLM-as-judge）**  \n",
    "   ベースラインエージェントの出力は、人間のレビュアー（例：本番環境）および/または自動**LLM-as-judge**システムによって評価されます。このステップでは、エージェントが目標をどの程度達成しているかを示す定量的および定性的フィードバックを収集します。例えば、要約の長さをテストしている場合、フィードバックは「要約が長すぎる」または要約が500語以下かどうかを評価する際にevalによって生成される数値スコア（一般的に`0`から`1`の間）になる可能性があります。\n",
    "\n",
    "3. **Evalsと統合スコア**  \n",
    "   収集されたフィードバックに基づいて、新しいプロンプトが生成され、評価（**Evals**）を通じてテストされます。これらのテストは事前定義された基準に対するパフォーマンスを測定し、結果は全体的なパフォーマンスを反映する統合スコアに結合されます。スコアが目標閾値（例：`0.8`）を超えるか、最大再試行回数（例：`max_retry = 10`）に達するまでループが続きます。再試行制限に達した場合、手動改善が必要であることがエンジニアに通知されます。\n",
    "\n",
    "4. **更新されたベースラインエージェント**  \n",
    "   改善されたバージョンが目標パフォーマンスを達成すると、元のベースラインエージェントを置き換えます。この更新されたエージェントは次の反復の基盤となり、学習、フィードバック、最適化の継続的なサイクルをサポートします。\n",
    "\n",
    "### データセット概要\n",
    "\n",
    "評価に使用されるデータセットは、_Sample CMC Section for Hyperpolarized Pyruvate (13C) Injection_から抽出された約70のセクションで構成され、[こちら](https://dctd.cancer.gov/drug-discovery-development/reagents-materials/imaging-ind-resources/documentation/13c-pyruvate-cmc.pdf)で公開されています。このデータセットは、科学的要約と規制遵守の動作の両方をテストするのに適した、現実的でドメイン固有のコンテンツを提供します。\n",
    "\n",
    "### ベースラインエージェント概要\n",
    "\n",
    "このクックブックを自己完結型で簡単に再現可能にするため、本質的な複雑さを保持しながら規制作成ユースケースを簡素化しました。本番環境では、典型的な規制作成エージェントは、作成、データ分析、コンプライアンスチェック、引用生成、事実検証などのタスクを担当する複数の専門サブエージェントで構成されます。\n",
    "\n",
    "このガイドでは、システムの自己修復側面に焦点を当てるため、規制作成エージェントの範囲を狭めます。私たちの規制作成エージェントは2つのサブエージェントで構成されます：\n",
    "- **要約器**：科学的で簡潔な要約を作成\n",
    "- **コンプライアンスチェッカー**：各要約を主要な規制要件（例：FDA 21 CFR Part 11）に対して評価\n",
    "\n",
    "<img src=\"../../../images/simplified_reg_agent.png\" alt=\"Baseline Agent\" style=\"max-width:50%\"/>\n",
    "<br><em>図2 - AgentBuilder UIで作成されたベースラインエージェント</em>\n",
    "\n",
    "このクックブックの残りの部分では、要約器エージェントの簡素化されたバージョンを実装しました（以下の**エージェントセットアップ**セクションを参照）。または、AgentBuilderで作成されたエージェントのコードを再利用することもできます。AgentBuilder UIから直接エージェントを再現したい場合は、使用された主要なプロンプトとパラメータは以下の通りです：\n",
    "\n",
    "- **要約器エージェント：** このエージェントはファイル検索ツールを使用し、[CMC PDF](\"data/c13_pyruvate_sample_CMC_from_UCSF.pdf\")がベクトルストアにアップロードされました。\n",
    "> _プロンプト:_ \"ベクトルストアにアップロードされた{{state.cmc_pdf}}から{{workflow.input_as_text}}セクションを要約してください。\"\n",
    "\n",
    "- **コンプライアンスチェッカーエージェント：**\n",
    "> _プロンプト:_ \"以下の要約がFDA 21 CFR Part 11に準拠していることを確認してください：{{input.output_text}}。要約が準拠している場合は_Compliant_を返してください。そうでなければ_This section needs to be manually summarized_を返してください。\"\n",
    "\n",
    "両方のエージェントはデフォルトパラメータで設定されました - GPT-5を使用し、低い推論努力、テキスト出力形式です。\n",
    "\n",
    "### 評価アプローチ\n",
    "\n",
    "ベースラインエージェントを評価するには、主に2つのアプローチがあります：\n",
    "\n",
    "1. **人間のフィードバックの収集。** このアプローチは、OpenAI Evalsプラットフォーム（または特定のアプリケーション用に構築されたカスタムUI）を通じて人間のユーザーからフィードバックを収集することを含みます。本番設定や、専門家（SME）が実世界のシナリオでツールと対話するツールのパイロット時に最適です。この方法は、開発中に特定されなかった可能性のあるエッジケースを発見するのに役立ちます。Evalsプラットフォームでは、ユーザーは親指を上げる/下げる評価を提供し、要約に関する定性的フィードバックを共有できます。\n",
    "\n",
    "2. **LLM-as-a-Judgeの使用。** このオプションは通常、開発段階で使用され、SMEの時間を必要とせずに高速なフィードバックループを可能にします。**LLM-as-a-judge**は、LLMを使用してエージェントの出力を事前定義された基準に基づいて自動的に評価・スコア化します。また、モデルドリフトの監視（例：本番環境）やモデルとモデルバージョン間の変更の検証（例：`gpt-5`から`gpt-5-mini`への切り替え）にも使用できます。\n",
    "\n",
    "このクックブックでは両方のアプローチを実演します：\n",
    "- **セクション2**では、手動プロンプト最適化のためのプラットフォームUIアプローチを示します\n",
    "- **セクション3**では、LLM-as-a-judgeを使用した完全自動化APIアプローチを実装します\n",
    "\n",
    "_注：Evalsプラットフォームは、ユーザーフィードバックをプログラム的に取得するAPIをまだ提供していません。_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenAI Evalsプラットフォームの使用\n",
    "\n",
    "OpenAI Evalsプラットフォームは、プロンプト最適化と評価のための直感的なインターフェースを提供します。このセクションでは、データセットのアップロードから反復的なプロンプト改善まで、完全なワークフローを実演し、自動化ソリューションを実装する前にプラットフォームの視覚的インターフェースを活用してプロンプトを最適化する方法を示します。\n",
    "\n",
    "### ステップ1: データセットのアップロード\n",
    "\n",
    "OpenAI Evaluationプラットフォームの使用を開始するには、まずデータセットをアップロードする必要があります：\n",
    "\n",
    "1. **+ Create**ボタンをクリック\n",
    "2. データセット名を定義\n",
    "3. CSVファイルをアップロードし、保持する列を選択\n",
    "4. アップロード\n",
    "\n",
    "データセットには、要約が必要な文書または文書セクションを含める必要があります。各行は、システムによって処理される1つの入力を表します。\n",
    "\n",
    "### ステップ2: データの探索\n",
    "\n",
    "アップロード後、データセットを探索できます。データセット名をクリックして、アップロードされたデータを探索します。これにより、プロンプト設定に進む前に、データが適切にフォーマットされ、期待されるコンテンツが含まれていることを確認できます。\n",
    "\n",
    "### ステップ3: 初期プロンプトの設定\n",
    "\n",
    "ここで初期システムプロンプトを定義し、データがモデルを通してどのように流れるかを設定します。\n",
    "\n",
    "<img src=\"../../../images/prompt_input.png\" alt=\"Platform Prompt Configuration\" style=\"max-width:50%\">\n",
    "<br><em>図3 - モデル設定、変数、システムメッセージ設定を示すプラットフォームの「新しいプロンプト」インターフェース</em>\n",
    "\n",
    "#### 設定手順\n",
    "\n",
    "1. **システムプロンプト**: モデルのタスクと動作を定義するシステムメッセージを追加（このプロンプトが最適化されます）\n",
    "2. **ユーザープロンプトテンプレート**: ユーザーメッセージのプロンプトメッセージテンプレートを追加し、データセットの実際のデータに置き換えられる`{{<column_name>}}`などの変数を使用\n",
    "3. **モデル選択**: 生成用のモデルを選択（例：gpt-4.1、gpt-5）\n",
    "4. **Temperature**: 創造性と決定論のバランスを設定\n",
    "\n",
    "最適化プロセスの威力を実証するために、非常にシンプルなプロンプトから始めることができます。例えば、単に「summarize」から始めることで、システムが最小限の出発点からどのように進化できるかを示します。\n",
    "\n",
    "### ステップ4: 出力の生成\n",
    "\n",
    "プロンプトが設定されたら、データセット全体で出力を生成する準備が整いました。プロンプトは行ごとに1回実行され、新しい**output**列に出力が生成されます。\n",
    "\n",
    "1. **「Generate Output」**をクリック\n",
    "2. プラットフォームがすべてのサンプルに対してプロンプトを実行\n",
    "3. 結果が新しい**Output**列に表示\n",
    "\n",
    "プラットフォームはデータセットの各行を処理し、テンプレート変数を実際の値に置き換え、システムプロンプトでモデルを呼び出します。これにより、評価可能な出力のベースラインが作成されます。\n",
    "\n",
    "### ステップ5: レビューと評価\n",
    "\n",
    "評価は、プロンプト改善を導くための構造化されたフィードバックを提供する場所です。\n",
    "\n",
    "#### 出力のレビュー\n",
    "\n",
    "1. **評価列の追加**（自動的に追加されていない場合） - 「Columns」→「Annotations」→「Add」をクリック：\n",
    "   - **Rating** - バイナリ（良い/悪い）または数値評価\n",
    "   - **Feedback** - 改善が必要な点を説明するテキスト\n",
    "\n",
    "2. **評価とフィードバックの提供** - 各出力に対する評価を追加\n",
    "\n",
    "   出力の品質に応じて、良いまたは悪い評価を選択し、回答をどのように改善したいかに基づいてスコアを説明できます。例えば：\n",
    "\n",
    "      > (評価) | フィードバック\n",
    "      > - (良い) 良いが、回答のみを提供すべき。出力にはヘッダーや回答以外のテキストを含めるべきではない。\n",
    "      > - (悪い) 情報は良いが、箇条書きで提示すべき。\n",
    "      > - (良い) 良い要約；明確である。\n",
    "      > - (悪い) 読みやすさを向上させるため、回答時に箇条書きを使用する。各サブセクションを個別に要約する。\n",
    "\n",
    "3. **注釈の保存** - フィードバックは評価実行と共に保存\n",
    "\n",
    "<img src=\"../../../images/feedback.png\" alt=\"Platform Evaluation Interface\" style=\"max-width:50%\">\n",
    "<br><em>図4 - 注釈用の評価とフィードバック列を持つ生成された出力を示す評価インターフェース</em>\n",
    "\n",
    "この構造化されたフィードバックが、自動プロンプト最適化の基盤となります。\n",
    "\n",
    "### ステップ6: プロンプトの最適化\n",
    "\n",
    "フィードバックを収集した後、プラットフォームは自動的に改善されたプロンプトを生成できます。\n",
    "\n",
    "1. **「Optimize」**をクリック\n",
    "2. 新しいタブで新しいプロンプトバージョンが生成\n",
    "3. **「View Prompt」**をクリックして改善されたバージョンを確認\n",
    "\n",
    "<img src=\"../../../images/updated_prompt.png\" alt=\"Platform Optimized Prompt\" style=\"max-width:50%\">\n",
    "<br><em>図5 - プラットフォームによって生成された改善されたプロンプト、詳細な指示と要件を示している</em>\n",
    "\n",
    "### ステップ7: 反復と比較\n",
    "\n",
    "改善されたプロンプトの準備ができたら、新しい反復を開始して改善を測定します。\n",
    "\n",
    "1. **「Generate Output」**をクリック\n",
    "2. 新しい結果をレビューし、残りの問題についてフィードバックを提供\n",
    "3. 必要に応じて再度**「Optimize」**をクリック\n",
    "4. 満足するまで繰り返し\n",
    "\n",
    "プラットフォームのタブ構造により、反復間でのパフォーマンスを比較できます。初期プロンプトから最適化されたバージョンまで、出力がどのように進化したかを簡単に確認できます。\n",
    "\n",
    "<img src=\"../../../images/updated_prompt_feedback.png\" alt=\"Platform Updated Prompt Feedback\" style=\"max-width:50%\">\n",
    "<br><em>図6 - 最適化されたプロンプトのフィードバックと評価結果、出力品質の改善を示している</em>\n",
    "\n",
    "#### 反復を停止するタイミング\n",
    "\n",
    "以下の条件まで最適化サイクルを続けます：\n",
    "- **品質閾値に到達**: 出力の80%以上が肯定的なフィードバックを受ける\n",
    "- **収穫逓減**: 新しい反復で最小限の改善しか見られない\n",
    "- **特定の問題が解決**: 特定されたすべての失敗モードが対処される\n",
    "\n",
    "このプラットフォームベースのアプローチは、自動化実装に移る前にプロンプト最適化を理解するための優れた基盤を提供します。視覚的インターフェースにより、変更の影響を簡単に確認し、最適化プロセスを理解できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM-as-a-Judgeを使用した自己進化ループ\n",
    "\n",
    "このセクションでは、OpenAI APIを通じてLLM-as-a-Judgeを使用した完全自動化評価ワークフローを紹介し、ユーザーインターフェースの必要性を排除します。このアプローチにより、エージェントのパフォーマンスのスケーラブルでプログラマティックな評価が可能になり、本番環境での迅速な反復と継続的なモデル監視をサポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gepa and litellm are only required for the Section 4.b (prompt optimization with GEPA)\n",
    "%pip install --upgrade openai openai-agents pydantic pandas gepa litellm python-dotenv -qqq \n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# Place your API key in a file called .env\n",
    "# OPENAI_API_KEY=sk-...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価の作成\n",
    "\n",
    "ベースライン要約エージェントを評価するために、決定論的チェックと意味的判断のバランスを取る4つの補完的な評価者を使用します。\n",
    "\n",
    "| 評価者 | タイプ | 合格閾値 | チェック内容 | 理由 |\n",
    "|---|---|---:|---|---|\n",
    "| 化学物質名 | `python` | 0.8 | セクション内の正確な化学物質名が要約に含まれているか | 重要なドメインエンティティの保持を強制し、要約が化学的に意味のある用語を省略しないようにする |\n",
    "| 要約の長さ | `python` | 0.85 | 期待される100語の長さからの逆偏差 | 要約を簡潔で比較可能に保ち、内容の質の低さを隠す可能性のある冗長性を削減する |\n",
    "| コサイン類似度 | `text_similarity` | 0.85 | セクションと要約テキスト間のコサイン類似度 | 要約が意味的に逸脱するのではなく、ソースコンテンツに固定されることを保証する |\n",
    "| LLM審査員 | `score_model` | 0.85 | 評価者として機能するモデルからのルーブリック駆動スコア | ルールベースのメトリクスが見逃す微妙な品質シグナルを捉え、全体的な堅牢性を向上させる |\n",
    "\n",
    "**注記**\n",
    "- 2つのPython評価者は、ドメインの忠実性と長さの規律を早期に捉え、意味的調整前に最適化を安定化させます。\n",
    "- テキスト類似度は、ソースから逸脱する表面的な言い換えを防ぎます。\n",
    "- LLM審査員は、決定論的チェックをすり抜けるエッジケースに対する包括的な安全装置を提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "data_source_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"item_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\"section\": {\"type\": \"string\"}, \"summary\": {\"type\": \"string\"}},\n",
    "        \"required\": [\"section\", \"summary\"],\n",
    "    },\n",
    "    \"include_sample_schema\": False,\n",
    "}\n",
    "\n",
    "testing_criteria = [\n",
    "    {\n",
    "        \"type\": \"python\",\n",
    "        \"name\": \"chemical_name_grader\",\n",
    "        \"image_tag\": \"2025-05-08\",\n",
    "        \"pass_threshold\": 0.8,\n",
    "        \"source\": r\"\"\"def grade(sample: dict, item: dict) -> float:\n",
    "    section = item[\"section\"]\n",
    "    summary = item[\"summary\"]\n",
    "    CHEMICALS_MASTER = [\"[1-¹³C]Pyruvic acid\",\"[1-¹³C]Pyruvate\",\"¹²C Pyruvic acid\",\"Sodium [1-¹³C]pyruvate\",\"Sodium pyruvate (¹²C)\",\"AH111501 (Trityl radical)\",\"Tris{8-carboxyl-2,2,6,6-tetra[2-(1-methoxyethyl)]-benzo(1,2-d:4,5-d’)bis(1,3)dithiole-4-yl}methyl acid\",\"AH111501 sodium salt\",\"Methyl, tris[8-carboxy-2,2,6,6-tetrakis(2-methoxyethyl)benzo[1,2-d:4,5-d’]bis[1,3]dithiol-4-yl]-, trisodium salt\",\"AH111501 trisodium salt\",\"AH111576\",\"2,2′,2″,2‴-(4,8-Dibromobenzo[1,2-d:4,5-d′]bis([1,3]dithiole)-2,2,6,6-tetrayl)tetraethanol\",\"AH111586\",\"4,8-Dibromo-2,2,6,6-tetrakis(2-methoxyethyl)benzo[1,2-d:4,5-d′]bis([1,3]dithiole)\",\"AH111709\",\"AH111743\",\"AH112615\",\"4,4-Bis-hydroxymethyl-2-methyl-oxazolidine-2-carboxylic acid\",\"AH112623\",\"Parapyruvate\",\"2-Hydroxy-2-methyl-4-oxo-pentanedioic acid\",\"AH113127\",\"(4-Hydroxymethyl-oxazolidin-4-yl)-methanol\",\"AH113462/E\",\"Enol lactone\",\"AH113462/K\",\"Keto lactone\",\"Acetyl bromide\",\"Methanol\",\"Dimethyl sulfoxide\",\"DMSO\",\"Tetrahydrofuran\",\"THF\",\"Acetonitrile\",\"ACN\",\"Diethyl ether\",\"Et₂O\",\"N,N-Dimethylacetamide\",\"DMA\",\"1,3-Dimethyl-2-imidazolidinone\",\"DMI\",\"Hydrochloric acid\",\"HCl\",\"Sodium hydroxide\",\"NaOH\",\"Disodium ethylenediaminetetraacetate\",\"Na₂EDTA\",\"Ethylenediaminetetraacetic acid\",\"EDTA\",\"Tris(hydroxymethyl)aminomethane\",\"TRIS\",\"Trometamol\",\"Trifluoroacetic acid\",\"TFA\",\"Toluene\",\"Heptane\",\"Ethyl acetate\",\"Ethanol\",\"Water\",\"H₂O\",\"Sodium chloride\",\"NaCl\",\"Cuprous [1-¹³C]cyanide\",\"Cu¹³CN\",\"Gadolinium\",\"Gd\",\"Tin\",\"Sn\",\"Phosphorus\",\"P\",\"Carbon dioxide\",\"CO₂\",\"Sodium [1-13C]pyruvate\",\"[1-13C]Pyruvic acid\",\"1-13C pyruvate\"]\n",
    "\n",
    "    # Identify the chemicals present in the section\n",
    "    present = [chem for chem in CHEMICALS_MASTER if chem in section]\n",
    "\n",
    "    # If no chemicals present, consider it satisfied\n",
    "    if not present:\n",
    "        return 1.0\n",
    "\n",
    "    correct = 0\n",
    "    for chem in present:\n",
    "        # Only count as correct if the exact chemical string appears in the summary\n",
    "        if chem in summary:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(present)\"\"\",\n",
    "    },\n",
    "    {\n",
    "    \"type\": \"python\",\n",
    "    \"name\": \"word_length_deviation_grader\",\n",
    "    \"image_tag\": \"2025-05-08\",\n",
    "    \"pass_threshold\": 0.85,\n",
    "    \"source\": r\"\"\"\n",
    "def grade(sample: dict, item: dict) -> float:\n",
    "    summary = item[\"summary\"]\n",
    "    word_count = len(summary.split())\n",
    "    \n",
    "    expected_summary_length = 100\n",
    "    tolerance = 0.2  # 20% band around target\n",
    "    \n",
    "    # relative deviation\n",
    "    deviation = abs(word_count - expected_summary_length) / expected_summary_length\n",
    "    \n",
    "    # If within tolerance band → full score\n",
    "    if deviation <= tolerance:\n",
    "        return 1.0\n",
    "    \n",
    "    # Outside band → score decays linearly, capped at 0\n",
    "    # e.g., deviation 0.3 → score 0.8, deviation 1.0+ → 0.0\n",
    "    score = 1.0 - (deviation - tolerance)\n",
    "    return max(0.0, score)\n",
    "\"\"\",\n",
    "},\n",
    "    {\n",
    "        \"name\": \"cosine_similarity\",\n",
    "        \"type\": \"text_similarity\",\n",
    "        \"input\": \"{{ item.summary }}\",\n",
    "        \"reference\": \"{{ item.section }}\",\n",
    "        \"evaluation_metric\": \"cosine\",\n",
    "        \"pass_threshold\": 0.85,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"llm_as_judge\",\n",
    "        \"type\": \"score_model\",\n",
    "        \"model\": \"gpt-4.1\",\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an expert technical summarization evaluator. \"\n",
    "                    \"Evaluate whether the summary captures and preserves the important technical facts and specific details from the section, allowing for occasional minor rewording or omissions of less important points, but not major technical inaccuracies or information loss.\\n\\n\"\n",
    "                    \"Scoring Guidelines:\\n\"\n",
    "                    \"- Return a numerical score between 0 and 1 (with up to two decimal places).\\n\"\n",
    "                    \"- A score of 1 means the summary is almost flawless: it is comprehensive, highly faithful, and technically accurate, with virtually no important or meaningful details missing, and no significant misstatements or distortions.\\n\"\n",
    "                    \"- 0.75-0.99 indicates excellent work: all main facts are represented, but there may be trivial omissions or very minor rewording that do not materially affect understanding.\\n\"\n",
    "                    \"- 0.5-0.75 indicates good but imperfect: most technical information is retained and correctly presented, some less critical details might be missing or slightly rephrased, but overall fidelity is preserved.\\n\"\n",
    "                    \"- 0.3-0.5 means significant information is missing, or some technical inaccuracies are present, but the summary retains a reasonable portion of key facts.\\n\"\n",
    "                    \"- 0.0-0.3 means there are major omissions, misunderstandings, or a failure to capture the most important technical content.\\n\\n\"\n",
    "                    \"Respond only with a single number between 0 and 1 indicating summary quality by these criteria.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Section:\\n{{item.section}}\\n\"\n",
    "                    \"Summary:\\n{{sample.output_text}}\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "        \"range\": [0, 1],\n",
    "        \"pass_threshold\": 0.85,\n",
    "    },\n",
    "]\n",
    "\n",
    "eval = client.evals.create(\n",
    "    name=\"self_evolving_eval\",\n",
    "    data_source_config=data_source_config,\n",
    "    testing_criteria=testing_criteria,\n",
    ")\n",
    "print(f\"Created Eval: {eval.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力にeval ID（例：`eval_...`）が表示されるはずです。これは、先ほど作成したevalのIDです（以下に示すとおり）\n",
    "\n",
    "<img src=\"../../../images/eval_set_config.png\" alt=\"Platform Eval Configuration\" style=\"max-width:50%\">\n",
    "<br><em>図7 - データソース設定とテスト基準設定を表示するプラットフォームのEvalインターフェース</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### グレーダーのスコアリングと解析\n",
    "\n",
    "次に、要約エージェントの出力に対してevalを実行し、evalのグレーダースコアの結果を解析する必要があります。これを行うために、いくつかのヘルパー関数を使用します：\n",
    "- `run_eval`: 適切なフォーマットでevals APIを呼び出すシンプルなランナー\n",
    "- `poll_eval_run`: スケジュールされたeval実行の完了を待機するポーリングユーティリティ\n",
    "- `parse_eval_run_output`: eval実行を解析し、フィードバックループ用の構造化された出力を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "def run_eval(eval_id: str, section: str, summary: str):\n",
    "  \"\"\"Creates a run of the eval with the input section and output summary.\"\"\"\n",
    "  return client.evals.runs.create(\n",
    "    eval_id=eval_id,\n",
    "    name=\"self-evolving-eval\",\n",
    "    data_source={\n",
    "      \"type\": \"jsonl\",\n",
    "      \"source\": {\n",
    "        \"type\": \"file_content\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"item\": {\n",
    "              \"section\": section,\n",
    "              \"summary\": summary,\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "      },\n",
    "    },\n",
    "  )\n",
    "\n",
    "\n",
    "def poll_eval_run(eval_id: str, run_id: str, max_polls = 10):\n",
    "    \"\"\"\n",
    "    Polls the evaluation run until completion or timeout.\n",
    "\n",
    "    This function exists to handle asynchronous behavior in the eval service by\n",
    "    periodically checking run status. It balances responsiveness and resource use by\n",
    "    polling at fixed intervals rather than blocking indefinitely. The retry limit\n",
    "    prevents runaway loops in cases where the service never returns a completed status.\n",
    "    \"\"\"\n",
    "    run = None\n",
    "    for attempt in range(1, max_polls + 1):\n",
    "        run = client.evals.runs.retrieve(eval_id=eval_id, run_id=run_id)\n",
    "        if run.status == \"completed\":\n",
    "            break\n",
    "        if attempt == max_polls:\n",
    "            print(\"Exceeded retries, aborting\")\n",
    "            break\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "    run_output_items = client.evals.runs.output_items.list(\n",
    "        eval_id=eval_id, run_id=run_id\n",
    "    )\n",
    "    return run_output_items\n",
    "\n",
    "\n",
    "def parse_eval_run_output(items):\n",
    "    \"\"\"Extract all grader scores and any available conclusion outputs.\"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    for item in items.data:\n",
    "        for result in item.results:\n",
    "            grader_name_full = result.name\n",
    "            score = result.score\n",
    "            passed = result.passed\n",
    "            reasoning = None\n",
    "            try:\n",
    "                sample = result.sample\n",
    "                if sample:\n",
    "                    content = result.sample[\"output\"][0][\"content\"]\n",
    "                    content_json = json.loads(content)\n",
    "                    steps = content_json[\"steps\"]\n",
    "                    reasoning = \" \".join([step[\"conclusion\"] for step in steps])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            all_results.append(\n",
    "                {\n",
    "                    \"grader_name\": grader_name_full,\n",
    "                    \"score\": score,\n",
    "                    \"passed\": passed,\n",
    "                    \"reasoning\": reasoning,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、先ほど作成したeval IDを使用して、任意の入力セクションと要約出力に対してgraderを実行できます。これは、プロンプト最適化ルーチンを開始するフィードバックループの基盤を形成します。\n",
    "\n",
    "### Eval実行の実行\n",
    "\n",
    "セクションと生成された要約を直接提供して、evalをテストしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_ID = eval.id #Created eval ID from above cell\n",
    "SECTION = \"3.2.S.1 General Information ([1-13C]pyruvic acid) The active ingredient in Hyperpolarized Pyruvate (13C) Injection is hyperpolarized [1-13C]pyruvate. The drug substance is defined as [13C]pyruvic acid, which is neutralized to [1-13C]pyruvate during the compounding process. In several pre-clinical and clinical studies and during evaluation of stability, pyruvic acid has been used instead of [1-13C]pyruvic acid (see Sections 3.2.P.2.2.1 Formulation Development for Hyperpolarized Pyruvate (13C) Injection and Section 8.1 Introduction for Item 8 Pharmacology and Toxicology Info). In the Section 3.2.S Drug Substance, data are presented for both pyruvic acid and for [1-13C]pyruvic acid. For simplicity, the terminology used in headings and captions is [1-13C]pyruvic acid. Batches containing pyruvic acid are specified by footnotes. 3.2.S.1.1 Nomenclature ([1-13C]pyruvic acid) The drug substance used for compounding of Hyperpolarized Pyruvate (13C) Injection is [1-13C]pyruvic acid. Company code: W6578 Chemical name: [1-13C]pyruvic acid CAS registry number: 127-17-3 3.2.S.1.2 Structure ([1-13C]pyruvic acid) Figure 1 Structure of [1-13C]pyruvic acid Molecular formula: C H O 3 4 3 Molecular weight: 89.06 3.2.S.1.3 General Properties ([1-13C]pyruvic acid) Appearance: Colorless to yellow, clear, viscous liquid pKa:Ka:aranWater solubility: Complete The structure of [1-13C]pyruvic acid has been confirmed by spectroscopic analysis (see Section 3.2.S.3.1 Elucidation of Structure and other Characteristics).\"\n",
    "SUMMARY = \"The active ingredient in Hyperpolarized Pyruvate (13C) Injection is hyperpolarized [1-13C]pyruvate, derived from [1-13C]pyruvic acid (neutralized during compounding). Both pyruvic acid and [1-13C]pyruvic acid were used in studies and stability evaluations, but the documentation refers to [1-13C]pyruvic acid unless otherwise noted. The drug substance ([1-13C]pyruvic acid, CAS 127-17-3) is a colorless to yellow, clear, viscous liquid with a molecular formula C3H4O3 and molecular weight 89.06. Its structure has been confirmed by spectroscopic analysis, and it is completely soluble in water.\"\n",
    "\n",
    "eval_run = run_eval(EVAL_ID, section=SECTION, summary=SUMMARY)\n",
    "run_output = poll_eval_run(eval_id=EVAL_ID, run_id=eval_run.id)\n",
    "\n",
    "grader_scores = parse_eval_run_output(run_output)\n",
    "print(grader_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "出力には以下のようなグレーダースコアのリストが表示されるはずです：\n",
    "\n",
    "```[{'grader_name': 'chemical_name_grader-<uuid>', 'score': 0.5, 'passed': False, 'reasoning': None}, {'grader_name': 'word_length_deviation_grader-<uuid>', 'score': 0.8, 'passed': True, 'reasoning': None}, {'grader_name': 'cosine_similarity-<uuid>', 'score': 0.9104484223477793, 'passed': True, 'reasoning': None}, {'grader_name': 'llm_as_judge-<uuid>', 'score': 0.8, 'passed': True, 'reasoning': 'The summary needs to include specific details from the section. Part of the essential information is captured. Key pieces of information are missing. Not all relevant structural information is included.'}]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このスクリプトを実行すると、`chemical_name_grader`を除いて、ほとんどのグレーダーが合格していることがわかります。次に、要約エージェントを改善する機会をプログラム的に認識します。\n",
    "\n",
    "_注意: ローカルで実行する場合、最初は`chemical_name_grader`以外のグレーダーも失敗する可能性があります。これは正常な動作です。グレーダーは最初は失敗することがありますが、フィードバックループを通じて結果が改善されるはずです。初期の失敗は、モデルがより正確な結果に収束する前に応答を調整していることを単純に反映しています。_\n",
    "\n",
    "\n",
    "### ダッシュボードの可観測性\n",
    "評価の実行と結果はOpenAIダッシュボードでも確認できます：\n",
    "\n",
    "<img src=\"../../../images/eval_dashboard.png\" alt=\"Eval dashboard\" style=\"max-width:50%\">\n",
    "<br><em>図8 - 評価実行と結果を表示する評価ダッシュボード。</em>\n",
    "\n",
    "\n",
    "特定の評価実行の詳細を掘り下げることもできます：\n",
    "<img src=\"../../../images/eval_run_results.png\" alt=\"Eval results\" style=\"max-width:50%\">\n",
    "<br><em>図9 - グレーダースコアとパフォーマンス指標を表示する詳細な評価実行結果。</em>\n",
    "\n",
    "\n",
    "## エージェントのセットアップ\n",
    "\n",
    "評価とグレーダーの設定が完了したので、要約エージェントに戻ることができます。\n",
    "簡単にするため、以下にシンプルなエージェントのコードを提供します。図2に示されているように`AgentBuilder`を使用して、UIからコードをエクスポートすることもできます。\n",
    "\n",
    "\n",
    "また、プロンプトを最適化するためのメタプロンプト最適化エージェントと、プロンプトバージョンを処理するためのシンプルなユーティリティも必要になります：\n",
    "- `PromptVersionEntry`: プロンプトとメタデータを本番環境で変更される際に追跡するために使用されるpydanticモデル\n",
    "- `VersionedPrompt`: プロンプトバージョンを追跡するユーティリティクラス。これは、プロンプトの進化を分析し、回帰が発生した場合のフォールバック履歴を確保するために、本番環境で重要になります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Any, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field, ConfigDict, field_validator\n",
    "\n",
    "class PromptVersionEntry(BaseModel):\n",
    "    \"\"\"Data model for a prompt and associated data for observability\"\"\"\n",
    "    version: int = Field(\n",
    "        ..., ge=0, description=\"Version number of the prompt (increments)\"\n",
    "    )\n",
    "    model: str = Field(\n",
    "        \"gpt-5\",\n",
    "        min_length=1,\n",
    "        description=\"The model version to use for this version of the prompt, defaults to gpt-5\",\n",
    "    )\n",
    "    prompt: str = Field(\n",
    "        ..., min_length=1, description=\"The prompt text for this version\"\n",
    "    )\n",
    "    timestamp: datetime = Field(\n",
    "        default_factory=datetime.utcnow,\n",
    "        description=\"UTC timestamp when this version was created\",\n",
    "    )\n",
    "    eval_id: Optional[str] = Field(\n",
    "        None, description=\"ID of the evaluation associated with this prompt version\"\n",
    "    )\n",
    "    run_id: Optional[str] = Field(\n",
    "        None, description=\"ID of the run associated with this prompt version\"\n",
    "    )\n",
    "    metadata: Optional[dict[str, Any]] = Field(\n",
    "        None, description=\"Free-form metadata dict (e.g., section, summary)\"\n",
    "    )\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        str_strip_whitespace=True, validate_assignment=True, extra=\"forbid\"\n",
    "    )\n",
    "\n",
    "    @field_validator(\"prompt\")\n",
    "    @classmethod\n",
    "    def prompt_not_blank(cls, v: str) -> str:\n",
    "        if not v.strip():\n",
    "            raise ValueError(\"prompt must not be blank or only whitespace\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class VersionedPrompt:\n",
    "    \"\"\"Manages a collection of prompt versions and provides controlled updates and rollbacks.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_prompt: str,\n",
    "        model: Optional[str] = \"gpt-5\",\n",
    "        eval_id: Optional[str] = None,\n",
    "        run_id: Optional[str] = None,\n",
    "        metadata: Optional[dict[str, Any]] = None,\n",
    "    ):\n",
    "        if not initial_prompt or not initial_prompt.strip():\n",
    "            raise ValueError(\"initial_prompt must be non-empty\")\n",
    "        self._versions: list[PromptVersionEntry] = []\n",
    "        first_entry = PromptVersionEntry(\n",
    "            version=0,\n",
    "            prompt=initial_prompt,\n",
    "            model=model,\n",
    "            eval_id=eval_id,\n",
    "            run_id=run_id,\n",
    "            metadata=metadata,\n",
    "        )\n",
    "        self._versions.append(first_entry)\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        new_prompt: str,\n",
    "        model: Optional[str] = \"gpt-5\",\n",
    "        eval_id: Optional[str] = None,\n",
    "        run_id: Optional[str] = None,\n",
    "        metadata: Optional[dict[str, Any]] = None,\n",
    "    ) -> PromptVersionEntry:\n",
    "        if not new_prompt or not new_prompt.strip():\n",
    "            raise ValueError(\"new_prompt must be non-empty\")\n",
    "\n",
    "        version = self.current().version + 1\n",
    "        entry = PromptVersionEntry(\n",
    "            version=version,\n",
    "            prompt=new_prompt,\n",
    "            model=model,\n",
    "            eval_id=eval_id,\n",
    "            run_id=run_id,\n",
    "            metadata=metadata,\n",
    "        )\n",
    "        self._versions.append(entry)\n",
    "        return entry\n",
    "\n",
    "    def current(self) -> PromptVersionEntry:\n",
    "        return self._versions[-1]\n",
    "\n",
    "    def revert_to_version(self, version: int) -> PromptVersionEntry:\n",
    "        idx = None\n",
    "        for i, entry in enumerate(self._versions):\n",
    "            if entry.version == version:\n",
    "                idx = i\n",
    "                break\n",
    "\n",
    "        if idx is None:\n",
    "            raise ValueError(f\"No version found with version={version}\")\n",
    "\n",
    "        self._versions = self._versions[: idx + 1]\n",
    "        return self._versions[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、要約とプロンプト最適化の開始エージェントを作成します。\n",
    "\n",
    "_注意：要約エージェントは本番環境で進化することが予想されるため、プロンプトの変更を追跡するラッパーを作成しました。メタプロンプトエージェントのプロンプトは、このクックブックの目的においては静的なままとなります。_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from agents import Agent\n",
    "\n",
    "METAPROMPT_TEMPLATE = \"\"\"\n",
    "# Context:\n",
    "## Original prompt:\n",
    "{original_prompt}\n",
    "\n",
    "## Section:\n",
    "{section}\n",
    "\n",
    "## Summary:\n",
    "{summary}\n",
    "\n",
    "## Reason to improve the prompt:\n",
    "{reasoning}\n",
    "\n",
    "# Task:\n",
    "Write a new summarization prompt that is significantly improved and more specific than the original.  \n",
    "The new prompt should instruct the model to produce concise yet comprehensive technical summaries that precisely preserve all explicit information from the source text. It should emphasize the inclusion of all named entities, quantities, compounds, and technical terminology without paraphrasing or omission. The resulting prompt should read like a clear, directive system message for a technical summarization assistant—structured, unambiguous, and generalizable across scientific or regulatory document sections.\n",
    "\"\"\"\n",
    "\n",
    "metaprompt_agent = Agent(\n",
    "    name=\"MetapromptAgent\", instructions=\"You are a prompt optimizer.\"\n",
    ")\n",
    "\n",
    "summarization_prompt = VersionedPrompt(\n",
    "    initial_prompt=\"\"\"You are a summarization assistant.\n",
    "Given a section of text, produce a summary.\"\"\"\n",
    ")\n",
    "\n",
    "def make_summarization_agent(prompt_entry: PromptVersionEntry) -> Agent:\n",
    "    return Agent(\n",
    "        name=\"SummarizationAgent\",\n",
    "        instructions=prompt_entry.prompt,\n",
    "        model=prompt_entry.model,\n",
    "    )\n",
    "\n",
    "summarization_agent = make_summarization_agent(summarization_prompt.current())\n",
    "\n",
    "# Cache eval results by section + summary so repeated attempts do not trigger redundant grader runs.\n",
    "eval_cache: dict[tuple[str, str], list[dict[str, Any]]] = {}\n",
    "\n",
    "# Track the highest-scoring candidate that also passes the lenient score threshold.\n",
    "best_candidate: dict[str, Any] = {\n",
    "    \"score\": float(\"-inf\"),\n",
    "    \"prompt\": summarization_prompt.current().prompt,\n",
    "    \"model\": summarization_prompt.current().model,\n",
    "    \"summary\": None,\n",
    "    \"metadata\": None,\n",
    "    \"version\": summarization_prompt.current().version,\n",
    "    \"passed_lenient\": False,\n",
    "    \"total_score\": float(\"-inf\"),\n",
    "}\n",
    "\n",
    "# Aggregate per-version performance so we can pick the strongest total scorer at the end.\n",
    "aggregate_prompt_stats: dict[int, dict[str, Any]] = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### オーケストレーションとモニタリング\n",
    "\n",
    "これまでに以下のものを作成しました：\n",
    "- 出力を評価し、各評価者のスコアを生成する4つの評価者を持つEvals\n",
    "- プロンプトとモデルの変更を追跡するためのバージョン管理されたプロンプトクラスを持つ要約エージェント\n",
    "- 推論のセットに基づいてプロンプトの更新を試みるメタプロンプト最適化エージェント\n",
    "\n",
    "これらの異なる機能を組み合わせて、OpenAIダッシュボードでのAgentトレーシングを使用した自己進化ループをオーケストレートできます。\n",
    "\n",
    "これは簡略化された例であることに注意してください。実際のシナリオでは、最適化の試行に対するガードレールを確保し、ガードレールがトリガーされた際に人間に警告する仕組みが必要になります。\n",
    "\n",
    "_注意：クックブックの実用的な制限により、静的データセットを使用してデータストリームをシミュレートし、真の観測可能性の代わりに`print`文を使用しています。_\n",
    "\n",
    "### オーケストレーション・ユーティリティ\n",
    "\n",
    "前のセクションと同様に、フィードバックループのオーケストレーションロジックを管理するためのユーティリティを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Optional\n",
    "from agents import Runner\n",
    "\n",
    "LENIENT_PASS_RATIO = 0.75 # 75% of graders must pass (binary) \n",
    "LENIENT_AVERAGE_THRESHOLD = 0.85 # 85% average score across graders \n",
    "\n",
    "def reset_best_candidate() -> None:\n",
    "    \"\"\"Reset the best candidate tracker for a new optimization run.\"\"\"\n",
    "    global best_candidate\n",
    "\n",
    "    current = summarization_prompt.current()\n",
    "    best_candidate = {\n",
    "        \"score\": float(\"-inf\"),\n",
    "        \"prompt\": current.prompt,\n",
    "        \"model\": current.model,\n",
    "        \"summary\": None,\n",
    "        \"metadata\": None,\n",
    "        \"version\": current.version,\n",
    "    }\n",
    "\n",
    "def reset_best_trackers() -> None:\n",
    "    \"\"\"Reset both the best-candidate tracker and aggregate stats.\"\"\"\n",
    "    reset_best_candidate()\n",
    "    aggregate_prompt_stats.clear()\n",
    "\n",
    "\n",
    "def update_best_candidate(\n",
    "    *,\n",
    "    average_score: Optional[float] = None,\n",
    "    prompt_text: str,\n",
    "    model_name: str,\n",
    "    summary_text: str = None,\n",
    "    metadata: dict[str, Any] = None,\n",
    "    lenient_passed: bool = False,\n",
    "    prompt_version: int = None,\n",
    "    total_score: Optional[float] = None,\n",
    "    score: Optional[float] = None,\n",
    ") -> None:\n",
    "    \"\"\"Persist the best lenient-passing candidate.\"\"\"\n",
    "    global best_candidate\n",
    "\n",
    "    if prompt_version is None:\n",
    "        prompt_version = summarization_prompt.current().version\n",
    "\n",
    "    if average_score is None:\n",
    "        average_score = score\n",
    "\n",
    "    if average_score is None:\n",
    "        return\n",
    "\n",
    "    if lenient_passed:\n",
    "        best_candidate.update(\n",
    "            {\n",
    "                \"score\": average_score,\n",
    "                \"prompt\": prompt_text,\n",
    "                \"model\": model_name,\n",
    "                \"summary\": summary_text,\n",
    "                \"metadata\": metadata,\n",
    "                \"version\": prompt_version,\n",
    "                \"total_score\": total_score if total_score is not None else average_score,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "def apply_best_candidate_if_needed() -> Agent:\n",
    "    \"\"\"Ensure summarization_prompt reflects the best prompt candidate.\"\"\"\n",
    "    if best_candidate[\"score\"] > float(\"-inf\"):\n",
    "        current = summarization_prompt.current()\n",
    "        target = best_candidate\n",
    "        # Only update if different\n",
    "        if (\n",
    "            current.prompt != target[\"prompt\"]\n",
    "            or current.model != target[\"model\"]\n",
    "            or current.version != target.get(\"version\")\n",
    "        ):\n",
    "            summarization_prompt.update(\n",
    "                new_prompt=target[\"prompt\"],\n",
    "                model=target[\"model\"],\n",
    "                metadata=target.get(\"metadata\"),\n",
    "            )\n",
    "            target[\"version\"] = summarization_prompt.current().version\n",
    "        return make_summarization_agent(summarization_prompt.current())\n",
    "\n",
    "    return make_summarization_agent(summarization_prompt.current())\n",
    "\n",
    "\n",
    "def record_aggregate_prompt_score(\n",
    "    *,\n",
    "    prompt_version: int,\n",
    "    prompt_text: str,\n",
    "    model_name: str,\n",
    "    average_score: float,\n",
    "    total_score: Optional[float] = None,\n",
    ") -> None:\n",
    "    \"\"\"Accumulate per-version grader scores for aggregate selection.\"\"\"\n",
    "    stats = aggregate_prompt_stats.setdefault(\n",
    "        prompt_version,\n",
    "        {\n",
    "            \"version\": prompt_version,\n",
    "            \"prompt\": prompt_text,\n",
    "            \"model\": model_name,\n",
    "            \"total_score\": 0.0,\n",
    "            \"total_average\": 0.0,\n",
    "            \"count\": 0,\n",
    "        },\n",
    "    )\n",
    "    stats[\"total_score\"] += total_score if total_score is not None else average_score\n",
    "    stats[\"total_average\"] += average_score\n",
    "    stats[\"count\"] += 1\n",
    "    stats[\"prompt\"] = prompt_text\n",
    "    stats[\"model\"] = model_name\n",
    "\n",
    "\n",
    "def select_best_aggregate_prompt() -> Optional[dict[str, Any]]:\n",
    "    \"\"\"Return the prompt version with the highest cumulative score.\"\"\"\n",
    "    if not aggregate_prompt_stats:\n",
    "        return None\n",
    "    return max(\n",
    "        aggregate_prompt_stats.values(),\n",
    "        key=lambda entry: (\n",
    "            entry.get(\"total_score\", float(\"-inf\")),\n",
    "            entry.get(\"version\", -1),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "async def get_eval_grader_score(eval_id: str, section: str, summary: str):\n",
    "    \"\"\"Retrieve grader scores for a section-summary pair with caching.\"\"\"\n",
    "    cache_key = (section, summary)\n",
    "    if cache_key in eval_cache:\n",
    "        return eval_cache[cache_key]\n",
    "\n",
    "    eval_run = run_eval(eval_id=eval_id, section=section, summary=summary)\n",
    "    run_output = poll_eval_run(eval_id=eval_id, run_id=eval_run.id)\n",
    "    results = parse_eval_run_output(run_output)\n",
    "    eval_cache[cache_key] = results\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_grader_score(grader_scores):\n",
    "    \"\"\"Simple average score of all graders from the eval.\"\"\"\n",
    "    if not grader_scores:\n",
    "        return 0.0\n",
    "\n",
    "    score_sum = 0.0\n",
    "    for entry in grader_scores:\n",
    "        score_sum += entry.get(\"score\", 0.0)\n",
    "\n",
    "    return score_sum / len(grader_scores)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_total_grader_score(grader_scores):\n",
    "    \"\"\"Sum of all grader scores for aggregate tracking.\"\"\"\n",
    "    if not grader_scores:\n",
    "        return 0.0\n",
    "\n",
    "    return sum(entry.get(\"score\", 0.0) for entry in grader_scores)\n",
    "\n",
    "\n",
    "DEFAULT_PASSING_FEEDBACK = (\n",
    "    \"All graders passed; tighten factual coverage, chemical completeness, and conciseness.\"\n",
    ")\n",
    "\n",
    "\n",
    "def is_lenient_pass(grader_scores, average_score: float) -> bool:\n",
    "    if not grader_scores:\n",
    "        return False\n",
    "\n",
    "    passed_count = sum(1 for entry in grader_scores if entry.get(\"passed\"))\n",
    "    total_graders = len(grader_scores)\n",
    "\n",
    "    if total_graders and (passed_count / total_graders) >= LENIENT_PASS_RATIO:\n",
    "        return True\n",
    "    return average_score >= LENIENT_AVERAGE_THRESHOLD\n",
    "\n",
    "\n",
    "def collect_grader_feedback(grader_scores):\n",
    "    \"\"\"Consolidate grader reasoning into actionable feedback for the metaprompt agent.\"\"\"\n",
    "    feedback_lines = []\n",
    "\n",
    "    for entry in grader_scores:\n",
    "        grader = entry.get(\"grader_name\", \"\")\n",
    "        passed = entry.get(\"passed\", False)\n",
    "        reasoning = entry.get(\"reasoning\")\n",
    "\n",
    "        if not passed:\n",
    "            if grader.startswith(\"chemical_name_grader\"):\n",
    "                feedback_lines.append(\n",
    "                    \"Not all chemical names in the input section were included in the summary.\"\n",
    "                )\n",
    "            elif grader.startswith(\"word_length_deviation_grader\"):\n",
    "                feedback_lines.append(\n",
    "                    \"The summary length deviates too much from the expected length.\"\n",
    "                )\n",
    "            elif grader.startswith(\"cosine_similarity\"):\n",
    "                feedback_lines.append(\n",
    "                    \"The summary is not sufficiently similar to the source section (cosine similarity too low).\"\n",
    "                )\n",
    "            elif grader.startswith(\"llm_as_judge\") and reasoning:\n",
    "                feedback_lines.append(reasoning)\n",
    "\n",
    "    if not feedback_lines:\n",
    "        feedback_lines.append(DEFAULT_PASSING_FEEDBACK)\n",
    "\n",
    "    return \"\".join(feedback_lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自己進化ループ\n",
    "\n",
    "要約リクエストのストリームをシミュレートするために、準備されたデータセットを入力し、素朴なプロンプトから最適化がどのように進化するかを観察します。\n",
    "\n",
    "> 参照されているdataset.csvはGithubリポジトリで見つけることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from agents import Agent, trace\n",
    "\n",
    "EVAL_ID = eval.id #Created eval ID from above cell\n",
    "MAX_OPTIMIZATION_RETRIES = 3\n",
    "\n",
    "async def self_evolving_loop(summarization_agent: Agent) -> Agent:\n",
    "    print(f\"Starting self-evolving loop | Initial prompt v{summarization_prompt.current().version}\")\n",
    "    print(f\"Prompt:{summarization_prompt.current().prompt}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    reset_best_trackers()\n",
    "    df = pd.read_csv(\"data/dataset.csv\")\n",
    "\n",
    "    with trace(\"Self-evolving Optimization Workflow\"):\n",
    "        for _, row in df.head().iterrows():\n",
    "            content = row.get(\"content\")\n",
    "            if pd.isna(content) or (isinstance(content, str) and not content.strip()):\n",
    "                continue\n",
    "\n",
    "            section_number = str(row[\"section_number\"])\n",
    "            section = str(content)\n",
    "            current_version = summarization_prompt.current().version\n",
    "\n",
    "            print(f\"[Section {section_number}] Using prompt v{current_version}\")\n",
    "\n",
    "            optimization_success = False\n",
    "\n",
    "            for attempt in range(1, MAX_OPTIMIZATION_RETRIES + 1):\n",
    "                print(f\"  Attempt {attempt}: evaluating summary...\")\n",
    "\n",
    "                summary_result = await Runner.run(summarization_agent, section)\n",
    "                summary = summary_result.final_output\n",
    "\n",
    "                grader_scores = await get_eval_grader_score(eval_id=EVAL_ID, summary=summary, section=section)\n",
    "                average_score = calculate_grader_score(grader_scores)\n",
    "                total_score = calculate_total_grader_score(grader_scores)\n",
    "                lenient_passed = is_lenient_pass(grader_scores, average_score)\n",
    "                print(\n",
    "                    f\"\tScores — avg={average_score:.3f}, total={total_score:.3f}, lenient_passed={lenient_passed}\"\n",
    "                )\n",
    "\n",
    "                record_aggregate_prompt_score(\n",
    "                    prompt_version=summarization_prompt.current().version,\n",
    "                    prompt_text=summarization_prompt.current().prompt,\n",
    "                    model_name=summarization_prompt.current().model,\n",
    "                    average_score=average_score,\n",
    "                    total_score=total_score,\n",
    "                )\n",
    "\n",
    "                update_best_candidate(\n",
    "                    average_score=average_score,\n",
    "                    prompt_text=summarization_prompt.current().prompt,\n",
    "                    model_name=summarization_prompt.current().model,\n",
    "                    summary_text=summary,\n",
    "                    metadata={\n",
    "                        \"section\": section_number,\n",
    "                        \"average_score\": average_score,\n",
    "                        \"grader_results\": grader_scores,\n",
    "                        \"prompt_version\": summarization_prompt.current().version,\n",
    "                    },\n",
    "                    lenient_passed=lenient_passed,\n",
    "                    prompt_version=summarization_prompt.current().version,\n",
    "                )\n",
    "\n",
    "                if lenient_passed:\n",
    "                    optimization_success = True\n",
    "                    print(f\"\tPassed with prompt v{summarization_prompt.current().version}\")\n",
    "                    break\n",
    "\n",
    "                print(\"\tFailed eval. Improving prompt...\")\n",
    "                eval_feedback = collect_grader_feedback(grader_scores)\n",
    "\n",
    "                metaprompt_result = await Runner.run(\n",
    "                    metaprompt_agent,\n",
    "                    input=METAPROMPT_TEMPLATE.format(\n",
    "                        original_prompt=summarization_prompt.current().prompt,\n",
    "                        section=section,\n",
    "                        summary=summary,\n",
    "                        reasoning=eval_feedback,\n",
    "                    ),\n",
    "                )\n",
    "                improved_prompt = metaprompt_result.final_output\n",
    "                summarization_prompt.update(\n",
    "                    new_prompt=improved_prompt,\n",
    "                    metadata={\"section\": section, \"summary\": summary},\n",
    "                )\n",
    "                summarization_agent = make_summarization_agent(summarization_prompt.current())\n",
    "\n",
    "                print(f\"\tPrompt improved → v{summarization_prompt.current().version}\")\n",
    "\n",
    "            if not optimization_success:\n",
    "                print(\n",
    "                    \"\tAll attempts failed; keeping latest prompt version \"\n",
    "                    f\"v{summarization_prompt.current().version} for the next section.\"\n",
    "                )\n",
    "\n",
    "    summarization_agent = apply_best_candidate_if_needed()\n",
    "\n",
    "    print(\"\" + \"-\" * 80)\n",
    "    print(\"Completed optimization loop.\")\n",
    "    print(f\"Final prompt version: v{summarization_prompt.current().version}\")\n",
    "    if best_candidate[\"score\"] > float(\"-inf\"):\n",
    "        print(\n",
    "            f\"Best lenient prompt: v{best_candidate.get('version')} (avg={best_candidate['score']:.3f})\"\n",
    "        )\n",
    "\n",
    "    aggregate_best = select_best_aggregate_prompt()\n",
    "    if aggregate_best:\n",
    "        per_section = (\n",
    "            aggregate_best.get(\"total_average\", 0.0) / aggregate_best.get(\"count\", 1)\n",
    "            if aggregate_best.get(\"count\")\n",
    "            else 0.0\n",
    "        )\n",
    "        print(\n",
    "            f\"Aggregate best prompt: v{aggregate_best.get('version')} \"\n",
    "            f\"(total={aggregate_best.get('total_score', 0.0):.3f}, avg/section={per_section:.3f}, model={aggregate_best.get('model', 'unknown')})\"\n",
    "        )\n",
    "\n",
    "    print(f\"Final prompt:{summarization_prompt.current().prompt}\")\n",
    "    return summarization_agent\n",
    "\n",
    "summarization_agent = await self_evolving_loop(summarization_agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**最終プロンプトの選択方法**\n",
    "\n",
    "- すべての評価では、採点者の平均スコア、採点者全体の合計スコア、および試行が寛大な基準を通過したかどうかがログに記録されます。\n",
    "- `best_candidate`は最新の寛大な合格を追跡しますが（透明性のため）、最終選択では集計された合計値を使用して、全体的に最高性能のプロンプトを確実に保持します。\n",
    "- ループが終了すると、`apply_best_candidate_if_needed`は最高の累積採点者スコアを持つプロンプトを復元します（同点の場合は最新バージョンを優先）。これにより、表示されるプロンプトが観測された中で最も強力なパフォーマーであることが保証されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は上記のコードの（要約された）出力例です。\n",
    "\n",
    "出力を検査すると、自己進化プロンプトが機能したことがわかります。考慮すべきいくつかの要点があります：\n",
    "1. 最適化は常に成功するとは限らないため、プロンプトのバージョンをロールバックできることが重要です\n",
    "2. 評価者からの情報の忠実度は、品質の高い最適化を確保するために極めて重要です"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Starting self-evolving loop | Initial prompt v0\n",
    "Prompt:You are a summarization assistant.\n",
    "Given a section of text, produce a summary.\n",
    "--------------------------------------------------------------------------------\n",
    "[Section 7.1] Using prompt v0\n",
    "  Attempt 1: evaluating summary...\n",
    "\tScores — avg=0.805, total=3.218, lenient_passed=False\n",
    "\tFailed eval. Improving prompt...\n",
    "\tPrompt improved → v1\n",
    "  Attempt 2: evaluating summary...\n",
    "\tScores — avg=0.720, total=2.881, lenient_passed=False\n",
    "\tFailed eval. Improving prompt...\n",
    "\tPrompt improved → v2\n",
    "  Attempt 3: evaluating summary...\n",
    "\tScores — avg=0.762, total=3.048, lenient_passed=True\n",
    "\tPassed with prompt v2\n",
    "[Section 7.2] Using prompt v2\n",
    "  Attempt 1: evaluating summary...\n",
    "\tScores — avg=0.612, total=2.450, lenient_passed=False\n",
    "\tFailed eval. Improving prompt...\n",
    "\tPrompt improved → v3\n",
    "  Attempt 2: evaluating summary...\n",
    "\tScores — avg=0.915, total=3.660, lenient_passed=True\n",
    "\tPassed with prompt v3\n",
    "[Section 3.2.P.2.1] Using prompt v3\n",
    "  Attempt 1: evaluating summary...\n",
    "\tScores — avg=0.684, total=2.736, lenient_passed=False\n",
    "\tFailed eval. Improving prompt...\n",
    "\tPrompt improved → v4\n",
    "  Attempt 2: evaluating summary...\n",
    "\tScores — avg=0.684, total=2.736, lenient_passed=False\n",
    "\tFailed eval. Improving prompt...\n",
    "\tPrompt improved → v5\n",
    "  Attempt 3: evaluating summary...\n",
    "\tScores — avg=0.920, total=3.680, lenient_passed=True\n",
    "\tPassed with prompt v5\n",
    "[Section 3.2.P.2.2] Using prompt v5\n",
    "  Attempt 1: evaluating summary...\n",
    "\tScores — avg=0.737, total=2.950, lenient_passed=True\n",
    "\tPassed with prompt v5\n",
    "[Section 3.2.P.2.3] Using prompt v5\n",
    "  Attempt 1: evaluating summary...\n",
    "\tScores — avg=0.750, total=3.000, lenient_passed=True\n",
    "\tPassed with prompt v5\n",
    "--------------------------------------------------------------------------------\n",
    "Completed optimization loop.\n",
    "Final prompt version: v5\n",
    "Best lenient prompt: v5 (avg=0.750)\n",
    "Aggregate best prompt: v5 (total=9.630, avg/section=0.802)\n",
    "Final prompt:**Optimized Technical Summarization System Prompt**\n",
    "\n",
    "You are a technical summarization assistant specialized in scientific and regulatory documents. Your objective is to generate a summary that preserves every explicit detail and organizational structure from the source text, without any paraphrasing, omission, or synthesis.\n",
    "\n",
    "**Strict Summarization Guidelines:**\n",
    "\n",
    "**1. Comprehensive Detail Inclusion:**  \n",
    "- Transcribe all named compounds, salts, excipients, drug substances, molecular designations, batch codes, identifiers, and CAS numbers exactly as written.\n",
    "- Include every stated concentration, unit, measurement, quantitative value, compositional detail, and preparatory parameter verbatim and in original format.\n",
    "- Accurately replicate all descriptions of appearance, color, physical state, rationale for inclusion, and labeling or typographical conventions present in the source.\n",
    "- Clearly include all section titles, headings, subsections, hierarchical numbering, referenced sections, and in-line citations or figures.\n",
    "\n",
    "**2. Prohibited Actions:**  \n",
    "- Do NOT paraphrase, summarize, interpret, synthesize, restructure, generalize, or alter any information at any level.\n",
    "- Do NOT omit, compress, merge, or reorder any data point, named entity, technical term, or explicit instruction from the source.\n",
    "- Do NOT introduce additional content, inference, or editorial clarification.\n",
    "\n",
    "**3. Structural and Formatting Requirements:**  \n",
    "- Maintain verbatim order, sectioning, and hierarchy from the source text, including all original lists, bullet points, numbering, or formatting.\n",
    "- Reproduce every element in the precise sequence, alignment, and structure as the input, ensuring maximal traceability.\n",
    "- If the source uses lists, tables, subpoints, or hierarchies, mirror them exactly.\n",
    "\n",
    "**4. Precision, Fidelity, and Reviewability:**  \n",
    "- Your summary must enable full regulatory or technical audit by containing every explicit detail, designation, and measurement from the original—unaltered and unabridged.\n",
    "- The output must be comprehensive, exhaustive, and identical in informational content and structure to the input. Every visible explicit detail must be present.\n",
    "\n",
    "**Output Instruction:**  \n",
    "Begin summarization after this message, applying the above rules without exception. Each output must be concise in format but all-inclusive in content, reflecting every explicit fact, designation, and organizational feature of the source text, and suitable for regulatory or expert review. No interpretation, paraphrasing, or omission is permitted under any circumstance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェントログとトレーシング\n",
    "\n",
    "ダッシュボードのログ下で最適化ワークフローの実行を確認できます：\n",
    "\n",
    "<img src=\"../../../images/agent_log_traces.png\" alt=\"Agent log traces\" style=\"max-width:50%\">\n",
    "<br><em>図10 - ダッシュボードで最適化ワークフローの実行を示すエージェントログトレース</em>\n",
    "\n",
    "そして、異なるエージェント呼び出しの詳細を掘り下げることができます：\n",
    "\n",
    "<img src=\"../../../images/agent_trace_details.png\" alt=\"Agent trace details\" style=\"max-width:50%\">\n",
    "<br><em>図11 - 個別のエージェント呼び出しと実行フローを示す詳細なエージェントトレース</em>\n",
    "\n",
    "### 継続的監視\n",
    "\n",
    "評価ループが完了したら、システムは新しい受信データを継続的に監視し、ブラインドデータセットでモデルのパフォーマンスを定期的に再評価する必要があります。これにより、データ分布が変化してもモデルが正確で準拠した状態を維持できます。\n",
    "\n",
    "継続的監視を有効にするには、cronジョブまたは軽量なスケジューラーループを統合して、データソースの更新（例：新しいPDFアップロードやデータベースエントリ）を定期的にチェックできます。新しいデータが検出されると、システムは前述の評価と最適化ループを自動的にトリガーします。\n",
    "\n",
    "例（疑似コード）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is pseudo-code and not meant to be run as-is\n",
    "\n",
    "import time\n",
    "\n",
    "def continuous_monitoring(interval_hours=24):\n",
    "    \"\"\"Periodically check for new data and trigger the evaluation loop.\"\"\"\n",
    "    while True:\n",
    "        print(\"Checking for new data...\")\n",
    "        if new_data_detected():\n",
    "            print(\"New data found — running evaluation and optimization loop.\")\n",
    "            self_evolving_loop()\n",
    "        else:\n",
    "            print(\"No new data. Sleeping until next cycle.\")\n",
    "        time.sleep(interval_hours * 3600)\n",
    "\n",
    "continuous_monitoring(interval_hours=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このアプローチにより、モデルは継続的に学習し適応することができ、新しいデータを処理するにつれて時間の経過とともに改善されます。これは、高品質で実世界での性能を維持するための重要な要件です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. さらなる発展\n",
    "\n",
    "### a. モデル評価\n",
    "\n",
    "これで、**evals**を使ってプロンプトを改善し、評価が定義された閾値を超えた場合に新しいプロンプトを受け入れる、完全に自動化されたループが完成しました。\n",
    "\n",
    "本番環境では、新しいユーザーリクエストが入ってくる際に、同様のフレームワークを使用してエージェントのパフォーマンスを監視することができます。\n",
    "上記で述べたように、これは簡略化された例であり、実際のシナリオでは、新しいプロンプトを承認するために追加のガードレールとhuman-in-the-loopアプローチが必要になります。\n",
    "\n",
    "この概念をさらに発展させると、evalsを使用してモデルバージョン、冗長性、推論などの異なるモデルパラメータ候補をテストすることもできます。考慮可能なパラメータの完全なセットを確認するには、[Agents SDKのModelSettingsクラス](https://openai.github.io/openai-agents-python/ref/model_settings/#agents.model_settings.ModelSettings)をチェックしてください。\n",
    "\n",
    "`compare_model_candidates`関数は以下の方法の例です：\n",
    "1. プロンプトを最適化する\n",
    "2. 最適化されたプロンプトを使用して、2つ以上の異なるモデルから候補出力を生成する\n",
    "3. evalsを使用して候補出力を評価し、最適な候補を選択する\n",
    "\n",
    "この関数は最小限のリファクタリングで`self_evolving_loop`関数に組み込むことができます。\n",
    "\n",
    "> **注意：** モデルバージョンの本番テストは、同じファミリーバージョン内のバージョン（例：gpt-5、gpt-5-mini、gpt-5-nano）に限定すべきです。ファミリー間のバージョン選択は本番デプロイメント前に実施することを推奨します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして、モデル比較コードを含む最終的な `self_evolving_loop`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "\n",
    "async def eval_agent_candidate(agent: Agent, section: str, prompt_text: str, model_name: str):\n",
    "    summary_result = await Runner.run(agent, section)\n",
    "    summary = summary_result.final_output\n",
    "\n",
    "    scores = await get_eval_grader_score(\n",
    "        eval_id=EVAL_ID, summary=summary, section=section\n",
    "    )\n",
    "    average = calculate_grader_score(scores)\n",
    "    lenient_passed = is_lenient_pass(scores, average)\n",
    "    passed = all(entry.get(\"passed\") is True for entry in scores)\n",
    "\n",
    "    update_best_candidate(\n",
    "        average_score=average,\n",
    "        prompt_text=prompt_text,\n",
    "        model_name=model_name,\n",
    "        summary_text=summary,\n",
    "        metadata={\n",
    "            \"section\": section,\n",
    "            \"average_score\": average,\n",
    "            \"grader_results\": scores,\n",
    "        },\n",
    "        lenient_passed=lenient_passed,\n",
    "    )\n",
    "\n",
    "    return {\"summary\": summary, \"scores\": scores, \"average\": average, \"passed\": passed}\n",
    "\n",
    "async def compare_model_candidates(\n",
    "    summarization_prompt,\n",
    "    eval_feedback: str,\n",
    "    section: str,\n",
    "    summary: str,\n",
    "    model_candidates=None,\n",
    "):\n",
    "    \"\"\"Improve the prompt, evaluate it across candidate models, and adopt the top performer.\"\"\"\n",
    "    if model_candidates is None:\n",
    "        model_candidates = [\"gpt-5\", \"gpt-5-mini\"]\n",
    "\n",
    "    metaprompt_result = await Runner.run(\n",
    "        metaprompt_agent,\n",
    "        input=METAPROMPT_TEMPLATE.format(\n",
    "            original_prompt=summarization_prompt.current().prompt,\n",
    "            section=section,\n",
    "            summary=summary,\n",
    "            reasoning=eval_feedback,\n",
    "        ),\n",
    "    )\n",
    "    improved_prompt = metaprompt_result.final_output\n",
    "\n",
    "    async def evaluate_model(model_name: str):\n",
    "        candidate_agent = Agent(\n",
    "            name=f\"SummarizationAgent:{model_name}\",\n",
    "            instructions=improved_prompt,\n",
    "            model=model_name,\n",
    "        )\n",
    "        result = await eval_agent_candidate(candidate_agent, section, improved_prompt, model_name)\n",
    "        return model_name, candidate_agent, result\n",
    "\n",
    "    best = {\n",
    "        \"average\": float(\"-inf\"),\n",
    "        \"passed\": False,\n",
    "        \"agent\": None,\n",
    "        \"model\": None,\n",
    "        \"summary\": None,\n",
    "    }\n",
    "\n",
    "    tasks = [asyncio.create_task(evaluate_model(model_name)) for model_name in model_candidates]\n",
    "    for task in asyncio.as_completed(tasks):\n",
    "        model_name, candidate_agent, result = await task\n",
    "        print(\n",
    "            f\"Candidate average — {model_name}: {result['average']:.4f} \"\n",
    "            f\"(passed={result.get('passed', False)})\"\n",
    "        )\n",
    "        if result[\"average\"] > best[\"average\"]:\n",
    "            best.update(\n",
    "                {\n",
    "                    \"average\": result[\"average\"],\n",
    "                    \"model\": model_name,\n",
    "                    \"summary\": result.get(\"summary\"),\n",
    "                    \"agent\": candidate_agent,\n",
    "                    \"passed\": result.get(\"passed\", False),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    for task in tasks:\n",
    "        if not task.done():\n",
    "            task.cancel()\n",
    "\n",
    "    if best[\"passed\"] and best[\"model\"]:\n",
    "        summarization_prompt.update(\n",
    "            new_prompt=improved_prompt,\n",
    "            model=best[\"model\"],\n",
    "            metadata={\"section\": section, \"summary\": best[\"summary\"]},\n",
    "        )\n",
    "        print(f\"Updated summarization_prompt with passing model: {best['model']}\")\n",
    "        return make_summarization_agent(summarization_prompt.current())\n",
    "\n",
    "    print(\n",
    "        f\"No passing models. Best candidate (model={best['model']}, \"\n",
    "        f\"avg={best['average']:.4f}) did not pass. Prompt not updated.\"\n",
    "    )\n",
    "    return None\n",
    "\n",
    "async def self_evolving_loop_with_model_comparison(summarization_agent: Agent) -> Agent:\n",
    "    print(\n",
    "        f\"Starting self-evolving loop | Initial prompt v{summarization_prompt.current().version}\"\n",
    "    )\n",
    "    print(f\"Prompt: {summarization_prompt.current().prompt}\")\n",
    "    print(f\"Model: {summarization_prompt.current().model}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    reset_best_trackers()\n",
    "    df = pd.read_csv(\"data/dataset.csv\")\n",
    "\n",
    "    with trace(\"Self-evolving Optimization Workflow: model comparison\"):\n",
    "        for _, row in df.head(5).iterrows():\n",
    "            content = row.get(\"content\")\n",
    "            if pd.isna(content) or (isinstance(content, str) and not content.strip()):\n",
    "                continue\n",
    "\n",
    "            section_number = str(row[\"section_number\"])\n",
    "            section = str(content)\n",
    "            current_version = summarization_prompt.current().version\n",
    "\n",
    "            print(f\"[Section {section_number}] Using prompt v{current_version}\")\n",
    "\n",
    "            summary_passed = False\n",
    "\n",
    "            for attempt in range(1, MAX_OPTIMIZATION_RETRIES + 1):\n",
    "                print(f\"\\tAttempt {attempt}: evaluating summary...\")\n",
    "\n",
    "                summary_result = await Runner.run(summarization_agent, section)\n",
    "                summary = summary_result.final_output\n",
    "\n",
    "                grader_scores = await get_eval_grader_score(\n",
    "                    eval_id=EVAL_ID, summary=summary, section=section\n",
    "                )\n",
    "                average_score = calculate_grader_score(grader_scores)\n",
    "                total_score = calculate_total_grader_score(grader_scores)\n",
    "                lenient_passed = is_lenient_pass(grader_scores, average_score)\n",
    "                print(\n",
    "                    f\"\\tScores — avg={average_score:.3f}, total={total_score:.3f}, lenient_passed={lenient_passed}\"\n",
    "                )\n",
    "\n",
    "                record_aggregate_prompt_score(\n",
    "                    prompt_version=summarization_prompt.current().version,\n",
    "                    prompt_text=summarization_prompt.current().prompt,\n",
    "                    model_name=summarization_prompt.current().model,\n",
    "                    average_score=average_score,\n",
    "                    total_score=total_score,\n",
    "                )\n",
    "\n",
    "                update_best_candidate(\n",
    "                    average_score=average_score,\n",
    "                    total_score=total_score,\n",
    "                    prompt_text=summarization_prompt.current().prompt,\n",
    "                    model_name=summarization_prompt.current().model,\n",
    "                    summary_text=summary,\n",
    "                    metadata={\n",
    "                        \"section\": section_number,\n",
    "                        \"average_score\": average_score,\n",
    "                        \"grader_results\": grader_scores,\n",
    "                        \"prompt_version\": summarization_prompt.current().version,\n",
    "                    },\n",
    "                    lenient_passed=lenient_passed,\n",
    "                    prompt_version=summarization_prompt.current().version,\n",
    "                )\n",
    "\n",
    "                if lenient_passed:\n",
    "                    summary_passed = True\n",
    "                    print(\n",
    "                        f\"\\tPassed with prompt v{summarization_prompt.current().version} (model={summarization_prompt.current().model})\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                print(\"\\tFailed eval. Improving prompt...\")\n",
    "                eval_feedback = collect_grader_feedback(grader_scores)\n",
    "\n",
    "                new_agent = await compare_model_candidates(\n",
    "                    summarization_prompt=summarization_prompt,\n",
    "                    eval_feedback=eval_feedback,\n",
    "                    section=section,\n",
    "                    summary=summary,\n",
    "                    # model_candidates could be given as an argument if you want to expand options.\n",
    "                )\n",
    "\n",
    "                if new_agent is None:\n",
    "                    print(\n",
    "                        \"\\tNo passing model found. Optimization failed for this section.\"\n",
    "                    )\n",
    "                    summary_passed = False\n",
    "                else:\n",
    "                    summarization_agent = new_agent\n",
    "                    summary_passed = True\n",
    "                    print(\n",
    "                        f\"\\tPrompt improved → v{summarization_prompt.current().version} \"\n",
    "                        f\"(model={summarization_prompt.current().model})\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "            if not summary_passed:\n",
    "                print(\n",
    "                    \"\\tAll attempts failed; keeping latest prompt version \"\n",
    "                    f\"v{summarization_prompt.current().version} (model={summarization_prompt.current().model}) for the next section.\"\n",
    "                )\n",
    "\n",
    "    summarization_agent = apply_best_candidate_if_needed()\n",
    "\n",
    "    print(\"\" + \"-\" * 80)\n",
    "    print(\"Completed optimization loop.\")\n",
    "    print(f\"Final prompt version: v{summarization_prompt.current().version}\")\n",
    "    print(f\"Final model: {summarization_prompt.current().model}\")\n",
    "    aggregate_best = select_best_aggregate_prompt()\n",
    "    if best_candidate[\"score\"] > float(\"-inf\"):\n",
    "        print(\n",
    "            f\"Best lenient prompt: v{best_candidate.get('version')} (avg={best_candidate['score']:.3f}, model={best_candidate.get('model', 'unknown')})\"\n",
    "        )\n",
    "    if aggregate_best:\n",
    "        per_section = (\n",
    "            aggregate_best.get(\"total_average\", 0.0) / aggregate_best.get(\"count\", 1)\n",
    "            if aggregate_best.get(\"count\")\n",
    "            else 0.0\n",
    "        )\n",
    "        print(\n",
    "            f\"Aggregate best prompt: v{aggregate_best.get('version')} \"\n",
    "            f\"(total={aggregate_best.get('total_score', 0.0):.3f}, avg/section={per_section:.3f}, model={aggregate_best.get('model', 'unknown')})\"\n",
    "        )\n",
    "    print(f\"Final prompt: {summarization_prompt.current().prompt}\")\n",
    "    print(f\"Final model: {summarization_prompt.current().model}\")\n",
    "    return summarization_agent\n",
    "\n",
    "summarization_agent = await self_evolving_loop_with_model_comparison(summarization_agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、モデルバージョンのスコアに関する追加情報を含む、非常に似た出力を確認できます："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Starting self-evolving loop | Initial prompt v0\n",
    "Prompt:\n",
    "\tYou are a summarization assistant.\n",
    "Given a section of text, produce a concise, accurate summary.\n",
    "\n",
    "[....]\n",
    "\n",
    "[Section 3.2.P.2.2] Using prompt v2\n",
    "\tAttempt 1: evaluating summary...\n",
    "\tFailed eval. Improving prompt...\n",
    "Candidate average — gpt-5: 0.3533 (passed=False)\n",
    "Candidate average — gpt-5-mini: 0.4670 (passed=False)\n",
    "No passing models. Best candidate (model=gpt-5-mini, avg=0.4670) did not pass. Prompt not updated.\n",
    "\tNo passing model found. Optimization failed for this section.\n",
    "\tAttempt 2: evaluating summary...\n",
    "Exceeded retries, aborting\n",
    "\tPassed with prompt v2\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "Completed optimization loop.\n",
    "Final prompt version: v2\n",
    "Final prompt:\n",
    "**Improved Prompt:**\n",
    "\n",
    "You are a summarization assistant.  \n",
    "Given any section of text, generate a concise and accurate summary that includes all key concepts, components, and their main characteristics or interactions as described in the original section. Your summary should be brief yet complete, faithfully reflecting essential information, descriptors, and relationships between elements while omitting unnecessary details. Ensure the summary maintains the original meaning and captures all critical content and terminology relevant to the section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Genetic-Pareto（GEPA）によるプロンプト最適化\n",
    "\n",
    "自己進化ループが機能し、Evalsを使用してプロンプトを自律的に改善できることを実証しました。しかし、システムプロンプトを改善するために、比較的単純で静的なメタプロンプトに依存していました。このセクションでは、Genetic-Pareto（GEPA）[[1]](##Citations)を使用したより動的で反射的な手法を探求します。GEPAは、エージェントの軌跡をサンプリングし、自然言語でそれらを反映し、プロンプトの修正を提案し、反復的なフィードバックループを通じてシステムを進化させるフレームワークです。\n",
    "\n",
    "[こちら](https://doi.org/10.48550/arXiv.2507.19457)で入手可能な論文で説明されているGEPA手法は、継続的で自己改善するプロンプト最適化のための魅力的な設計図を提供します。以下のコードは、[こちら](https://github.com/gepa-ai/gepa)で入手可能なGEPA Githubリポジトリを大いに参考にしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gepa\n",
    "from gepa import EvaluationBatch\n",
    "\n",
    "# Extract sections from dataset\n",
    "def read_csv_content(file_path: str) -> list[dict]:\n",
    "    \"\"\"Read csv and return section to summarize.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return [{'content': content} for content in df['content'].tolist()]\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "trainset = read_csv_content(\"data/dataset.csv\")\n",
    "val_cut = max(1, int(0.1 * len(trainset)))\n",
    "valset = trainset[:val_cut] if len(trainset) > 1 else trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "私たちは小さなアダプターを追加することで、グレーダーとヘルパー関数を再利用し、セットアップがGEPAで動作するようにします。GEPAの`GEPAAdapter`により、評価フレームワークに簡単に組み込むことができます。3つのフックを定義しました：\n",
    "\n",
    "- `evaluate`: 前のセクションで定義されたグレーダー（chemical_name_grader、word_length_deviation_grader、cosine_similarity、llm_as_judge）を使用して要約を実行し、評価を行います。\n",
    "- `get_components_to_update`: GEPAが進化させるべきテキストフィールド（ここではsystem_prompt）を取得します。\n",
    "- `make_reflective_dataset`: リフレクション用に入力、出力、フィードバックをパッケージ化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalsBackedSummarizationAdapter:\n",
    "    \"\"\"\n",
    "    Minimal adapter for GEPA:\n",
    "      - evaluate(...) -> EvaluationBatch (scores + outputs + feedback-rich trajectories)\n",
    "      - get_components_to_update(...) returns the prompt to update\n",
    "      - make_reflective_dataset(...) packages examples for reflection\n",
    "    \"\"\"\n",
    "    propose_new_texts = None  # use GEPA's default reflection flow\n",
    "\n",
    "    def __init__(self, client, eval_id: str, gen_model: str = \"gpt-5\", user_prefix: str | None = None):\n",
    "        self.client = client\n",
    "        self.eval_id = eval_id\n",
    "        self.gen_model = gen_model\n",
    "        self.user_prefix = user_prefix or \"Summarize:\\n\\n\"\n",
    "\n",
    "    # Same summarization agent as in the previous section\n",
    "    def _summarize(self, system_prompt: str, section: str) -> str:\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=self.gen_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{self.user_prefix}{section}\"},\n",
    "            ],\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "\n",
    "    # Required by GEPA: run eval minibatch\n",
    "    def evaluate(self, inputs: list[dict], candidate: dict, capture_traces: bool = True) -> EvaluationBatch:\n",
    "        system_prompt = candidate[\"system_prompt\"]\n",
    "\n",
    "        scores: list[float] = []\n",
    "        outputs: list[str] = []\n",
    "        trajectories: list[dict] = []\n",
    "\n",
    "        for item in inputs:\n",
    "            section = item[\"content\"]\n",
    "\n",
    "            # 1) Generate with the candidate prompt\n",
    "            summary = self._summarize(system_prompt, section)\n",
    "            outputs.append(summary)\n",
    "\n",
    "            # 2) Grade using previous evals pipeline\n",
    "            run = run_eval(eval_id=self.eval_id, section=section, summary=summary)\n",
    "            out_items = poll_eval_run(eval_id=self.eval_id, run_id=run.id)\n",
    "            grader_scores = parse_eval_run_output(out_items)\n",
    "\n",
    "            # 3) Score + actionable feedback\n",
    "            scalar = calculate_grader_score(grader_scores)\n",
    "            feedback = collect_grader_feedback(grader_scores) or \"All graders passed; keep precision and coverage.\"\n",
    "\n",
    "            scores.append(float(scalar))\n",
    "            trajectories.append(\n",
    "                {\n",
    "                    \"inputs\": {\"section\": section},\n",
    "                    \"generated_output\": summary,\n",
    "                    \"metrics\": {\n",
    "                        \"combined\": float(scalar),\n",
    "                        \"by_grader\": grader_scores,  # keeping for analysis if needed\n",
    "                    },\n",
    "                    \"feedback\": feedback,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return EvaluationBatch(scores=scores, outputs=outputs, trajectories=trajectories)\n",
    "\n",
    "    # Required by GEPA: text field to evolve\n",
    "    def get_components_to_update(self, candidate: dict) -> list[str]:\n",
    "        return [\"system_prompt\"]\n",
    "\n",
    "    # Required by GEPA: build the reflective dataset the reflection LM will read\n",
    "    def make_reflective_dataset(self, candidate: dict, eval_batch: EvaluationBatch, components_to_update: list[str]) -> dict:\n",
    "        examples = []\n",
    "        for traj in (eval_batch.trajectories or []):\n",
    "            examples.append(\n",
    "                {\n",
    "                    \"Inputs\": {\"section\": traj[\"inputs\"][\"section\"]},\n",
    "                    \"Generated Outputs\": traj[\"generated_output\"],\n",
    "                    \"Feedback\": traj[\"feedback\"],\n",
    "                }\n",
    "            )\n",
    "        return {\"system_prompt\": examples}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アダプターの準備ができたので、比較のために以前の自己進化ループと同じ開始プロンプト（`\"You are a summarization assistant. Given a section of text, produce a summary.\"`）とモデル（ここでは`gpt-5`）を使用してGEPAを実行できます。アダプターインスタンス、シード候補、および訓練/検証セットを`gepa.optimize(...)`に提供します。最適化中、GEPAは候補をスコア付けするためにアダプターを繰り返し呼び出し、フィードバックを反映し、最終的に最良の進化したプロンプトを生成します。\n",
    "\n",
    "_注意：GEPAの完了には約10-15分かかる場合があります。_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_candidate = {\"system_prompt\": \"You are a summarization assistant. Given a section of text, produce a summary.\"}\n",
    "\n",
    "adapter = EvalsBackedSummarizationAdapter(\n",
    "    client=client,\n",
    "    eval_id=EVAL_ID,\n",
    "    gen_model=summarization_prompt.current().model,  \n",
    ")\n",
    "\n",
    "# Keeping max_metric_calls small for the cookbook. \n",
    "# In practice, use a larger value to allow more optimization iterations.\n",
    "result = gepa.optimize(\n",
    "    seed_candidate=seed_candidate,\n",
    "    trainset=trainset,\n",
    "    valset=valset,\n",
    "    adapter=adapter,\n",
    "    reflection_lm=\"gpt-5\",\n",
    "    max_metric_calls=10,\n",
    "    track_best_outputs=True,\n",
    "    display_progress_bar=True\n",
    ")\n",
    "\n",
    "best_prompt = result.best_candidate[\"system_prompt\"]\n",
    "print(\"\\n=== Best evolved instruction ===\\n\")\n",
    "print(best_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は上記のコードの（簡略化された）出力例です："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Iteration 0: Base program full valset score: 0.2183466466681351\n",
    "Iteration 1: Selected program 0 score: 0.2183466466681351\n",
    "Iteration 1: Proposed new text for system_prompt: \n",
    "\n",
    "[.......]\n",
    "\n",
    "Iteration 3: New subsample score 0.6592202195294341 is better than old score 0.6565039300893376. Continue to full eval and add to candidate pool.\n",
    "GEPA Optimization:  90%|█████████ | 18/20 [39:21<04:22, 131.19s/rollouts]\n",
    "Iteration 3: Full valset score for new program: 0.2225472423976205\n",
    "Iteration 3: Full train_val score for new program: 0.2225472423976205\n",
    "Iteration 3: Individual valset scores for new program: [0.22866548337721018, 0.21864704884895614, 0.2203291949666952]\n",
    "Iteration 3: New valset pareto front scores: [0.23142100182952327, 0.2389098334382265, 0.23513790628541456]\n",
    "Iteration 3: Full valset pareto front score: 0.2351562471843881\n",
    "Iteration 3: Updated valset pareto front programs: [{1}, {1}, {1}]\n",
    "Iteration 3: Best valset aggregate score so far: 0.2351562471843881\n",
    "Iteration 3: Best program as per aggregate score on train_val: 1\n",
    "Iteration 3: Best program as per aggregate score on valset: 1\n",
    "Iteration 3: Best score on valset: 0.2351562471843881\n",
    "Iteration 3: Best score on train_val: 0.2351562471843881\n",
    "Iteration 3: Linear pareto front program index: 1\n",
    "Iteration 3: New program candidate index: 2\n",
    "\n",
    "=== Best evolved instruction ===\n",
    "\n",
    "You are a domain-aware summarization assistant for technical pharmaceutical texts. Given a “section” of text, produce a concise summary that preserves key technical facts and exact nomenclature.\n",
    "\n",
    "Requirements:\n",
    "- Length and format:\n",
    "  - Write 1–3 sentences totaling about 45–70 words (never exceed 90 words). Default to ~60 words.\n",
    "  - Use a single paragraph (no bullet points, headings, or heavy formatting).\n",
    "- Preserve exact technical names and notation:\n",
    "  - Include every chemical name that appears in the section at least once, with exact spelling, capitalization, isotopic labels, brackets, hyphens, salts, and buffer names (e.g., Hyperpolarized Pyruvate (13C) Injection; [1-13C]pyruvic acid; hyperpolarized [1-13C]pyruvate; 15 mM AH111501 sodium salt; TRIS/EDTA buffer solution).\n",
    "  - Keep study identifiers, section numbers, regulatory citations, and codes verbatim when mentioned (e.g., GE-101-001, GE-101-003, USP <797>, 3.2.P.7, company codes, CAS numbers).\n",
    "...\n",
    "Self-check before finalizing:\n",
    "- Have you included every chemical name exactly as written?\n",
    "- Is the summary within 45–70 words (≤90 max) and a single paragraph?\n",
    "- Are key process/regulatory/test details and critical numbers preserved without unnecessary verbosity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このクックブックでは、プロンプト最適化に対する3つの異なるアプローチを探求しました：\n",
    "\n",
    "- **OpenAI Platform Optimizer:** 手動で入力された人間のフィードバック（サムズアップ/ダウンと文章コメント）を含むデータセットで_Optimize_ボタンを使用し、最小限の設定で迅速に強力なプロンプトを生成しました。この手法は高速な反復処理に優れていますが、本番環境に必要な自動化は提供されません。\n",
    "\n",
    "- **静的メタプロンプトを使用した最適化:** 4つの異なる評価器を組み込んだループにより、手動介入なしで自動探索と反復的な自己改善を可能にしました。しかし、その探索空間は単一の静的メタプロンプトによって制限され、評価はセクションごとに実行されました。その結果、このアプローチは、より広範な汎化を達成するのではなく、即座の評価器フィードバックに過学習するリスクがありました。\n",
    "\n",
    "- **GEPA最適化:** より構造化された検索プロセスを提供し、反省的な更新は定量的スコアと文章フィードバックの両方によって情報を得て、候補は一つのデータセットで訓練され、別のデータセットで検証されました。この手法は、より堅牢で汎化されたプロンプトを生成し、そのパフォーマンスのより明確な実証的証拠を提供しました。\n",
    "\n",
    "_注：各手法によって生成されたプロンプトの例は付録で確認できます。_\n",
    "\n",
    "使用ケースに応じて、速度（OpenAI optimizer）、軽量な自動化（静的メタプロンプト）、または体系的な汎化（GEPA）を優先することができます。実際には、高速な反復処理から始めて反省的最適化に進むことでこれらの手法を組み合わせることで、機敏性とパフォーマンスの両方を実現できます。\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 貢献者\n",
    "\n",
    "このクックブックは[Bain](www.bain.com)と[OpenAI](openai.com)の共同コラボレーションに基づいています。\n",
    "\n",
    "[Calvin Maguranis](https://www.linkedin.com/in/calvin-maguranis-b9956045/)  \n",
    "[Fanny Perraudeau](https://www.linkedin.com/in/fanny-sabran-perraudeau-494b7573/)   \n",
    "[Giorgio Saladino](https://www.linkedin.com/in/giorgio-saladino-202/)   \n",
    "[Shikhar Kwatra](https://www.linkedin.com/in/shikharkwatra/)    \n",
    "[Valentina Frenkel](https://www.linkedin.com/in/valentina-frenkel/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引用文献\n",
    "\n",
    "[1] _GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning_ by Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab - https://arxiv.org/abs/2507.19457"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 付録\n",
    "\n",
    "### 出力プロンプトの例：\n",
    "\n",
    "- **初期プロンプト:**  \n",
    "```pgsql \n",
    "You are a summarization assistant. Given a section of text, produce a summary.\n",
    "```\n",
    "\n",
    "- **OpenAI Platform Optimizer:** \n",
    "```pgsql \n",
    "You are a summarization assistant.\n",
    "Task: Summarize the provided text concisely and accurately.\n",
    "Output requirements:\n",
    "- Output only the summary. Do not add titles, labels (e.g.,\n",
    "\"Summary:\"), prefaces, or commentary.\n",
    "- Preserve the document's structure. If multiple sections/subsections appear, summarize each one.\n",
    "- Use a numbered list for sections/subsections (use their numbers/titles when present).\n",
    "- Under each, use short dash bullets for key points.\n",
    "- If there is only a single short section, return a brief bullet list or 1-2 concise sentences.\n",
    "- Split any inline lists into separate bullets.\n",
    "- Use plain, simple language. Keep bullets tight (ideally one line each). Remove redundancy.\n",
    "- Include important quantitative details (values, units, conditions) and constraints. Do not invent information.\n",
    "- Keep formatting simple: plain text, \"1.\" numbering and \"-\" bullets only. No tables or special markup.\n",
    "- Retain exact technical terms/notation from the source (e.g., chemical names, isotopic labels).\n",
    "- If a section is explicitly marked \"Not applicable,\" include that status; otherwise do not add it.\n",
    "```\n",
    "\n",
    "- **静的メタプロンプト:** \n",
    "```pgsql \n",
    "You are a technical summarization assistant for scientific and regulatory documentation. Your task is to generate a concise, comprehensive, and fully detailed summary of any scientific, technical, or regulatory text provided. Strictly adhere to the following instructions:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Complete and Exact Information Inclusion**  \n",
    "- Capture *every* explicit fact, technical value, specification, quantity, measurement, regulatory reference, entity, process, site, and contextual detail verbatim from the source text.\n",
    "- Do not omit or generalize any explicit information, no matter how minor.\n",
    "\n",
    "**2. Precise Terminology and Named Entity Retention**  \n",
    "- Reproduce all names of chemicals, drugs, mixtures, buffer components, devices, companies, institutions, regulatory standards, section numbers, and procedural labels *exactly as stated*.\n",
    "- Report all quantities, measurements, concentrations, ratios, masses, volumes, compositions, pH values, and units precisely as given.\n",
    "- Do not paraphrase, rename, substitute, or simplify any term or value.\n",
    "\n",
    "**3. All Procedural Details and Justifications**  \n",
    "- Explicitly include all described procedures, technical processes (e.g., terminal sterilization, aseptic processing), operational constraints, process justifications, compliance requirements, and standards references.\n",
    "- Clearly state all reasons provided for choosing or omitting particular methods or processes.\n",
    "\n",
    "**4. Regulatory and Compliance References**  \n",
    "- Accurately cite all regulations, standards (e.g., USP <797>), compliance statements, section numbers, and cross-references as in the original.\n",
    "- Include all explicit mentions of compliance, applicability, and site location details.\n",
    "\n",
    "**5. Explicit Statements of Absence, Limitations, and Applicability**  \n",
    "- Clearly state any declarations of absence, inapplicability (\"Not applicable\"), or limitations exactly as written in the source.\n",
    "\n",
    "**6. Structural and Organizational Fidelity**  \n",
    "- Precisely reflect the original document's section and subsection hierarchy, using clear section labels and indentation.\n",
    "- Present all enumerations, lists, and tabulated data in structured bullet-point or numbered format, organized in accordance with the source document's arrangement.\n",
    "\n",
    "**7. No Paraphrasing, Summarizing, or Reinterpretation**  \n",
    "- Do *not* paraphrase, summarize contextually, reinterpret, or alter the meaning or sequence of any content.\n",
    "- Remove only literal repetitions or redundant phrasing; otherwise, preserve all explicit statements, technical details, and contextual notes.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Output Objective:**  \n",
    "Produce a summary that delivers the full technical, factual, and regulatory content and structure of the original text, reformatted by eliminating only redundant language. The summary must enable audit, regulatory review, or peer reference without loss of any explicit information or terminology from the source.\n",
    "\n",
    "---\n",
    "\n",
    "*Apply these instructions rigorously to every provided document section to ensure scientific and regulatory accuracy and completeness.*\n",
    "```\n",
    "\n",
    "- **GEPA optimizer**: \n",
    "```pgsql \n",
    "You are a domain-aware summarization assistant for technical pharmaceutical texts. Given a \"section\" of text, produce a concise, single-paragraph summary that preserves key technical facts and exact nomenclature.\n",
    "\n",
    "Length and format\n",
    "- Write 1–3 sentences totaling about 45–70 words (target ~60; never exceed 90).\n",
    "- Use one paragraph; no bullets, headings, tables, or heavy formatting.\n",
    "\n",
    "Exact names and notation\n",
    "- Include every chemical name that appears in the section at least once, using the exact original spelling, capitalization, punctuation, isotopic labels, brackets, hyphens, salts, buffer names, and parenthetical qualifiers. Treat distinct case/format variants as distinct names (e.g., [1-13C]pyruvic acid and [1-13C]Pyruvic acid are separate and each must appear once).\n",
    "- Examples you must preserve verbatim when present: Hyperpolarized Pyruvate (13C) Injection; non-polarized Pyruvate Injection; Pyruvate (13C) Injection; hyperpolarized [1-13C]pyruvate; Mixture of [1-13C]pyruvic acid and 15 mM AH111501 sodium salt; TRIS/EDTA buffer solution; TRIS; NaOH; Na2EDTA; [1-13C]pyruvic acid; AH111501 sodium salt.\n",
    "- Also preserve exact study identifiers, batch codes, section numbers, regulatory citations, and instrument parameters as written (e.g., GE-101-001, GE-101-003, USP <797>, 3.2.P.5.2.5, FFF106/140-806, FFF106/142-806, 3T MRI, 5 degree RF pulse, TR=3s, 90 degree pulse, 64 averages, TR=10s, 10 μl Gd/ml solution).\n",
    "\n",
    "Content prioritization (if space is tight)\n",
    "1) What the section is about (topic/purpose).\n",
    "2) All named chemical entities and compositions (list all chemical names at least once; include concentrations/amounts if given).\n",
    "3) Critical process/handling facts (e.g., aseptic processing vs terminal sterilization; ISO classifications; filtration specs; compounding/filling steps; temperatures/times/volumes; storage/administration limits).\n",
    "4) Container/packaging specifics (e.g., cryovials, \"sterile fluid path\").\n",
    "5) Microbiological/testing/regulatory details (e.g., sterility/pyrogenicity testing timing; USP <797>; state board compliance; site/manufacturer if stated).\n",
    "6) Overages/single-dose formulas and key quantities.\n",
    "\n",
    "Numerical fidelity\n",
    "- Preserve all critical numbers and units exactly (e.g., 1.44 g, 27.7 mg, 15 mM, 18 mL, 1.47 g, two 0.2 μm filters, ISO 7, ISO 5, 38 mL).\n",
    "- Include testing/analysis parameters when present (e.g., polarization/relaxation time (T1); number of spectra; pulse angles; TR values; MRI location relative to clean room).\n",
    "\n",
    "Style and compression\n",
    "- Be neutral and factual; do not infer unstated information.\n",
    "- Consolidate repeated statements; compress lists with commas/semicolons to save words.\n",
    "- Mention tables/figures only to convey key data; do not reproduce them.\n",
    "- If many chemicals are present, ensure each distinct name appears once; group them succinctly.\n",
    "- Avoid symbols or special formatting not in the source text.\n",
    "\n",
    "Common domain cues to include when present\n",
    "- Aseptic processing vs terminal sterilization and the rationale/timing (e.g., \"tested for sterility and pyrogenicity subsequent to patient administration\").\n",
    "- Environmental/processing controls (ISO 7/ISO 5; LAF unit; filtration; filling/weight targets per cryovial).\n",
    "- Site/regulatory context (e.g., USP <797>; California State Board of Pharmacy; University of California, San Francisco Department of Clinical Pharmacy).\n",
    "- Study/kit equivalence statements (e.g., equivalence to GE-101-001/GE-101-003 formulations).\n",
    "- QC/measurement methods (e.g., capacitive threshold at Administration syringe nominal 38 mL).\n",
    "\n",
    "Self-check before finalizing\n",
    "- Does the paragraph contain every distinct chemical name exactly as written in the section (including case and notation variants)?\n",
    "- Is the summary 45–70 words (≤90), in a single paragraph?\n",
    "- Are the most critical process/regulatory/testing details and all key numbers preserved without unnecessary verbosity?`\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
