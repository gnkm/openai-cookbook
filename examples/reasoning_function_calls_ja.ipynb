{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論モデルでの関数呼び出しの管理\n",
    "OpenAIは現在、[推論モデル](https://platform.openai.com/docs/guides/reasoning?api-mode=responses)を使用した関数呼び出しを提供しています。推論モデルは論理的な思考の連鎖に従うように訓練されており、複雑または多段階のタスクにより適しています。\n",
    "\n",
    "> _o3やo4-miniなどの推論モデルは、推論を実行するために強化学習で訓練されたLLMです。推論モデルは回答する前に考え、ユーザーに応答する前に長い内部思考の連鎖を生成します。推論モデルは複雑な問題解決、コーディング、科学的推論、およびエージェント的ワークフローのための多段階計画において優れています。また、軽量なコーディングエージェントであるCodex CLIにとって最適なモデルでもあります。_\n",
    "\n",
    "ほとんどの場合、APIを介してこれらのモデルを使用することは非常にシンプルで、馴染みのある「チャット」モデルを使用することと同等です。\n",
    "\n",
    "ただし、特に関数呼び出しなどの機能を使用する際には、留意すべきいくつかのニュアンスがあります。\n",
    "\n",
    "このノートブックのすべての例では、会話状態の管理に便利な抽象化を提供する新しい[Responses API](https://community.openai.com/t/introducing-the-responses-api/1140929)を使用しています。ただし、ここでの原則は古いchat completions APIを使用する場合にも関連します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論モデルへのAPI呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai\n",
    "# Import libraries \n",
    "import json\n",
    "from openai import OpenAI\n",
    "from uuid import uuid4\n",
    "from typing import Callable\n",
    "\n",
    "client = OpenAI()\n",
    "MODEL_DEFAULTS = {\n",
    "    \"model\": \"o4-mini\", # 200,000 token context window\n",
    "    \"reasoning\": {\"effort\": \"low\", \"summary\": \"auto\"}, # Automatically summarise the reasoning process. Can also choose \"detailed\" or \"none\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responses APIを使用して推論モデルに簡単な呼び出しを行ってみましょう。\n",
    "低い推論努力を指定し、便利な`output_text`属性でレスポンスを取得します。\n",
    "フォローアップの質問をして、`previous_response_id`を使用することで、OpenAIに会話履歴を自動的に管理させることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among the last four Summer Olympic host cities—Tokyo (2020), Rio de Janeiro (2016), London (2012) and Beijing (2008)—Rio de Janeiro has by far the warmest climate. Average annual temperatures are roughly:\n",
      "\n",
      "• Rio de Janeiro: ≈ 23 °C  \n",
      "• Tokyo: ≈ 16 °C  \n",
      "• Beijing: ≈ 13 °C  \n",
      "• London: ≈ 11 °C  \n",
      "\n",
      "So Rio de Janeiro has the highest average temperature.\n",
      "Among those four, London has the lowest average annual temperature, at about 11 °C.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    input=\"Which of the last four Olympic host cities has the highest average temperature?\",\n",
    "    **MODEL_DEFAULTS\n",
    ")\n",
    "print(response.output_text)\n",
    "\n",
    "response = client.responses.create(\n",
    "    input=\"what about the lowest?\",\n",
    "    previous_response_id=response.id,\n",
    "    **MODEL_DEFAULTS\n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とても簡単です！\n",
    "\n",
    "私たちは比較的複雑な質問をしており、モデルが計画を立てて段階的に進める必要があるかもしれませんが、この推論は私たちには隠されています - 単に少し長く待ってから応答が表示されるだけです。\n",
    "\n",
    "しかし、出力を詳しく調べると、モデルがモデルのコンテキストウィンドウに含まれているが、エンドユーザーには公開されていない隠された「推論」トークンのセットを使用していることがわかります。\n",
    "これらのトークンと推論の要約（ただし使用された実際のトークンではない）を応答で確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Determining lowest temperatures**\n",
      "\n",
      "The user is asking about the lowest average temperatures of the last four Olympic host cities: Tokyo, Rio, London, and Beijing. I see London has the lowest average temperature at around 11°C. If I double-check the annual averages: Rio is about 23°C, Tokyo is around 16°C, and Beijing is approximately 13°C. So, my final answer is London with an average of roughly 11°C. I could provide those approximate values clearly for the user.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 136,\n",
       " 'input_tokens_details': {'cached_tokens': 0},\n",
       " 'output_tokens': 89,\n",
       " 'output_tokens_details': {'reasoning_tokens': 64},\n",
       " 'total_tokens': 225}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(next(rx for rx in response.output if rx.type == 'reasoning').summary[0].text)\n",
    "response.usage.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらの推論トークンについて知っておくことは重要です。なぜなら、従来のチャットモデルと比べて、利用可能なコンテキストウィンドウをより早く消費することを意味するからです。\n",
    "\n",
    "## カスタム関数の呼び出し\n",
    "カスタムツールの使用も必要とする複雑なリクエストをモデルに求めた場合、何が起こるでしょうか？\n",
    "* オリンピック開催都市についてさらに質問があるが、各都市のIDを含む内部データベースもあると想像してみましょう。\n",
    "* モデルは結果を返す前に、推論プロセスの途中で私たちのツールを呼び出す必要がある可能性があります。\n",
    "* ランダムなUUIDを生成する関数を作成し、モデルにこれらのUUIDについて推論するよう求めてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_city_uuid(city: str) -> str:\n",
    "    \"\"\"Just a fake tool to return a fake UUID\"\"\"\n",
    "    uuid = str(uuid4())\n",
    "    return f\"{city} ID: {uuid}\"\n",
    "\n",
    "# The tool schema that we will pass to the model\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"get_city_uuid\",\n",
    "        \"description\": \"Retrieve the internal ID for a city from the internal database. Only invoke this function if the user needs to know the internal ID for a city.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": {\"type\": \"string\", \"description\": \"The name of the city to get information about\"}\n",
    "            },\n",
    "            \"required\": [\"city\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# This is a general practice - we need a mapping of the tool names we tell the model about, and the functions that implement them.\n",
    "tool_mapping = {\n",
    "    \"get_city_uuid\": get_city_uuid\n",
    "}\n",
    "\n",
    "# Let's add this to our defaults so we don't have to pass it every time\n",
    "MODEL_DEFAULTS[\"tools\"] = tools\n",
    "\n",
    "response = client.responses.create(\n",
    "    input=\"What's the internal ID for the lowest-temperature city?\",\n",
    "    previous_response_id=response.id,\n",
    "    **MODEL_DEFAULTS)\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は`output_text`が取得できませんでした。レスポンスの出力を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_68246219e8288191af051173b1d53b3f0c4fbdb0d4a46f3c', summary=[], type='reasoning', status=None),\n",
       " ResponseFunctionToolCall(arguments='{\"city\":\"London\"}', call_id='call_Mx6pyTjCkSkmASETsVASogoC', name='get_city_uuid', type='function_call', id='fc_6824621b8f6c8191a8095df7230b611e0c4fbdb0d4a46f3c', status='completed')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論ステップと併せて、モデルは正常にツール呼び出しの必要性を特定し、関数呼び出しに送信する指示を返しました。\n",
    "\n",
    "関数を呼び出して結果をモデルに送信し、推論を継続できるようにしましょう。\n",
    "関数のレスポンスは特別な種類のメッセージなので、次のメッセージを特別な種類の入力として構造化する必要があります：\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": function_call.call_id,\n",
    "    \"output\": tool_output\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the function call(s) from the response\n",
    "new_conversation_items = []\n",
    "function_calls = [rx for rx in response.output if rx.type == 'function_call']\n",
    "for function_call in function_calls:\n",
    "    target_tool = tool_mapping.get(function_call.name)\n",
    "    if not target_tool:\n",
    "        raise ValueError(f\"No tool found for function call: {function_call.name}\")\n",
    "    arguments = json.loads(function_call.arguments) # Load the arguments as a dictionary\n",
    "    tool_output = target_tool(**arguments) # Invoke the tool with the arguments\n",
    "    new_conversation_items.append({\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": function_call.call_id, # We map the response back to the original function call\n",
    "        \"output\": tool_output\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The internal ID for London is 816bed76-b956-46c4-94ec-51d30b022725.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    input=new_conversation_items,\n",
    "    previous_response_id=response.id,\n",
    "    **MODEL_DEFAULTS\n",
    ")\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これはここでは素晴らしく動作します - モデルが応答するために必要なのは単一の関数呼び出しだけであることがわかっているためです - しかし、推論を完了するために複数のツール呼び出しを実行する必要がある状況も考慮する必要があります。\n",
    "\n",
    "2番目の呼び出しを追加して、ウェブ検索を実行してみましょう。\n",
    "\n",
    "OpenAIのウェブ検索ツールは、推論モデルでは標準では利用できません（2025年5月時点 - これは近いうちに変更される可能性があります）が、4o miniまたは他のウェブ検索対応モデルを使用してカスタムウェブ検索関数を作成するのはそれほど難しくありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(query: str) -> str:\n",
    "    \"\"\"Search the web for information and return back a summary of the results\"\"\"\n",
    "    result = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=f\"Search the web for '{query}' and reply with only the result.\",\n",
    "        tools=[{\"type\": \"web_search_preview\"}],\n",
    "    )\n",
    "    return result.output_text\n",
    "\n",
    "tools.append({\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"web_search\",\n",
    "        \"description\": \"Search the web for information and return back a summary of the results\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\"type\": \"string\", \"description\": \"The query to search the web for.\"}\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    })\n",
    "tool_mapping[\"web_search\"] = web_search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複数の関数を順次実行する\n",
    "\n",
    "一部のOpenAIモデルは`parallel_tool_calls`パラメータをサポートしており、これによりモデルが関数の配列を返し、それらを並列で実行することができます。しかし、推論モデルは順次実行する必要がある一連の関数呼び出しを生成する場合があります。特に、一部のステップが前のステップの結果に依存する場合があるためです。\n",
    "\n",
    "そのため、任意の複雑な推論ワークフローを処理するために使用できる一般的なパターンを定義する必要があります：\n",
    "\n",
    "* 会話の各ステップで、ループを初期化する\n",
    "* レスポンスに関数呼び出しが含まれている場合、推論が進行中であると仮定し、関数の結果（および中間的な推論）をモデルにフィードバックして、さらなる推論を行う必要がある\n",
    "* 関数呼び出しがなく、代わりにタイプが'message'の`Response.output`を受信した場合、エージェントが推論を完了したと安全に仮定でき、ループから抜け出すことができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's wrap our logic above into a function which we can use to invoke tool calls.\n",
    "def invoke_functions_from_response(response,\n",
    "                                   tool_mapping: dict[str, Callable] = tool_mapping\n",
    "                                   ) -> list[dict]:\n",
    "    \"\"\"Extract all function calls from the response, look up the corresponding tool function(s) and execute them.\n",
    "    (This would be a good place to handle asynchroneous tool calls, or ones that take a while to execute.)\n",
    "    This returns a list of messages to be added to the conversation history.\n",
    "    \"\"\"\n",
    "    intermediate_messages = []\n",
    "    for response_item in response.output:\n",
    "        if response_item.type == 'function_call':\n",
    "            target_tool = tool_mapping.get(response_item.name)\n",
    "            if target_tool:\n",
    "                try:\n",
    "                    arguments = json.loads(response_item.arguments)\n",
    "                    print(f\"Invoking tool: {response_item.name}({arguments})\")\n",
    "                    tool_output = target_tool(**arguments)\n",
    "                except Exception as e:\n",
    "                    msg = f\"Error executing function call: {response_item.name}: {e}\"\n",
    "                    tool_output = msg\n",
    "                    print(msg)\n",
    "            else:\n",
    "                msg = f\"ERROR - No tool registered for function call: {response_item.name}\"\n",
    "                tool_output = msg\n",
    "                print(msg)\n",
    "            intermediate_messages.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": response_item.call_id,\n",
    "                \"output\": tool_output\n",
    "            })\n",
    "        elif response_item.type == 'reasoning':\n",
    "            print(f'Reasoning step: {response_item.summary}')\n",
    "    return intermediate_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、先ほど説明したループの概念を実演してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning step: []\n",
      "Invoking tool: get_city_uuid({'city': 'Beijing'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: get_city_uuid({'city': 'London'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: get_city_uuid({'city': 'Rio de Janeiro'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: get_city_uuid({'city': 'Tokyo'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: get_city_uuid({'city': 'Paris'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: get_city_uuid({'city': 'Turin'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: get_city_uuid({'city': 'Vancouver'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: get_city_uuid({'city': 'Sochi'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: get_city_uuid({'city': 'Pyeongchang'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: web_search({'query': '2025 Beijing Olympics news'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: web_search({'query': '2025 London Olympics news'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: web_search({'query': '2025 Rio de Janeiro Olympics news'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: web_search({'query': '2025 Tokyo Olympics news'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: web_search({'query': '2025 Paris Olympics news'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: web_search({'query': '2025 Turin Olympics news'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: web_search({'query': '2025 Vancouver Olympics news'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: [Summary(text='**Focusing on Olympic News**\\n\\nI need to clarify that the Invictus Games are not related to the Olympics, so I should exclude them from my search. That leaves me with Olympic-specific news focusing on Paris. I also want to consider past events, like Sochi and Pyeongchang, so I think it makes sense to search for news related to Sochi as well. Let’s focus on gathering relevant Olympic updates to keep things organized.', type='summary_text')]\n",
      "Invoking tool: web_search({'query': '2025 Sochi Olympics news'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Invoking tool: web_search({'query': '2025 Pyeongchang Olympics news'})\n",
      "More reasoning required, continuing...\n",
      "Reasoning step: []\n",
      "Here are the internal IDs for all cities that have hosted Olympic Games in the last 20 years (2005–2025), along with those cities that have notable 2025 news stories specifically about the Olympics:\n",
      "\n",
      "1. Beijing (2008 Summer; 2022 Winter)  \n",
      "   • UUID: 5b058554-7253-4d9d-a434-5d4ccc87c78b  \n",
      "   • 2025 Olympic News? No major Olympic-specific news in 2025\n",
      "\n",
      "2. London (2012 Summer)  \n",
      "   • UUID: 9a67392d-c319-4598-b69a-adc5ffdaaba2  \n",
      "   • 2025 Olympic News? No\n",
      "\n",
      "3. Rio de Janeiro (2016 Summer)  \n",
      "   • UUID: ad5eaaae-b280-4c1d-9360-3a38b0c348c3  \n",
      "   • 2025 Olympic News? No\n",
      "\n",
      "4. Tokyo (2020 Summer)  \n",
      "   • UUID: 66c3a62a-840c-417a-8fad-ce87b97bb6a3  \n",
      "   • 2025 Olympic News? No\n",
      "\n",
      "5. Paris (2024 Summer)  \n",
      "   • UUID: a2da124e-3fad-402b-8ccf-173f63b4ff68  \n",
      "   • 2025 Olympic News? Yes  \n",
      "     – Olympic cauldron balloon to float annually over Paris into 2028 ([AP News])  \n",
      "     – IOC to replace defective Paris 2024 medals ([NDTV Sports])  \n",
      "     – IOC elects Kirsty Coventry as president at March 2025 session ([Wikipedia])  \n",
      "     – MLB cancels its planned 2025 Paris regular-season games ([AP News])\n",
      "\n",
      "6. Turin (2006 Winter)  \n",
      "   • UUID: 3674750b-6b76-49dc-adf4-d4393fa7bcfa  \n",
      "   • 2025 Olympic News? No (Host of Special Olympics World Winter Games, but not mainline Olympics)\n",
      "\n",
      "7. Vancouver (2010 Winter)  \n",
      "   • UUID: 22517787-5915-41c8-b9dd-a19aa2953210  \n",
      "   • 2025 Olympic News? No\n",
      "\n",
      "8. Sochi (2014 Winter)  \n",
      "   • UUID: f7efa267-c7da-4cdc-a14f-a4844f47b888  \n",
      "   • 2025 Olympic News? No\n",
      "\n",
      "9. Pyeongchang (2018 Winter)  \n",
      "   • UUID: ffb19c03-5212-42a9-a527-315d35efc5fc  \n",
      "   • 2025 Olympic News? No\n",
      "\n",
      "Summary of cities with 2025 Olympic-related news:  \n",
      "• Paris (a2da124e-3fad-402b-8ccf-173f63b4ff68)\n"
     ]
    }
   ],
   "source": [
    "initial_question = (\n",
    "    \"What are the internal IDs for the cities that have hosted the Olympics in the last 20 years, \"\n",
    "    \"and which of those cities have recent news stories (in 2025) about the Olympics? \"\n",
    "    \"Use your internal tools to look up the IDs and the web search tool to find the news stories.\"\n",
    ")\n",
    "\n",
    "# We fetch a response and then kick off a loop to handle the response\n",
    "response = client.responses.create(\n",
    "    input=initial_question,\n",
    "    **MODEL_DEFAULTS,\n",
    ")\n",
    "while True:   \n",
    "    function_responses = invoke_functions_from_response(response)\n",
    "    if len(function_responses) == 0: # We're done reasoning\n",
    "        print(response.output_text)\n",
    "        break\n",
    "    else:\n",
    "        print(\"More reasoning required, continuing...\")\n",
    "        response = client.responses.create(\n",
    "            input=function_responses,\n",
    "            previous_response_id=response.id,\n",
    "            **MODEL_DEFAULTS\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手動会話オーケストレーション\n",
    "ここまで順調です！モデルが実行を一時停止して関数を実行してから続行する様子を見るのは本当にクールです。\n",
    "実際には、上記の例はかなり単純で、本番環境のユースケースはもっと複雑になる可能性があります：\n",
    "* コンテキストウィンドウが大きくなりすぎて、古くて関連性の低いメッセージを削除したり、これまでの会話を要約したりしたい場合があります\n",
    "* ユーザーが会話を前後に移動して回答を再生成できるようにしたい場合があります\n",
    "* 監査目的でOpenAIのストレージとオーケストレーションに依存するのではなく、独自のデータベースにメッセージを保存したい場合があります\n",
    "* など\n",
    "\n",
    "このような状況では、会話を完全にコントロールしたい場合があります。`previous_message_id`を使用する代わりに、APIを「ステートレス」として扱い、毎回モデルに入力として送信する会話アイテムの配列を作成・維持することができます。\n",
    "\n",
    "これにはReasoningモデル特有の考慮すべき微妙な点があります。\n",
    "* 特に、会話履歴において推論と関数呼び出しの応答を保持することが不可欠です。\n",
    "* これにより、モデルはどのような思考の連鎖ステップを実行したかを追跡できます。これらが含まれていない場合、APIはエラーを返します。\n",
    "\n",
    "上記の例を再度実行し、メッセージを自分たちでオーケストレーションし、トークン使用量を追跡してみましょう。\n",
    "\n",
    "---\n",
    "*以下のコードは読みやすさのために構造化されています - 実際には、エッジケースを処理するためのより洗練されたワークフローを検討することをお勧めします*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "User message: Of those cities that have hosted the summer Olympic games in the last 20 years - do any of them have IDs beginning with a number and a temperate climate? Use your available tools to look up the IDs for each city and make sure to search the web to find out about the climate.\n",
      "*******************************************************************************\n",
      "More reasoning required, continuing...\n",
      "**Clarifying Olympic Cities**\n",
      "\n",
      "The user is asking about cities that hosted the Summer Olympics in the last 20 years. The relevant years to consider are 2004 Athens, 2008 Beijing, 2012 London, 2016 Rio de Janeiro, and 2020 Tokyo. If we're considering 2025, then 2004 would actually be 21 years ago, so I should focus instead on the years from 2005 onwards. Therefore, the cities to include are Beijing, London, Rio, and Tokyo. I’ll exclude Paris since it hasn’t hosted yet.\n",
      "Reasoning step: [Summary(text=\"**Clarifying Olympic Cities**\\n\\nThe user is asking about cities that hosted the Summer Olympics in the last 20 years. The relevant years to consider are 2004 Athens, 2008 Beijing, 2012 London, 2016 Rio de Janeiro, and 2020 Tokyo. If we're considering 2025, then 2004 would actually be 21 years ago, so I should focus instead on the years from 2005 onwards. Therefore, the cities to include are Beijing, London, Rio, and Tokyo. I’ll exclude Paris since it hasn’t hosted yet.\", type='summary_text')]\n",
      "Invoking tool: get_city_uuid({'city': 'Beijing'})\n",
      "Invoking tool: get_city_uuid({'city': 'London'})\n",
      "Invoking tool: get_city_uuid({'city': 'Rio de Janeiro'})\n",
      "Invoking tool: get_city_uuid({'city': 'Tokyo'})\n",
      "More reasoning required, continuing...\n",
      "\n",
      "Reasoning step: []\n",
      "Invoking tool: web_search({'query': 'London climate'})\n",
      "Invoking tool: web_search({'query': 'Tokyo climate'})\n",
      "More reasoning required, continuing...\n",
      "\n",
      "I looked up the internal IDs and climates for each Summer-Olympics host of the last 20 years:\n",
      "\n",
      "• Beijing  \n",
      "  – ID: 937b336d-2708-4ad3-8c2f-85ea32057e1e (starts with “9”)  \n",
      "  – Climate: humid continental (cold winters, hot summers) → not temperate\n",
      "\n",
      "• London  \n",
      "  – ID: ee57f35a-7d1b-4888-8833-4ace308fa004 (starts with “e”)  \n",
      "  – Climate: temperate oceanic (mild, moderate rainfall)\n",
      "\n",
      "• Rio de Janeiro  \n",
      "  – ID: 2a70c45e-a5b4-4e42-8d2b-6c1dbb2aa2d9 (starts with “2”)  \n",
      "  – Climate: tropical (hot/wet)\n",
      "\n",
      "• Tokyo  \n",
      "  – ID: e5de3686-a7d2-42b8-aca5-6b6e436083ff (starts with “e”)  \n",
      "  – Climate: humid subtropical (hot, humid summers; mild winters)\n",
      "\n",
      "The only IDs that begin with a numeral are Beijing (“9…”) and Rio (“2…”), but neither city has a temperate climate. Therefore, none of the last-20-years hosts combine an ID starting with a number with a temperate climate.\n",
      "*******************************************************************************\n",
      "User message: Great thanks! We've just updated the IDs - could you please check again?\n",
      "*******************************************************************************\n",
      "More reasoning required, continuing...\n",
      "\n",
      "Reasoning step: []\n",
      "Invoking tool: get_city_uuid({'city': 'Beijing'})\n",
      "Invoking tool: get_city_uuid({'city': 'London'})\n",
      "Invoking tool: get_city_uuid({'city': 'Rio de Janeiro'})\n",
      "Invoking tool: get_city_uuid({'city': 'Tokyo'})\n",
      "Here are the updated IDs along with their climates:\n",
      "\n",
      "• Beijing  \n",
      "  – ID: 8819a1fd-a958-40e6-8ba7-9f450b40fb13 (starts with “8”)  \n",
      "  – Climate: humid continental → not temperate\n",
      "\n",
      "• London  \n",
      "  – ID: 50866ef9-6505-4939-90e7-e8b930815782 (starts with “5”)  \n",
      "  – Climate: temperate oceanic\n",
      "\n",
      "• Rio de Janeiro  \n",
      "  – ID: 5bc1b2de-75da-4689-8bff-269e60af32cb (starts with “5”)  \n",
      "  – Climate: tropical → not temperate\n",
      "\n",
      "• Tokyo  \n",
      "  – ID: 9d1c920e-e725-423e-b83c-ec7d97f2e79f (starts with “9”)  \n",
      "  – Climate: humid subtropical → not temperate\n",
      "\n",
      "Of these, the only city with a temperate climate is London, but its ID begins with “5” (a number) – so it does meet “ID beginning with a number AND temperate climate.” \n",
      "Total tokens used: 17154 (8.58% of o4-mini's context window)\n"
     ]
    }
   ],
   "source": [
    "# Let's initialise our conversation with the first user message\n",
    "total_tokens_used = 0\n",
    "user_messages = [\n",
    "    (\n",
    "        \"Of those cities that have hosted the summer Olympic games in the last 20 years - \"\n",
    "        \"do any of them have IDs beginning with a number and a temperate climate? \"\n",
    "        \"Use your available tools to look up the IDs for each city and make sure to search the web to find out about the climate.\"\n",
    "    ),\n",
    "    \"Great thanks! We've just updated the IDs - could you please check again?\"\n",
    "    ]\n",
    "\n",
    "conversation = []\n",
    "for message in user_messages:\n",
    "    conversation_item = {\n",
    "        \"role\": \"user\",\n",
    "        \"type\": \"message\",\n",
    "        \"content\": message\n",
    "    }\n",
    "    print(f\"{'*' * 79}\\nUser message: {message}\\n{'*' * 79}\")\n",
    "    conversation.append(conversation_item)\n",
    "    while True: # Response loop\n",
    "        response = client.responses.create(\n",
    "            input=conversation,\n",
    "            **MODEL_DEFAULTS\n",
    "        )\n",
    "        total_tokens_used += response.usage.total_tokens\n",
    "        reasoning = [rx.to_dict() for rx in response.output if rx.type == 'reasoning']\n",
    "        function_calls = [rx.to_dict() for rx in response.output if rx.type == 'function_call']\n",
    "        messages = [rx.to_dict() for rx in response.output if rx.type == 'message']\n",
    "        if len(reasoning) > 0:\n",
    "            print(\"More reasoning required, continuing...\")\n",
    "            # Ensure we capture any reasoning steps\n",
    "            conversation.extend(reasoning)\n",
    "            print('\\n'.join(s['text'] for r in reasoning for s in r['summary']))\n",
    "        if len(function_calls) > 0:\n",
    "            function_outputs = invoke_functions_from_response(response)\n",
    "            # Preserve order of function calls and outputs in case of multiple function calls (currently not supported by reasoning models, but worth considering)\n",
    "            interleaved = [val for pair in zip(function_calls, function_outputs) for val in pair]\n",
    "            conversation.extend(interleaved)\n",
    "        if len(messages) > 0:\n",
    "            print(response.output_text)\n",
    "            conversation.extend(messages)\n",
    "        if len(function_calls) == 0:  # No more functions = We're done reasoning and we're ready for the next user message\n",
    "            break\n",
    "print(f\"Total tokens used: {total_tokens_used} ({total_tokens_used / 200_000:.2%} of o4-mini's context window)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要\n",
    "このクックブックでは、OpenAIの推論モデルと関数呼び出しを組み合わせて、ウェブ検索を含む外部データソースに依存する複数ステップのタスクを実行する方法を特定しました。\n",
    "\n",
    "重要なことに、関数呼び出しプロセスにおける推論モデル特有のニュアンスを取り上げました。具体的には：\n",
    "* モデルは複数の関数呼び出しや推論ステップを連続して実行することを選択する場合があり、一部のステップは前のステップの結果に依存する可能性がある\n",
    "* これらのステップがいくつになるかを事前に知ることはできないため、ループを使用してレスポンスを処理する必要がある\n",
    "* Responses APIは`previous_response_id`パラメータを使用してオーケストレーションを簡単にしますが、手動制御が必要な場合は、「思考の連鎖」を保持するために会話アイテムの正しい順序を維持することが重要\n",
    "\n",
    "---\n",
    "\n",
    "ここで使用した例はかなりシンプルですが、この技術をより実世界のユースケースに拡張できることを想像できるでしょう。例えば：\n",
    "\n",
    "* 顧客の取引履歴と最近の対応記録を調べて、プロモーションオファーの対象かどうかを判断する\n",
    "* 最近の取引ログ、位置情報データ、デバイスメタデータを呼び出して、取引が不正である可能性を評価する\n",
    "* 内部HRデータベースを確認して、従業員の福利厚生利用状況、勤続年数、最近のポリシー変更を取得し、個人向けのHR質問に回答する\n",
    "* 内部ダッシュボード、競合他社のニュースフィード、市場分析を読み取って、経営陣の重点分野に合わせた日次エグゼクティブブリーフィングを作成する"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
