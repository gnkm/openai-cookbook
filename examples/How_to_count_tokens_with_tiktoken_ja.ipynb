{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tiktokenを使ったトークン数の計算方法\n",
    "\n",
    "[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md)は、OpenAIが開発した高速なオープンソースのトークナイザーです。\n",
    "\n",
    "テキスト文字列（例：`\"tiktoken is great!\"`）とエンコーディング（例：`\"cl100k_base\"`）が与えられると、トークナイザーはテキスト文字列をトークンのリスト（例：`[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]`）に分割できます。\n",
    "\n",
    "テキスト文字列をトークンに分割することは、GPTモデルがテキストをトークンの形で認識するため有用です。テキスト文字列に含まれるトークン数を知ることで、(a) その文字列がテキストモデルで処理するには長すぎるかどうか、(b) OpenAI APIの呼び出しにかかるコスト（使用量はトークン単位で課金されるため）を把握できます。\n",
    "\n",
    "## エンコーディング\n",
    "\n",
    "エンコーディングは、テキストがトークンに変換される方法を指定します。異なるモデルは異なるエンコーディングを使用します。\n",
    "\n",
    "`tiktoken`は、OpenAIモデルで使用される3つのエンコーディングをサポートしています：\n",
    "\n",
    "| エンコーディング名        | OpenAIモデル                                         |\n",
    "|-------------------------|-----------------------------------------------------|\n",
    "| `o200k_base`            | `gpt-4o`, `gpt-4o-mini`                             |\n",
    "| `cl100k_base`           | `gpt-4-turbo`, `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`, `text-embedding-3-small`, `text-embedding-3-large`  |\n",
    "| `p50k_base`             | Codexモデル, `text-davinci-002`, `text-davinci-003`|\n",
    "| `r50k_base` (または `gpt2`) | `davinci`などのGPT-3モデル                         |\n",
    "\n",
    "以下のように`tiktoken.encoding_for_model()`を使用してモデルのエンコーディングを取得できます：\n",
    "```python\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "```\n",
    "\n",
    "`p50k_base`は`r50k_base`と大幅に重複しており、コード以外のアプリケーションでは通常同じトークンが生成されることに注意してください。\n",
    "\n",
    "## 言語別のトークナイザーライブラリ\n",
    "\n",
    "`o200k_base`、`cl100k_base`、`p50k_base`エンコーディング用：\n",
    "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\n",
    "- .NET / C#: [SharpToken](https://github.com/dmitry-brazhenko/SharpToken), [TiktokenSharp](https://github.com/aiqinxuancai/TiktokenSharp)\n",
    "- Java: [jtokkit](https://github.com/knuddelsgmbh/jtokkit)\n",
    "- Golang: [tiktoken-go](https://github.com/pkoukk/tiktoken-go)\n",
    "- Rust: [tiktoken-rs](https://github.com/zurawiki/tiktoken-rs)\n",
    "\n",
    "`r50k_base`（`gpt2`）エンコーディング用のトークナイザーは、多くの言語で利用可能です。\n",
    "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)（または代替として[GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast)）\n",
    "- JavaScript: [gpt-3-encoder](https://www.npmjs.com/package/gpt-3-encoder)\n",
    "- .NET / C#: [GPT Tokenizer](https://github.com/dluc/openai-tools)\n",
    "- Java: [gpt2-tokenizer-java](https://github.com/hyunwoongko/gpt2-tokenizer-java)\n",
    "- PHP: [GPT-3-Encoder-PHP](https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP)\n",
    "- Golang: [tiktoken-go](https://github.com/pkoukk/tiktoken-go)\n",
    "- Rust: [tiktoken-rs](https://github.com/zurawiki/tiktoken-rs)\n",
    "\n",
    "（OpenAIはサードパーティライブラリの推奨や保証は行いません。）\n",
    "\n",
    "## 文字列の一般的なトークン化方法\n",
    "\n",
    "英語では、トークンの長さは通常1文字から1単語の範囲（例：`\"t\"`や`\" great\"`）ですが、一部の言語では1文字より短いトークンや1単語より長いトークンもあります。スペースは通常、単語の開始部分とグループ化されます（例：`\"is \"`や`\" \"`+`\"is\"`ではなく`\" is\"`）。文字列がどのようにトークン化されるかは、[OpenAI Tokenizer](https://beta.openai.com/tokenizer)やサードパーティの[Tiktokenizer](https://tiktokenizer.vercel.app/) Webアプリで素早く確認できます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. `tiktoken`をインストール\n",
    "\n",
    "必要に応じて、`pip`を使用して`tiktoken`をインストールしてください："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tiktoken -q\n",
    "%pip install --upgrade openai -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `tiktoken`をインポートする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. エンコーディングの読み込み\n",
    "\n",
    "`tiktoken.get_encoding()`を使用して、名前でエンコーディングを読み込みます。\n",
    "\n",
    "初回実行時は、ダウンロードのためにインターネット接続が必要です。その後の実行では、インターネット接続は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定されたモデル名に対して正しいエンコーディングを自動的に読み込むには、`tiktoken.encoding_for_model()`を使用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `encoding.encode()`でテキストをトークンに変換する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.encode()`メソッドは、テキスト文字列をトークン整数のリストに変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83, 8251, 2488, 382, 2212, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode(\"tiktoken is great!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.encode()`によって返されるリストの長さを数えることで、トークン数をカウントします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"tiktoken is great!\", \"o200k_base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `encoding.decode()`でトークンをテキストに変換する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.decode()`は、トークンの整数のリストを文字列に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken is great!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([83, 8251, 2488, 382, 2212, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "警告：`.decode()`は単一のトークンに適用できますが、utf-8境界上にないトークンに対しては情報が失われる可能性があることに注意してください。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単一のトークンに対しては、`.decode_single_token_bytes()`が単一の整数トークンをそれが表すバイト列に安全に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b't', b'ikt', b'oken', b' is', b' great', b'!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding.decode_single_token_bytes(token) for token in [83, 8251, 2488, 382, 2212, 0]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（文字列の前にある `b` は、その文字列がバイト文字列であることを示しています。）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. エンコーディングの比較\n",
    "\n",
    "異なるエンコーディングは、単語の分割方法、スペースのグループ化、非英語文字の処理方法が異なります。上記の方法を使用して、いくつかの例文字列で異なるエンコーディングを比較することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_encodings(example_string: str) -> None:\n",
    "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
    "    # print the example string\n",
    "    print(f'\\nExample string: \"{example_string}\"')\n",
    "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
    "    for encoding_name in [\"r50k_base\", \"p50k_base\", \"cl100k_base\", \"o200k_base\"]:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        token_integers = encoding.encode(example_string)\n",
    "        num_tokens = len(token_integers)\n",
    "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
    "        print()\n",
    "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
    "        print(f\"token integers: {token_integers}\")\n",
    "        print(f\"token bytes: {token_bytes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"antidisestablishmentarianism\"\n",
      "\n",
      "r50k_base: 5 tokens\n",
      "token integers: [415, 29207, 44390, 3699, 1042]\n",
      "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
      "\n",
      "p50k_base: 5 tokens\n",
      "token integers: [415, 29207, 44390, 3699, 1042]\n",
      "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
      "\n",
      "cl100k_base: 6 tokens\n",
      "token integers: [519, 85342, 34500, 479, 8997, 2191]\n",
      "token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n",
      "\n",
      "o200k_base: 6 tokens\n",
      "token integers: [493, 129901, 376, 160388, 21203, 2367]\n",
      "token bytes: [b'ant', b'idis', b'est', b'ablishment', b'arian', b'ism']\n"
     ]
    }
   ],
   "source": [
    "compare_encodings(\"antidisestablishmentarianism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"2 + 2 = 4\"\n",
      "\n",
      "r50k_base: 5 tokens\n",
      "token integers: [17, 1343, 362, 796, 604]\n",
      "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
      "\n",
      "p50k_base: 5 tokens\n",
      "token integers: [17, 1343, 362, 796, 604]\n",
      "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
      "\n",
      "cl100k_base: 7 tokens\n",
      "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
      "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n",
      "\n",
      "o200k_base: 7 tokens\n",
      "token integers: [17, 659, 220, 17, 314, 220, 19]\n",
      "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
     ]
    }
   ],
   "source": [
    "compare_encodings(\"2 + 2 = 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"お誕生日おめでとう\"\n",
      "\n",
      "r50k_base: 14 tokens\n",
      "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "p50k_base: 14 tokens\n",
      "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "cl100k_base: 9 tokens\n",
      "token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\n",
      "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n",
      "\n",
      "o200k_base: 8 tokens\n",
      "token integers: [8930, 9697, 243, 128225, 8930, 17693, 4344, 48669]\n",
      "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n"
     ]
    }
   ],
   "source": [
    "compare_encodings(\"お誕生日おめでとう\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. チャット補完APIコールのトークン数カウント\n",
    "\n",
    "`gpt-4o-mini`や`gpt-4`などのChatGPTモデルは、従来の補完モデルと同じ方法でトークンを使用しますが、メッセージベースの形式のため、会話で使用されるトークン数を数えることがより困難になります。\n",
    "\n",
    "以下は、`gpt-3.5-turbo`、`gpt-4`、`gpt-4o`、`gpt-4o-mini`に渡されるメッセージのトークン数をカウントする関数の例です。\n",
    "\n",
    "メッセージからトークンがカウントされる正確な方法は、モデルによって変わる可能性があることに注意してください。以下の関数からのカウントは推定値として考え、永続的な保証ではないことを理解してください。\n",
    "\n",
    "特に、オプションのfunctions入力を使用するリクエストは、以下で計算される推定値に加えて、追加のトークンを消費します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using o200k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0125\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        \"gpt-4o-mini-2024-07-18\",\n",
    "        \"gpt-4o-2024-08-06\"\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0125\")\n",
    "    elif \"gpt-4o-mini\" in model:\n",
    "        print(\"Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\")\n",
    "    elif \"gpt-4o\" in model:\n",
    "        print(\"Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4o-2024-08-06\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n",
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4-0613\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4\n",
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4o\n",
      "Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\n",
      "124 prompt tokens counted by num_tokens_from_messages().\n",
      "124 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4o-mini\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "124 prompt tokens counted by num_tokens_from_messages().\n",
      "124 prompt tokens counted by the OpenAI API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's verify the function above matches the OpenAI API response\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for model in [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"gpt-4\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\"\n",
    "    ]:\n",
    "    print(model)\n",
    "    # example token count from the function defined above\n",
    "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
    "    # example token count from the OpenAI API\n",
    "    response = client.chat.completions.create(model=model,\n",
    "    messages=example_messages,\n",
    "    temperature=0,\n",
    "    max_tokens=1)\n",
    "    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ツール呼び出しを含むチャット補完のトークン数計算\n",
    "\n",
    "次に、関数呼び出しを含む可能性のあるメッセージに対してこの計算を適用する方法を見ていきます。ツール自体のフォーマットにより、これは直ちには簡単ではありません。\n",
    "\n",
    "以下は、`gpt-3.5-turbo`、`gpt-4`、`gpt-4o`、`gpt-4o-mini`に渡されるツールを含むメッセージのトークン数を計算するサンプル関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_for_tools(functions, messages, model):\n",
    "    \n",
    "    # Initialize function settings to 0\n",
    "    func_init = 0\n",
    "    prop_init = 0\n",
    "    prop_key = 0\n",
    "    enum_init = 0\n",
    "    enum_item = 0\n",
    "    func_end = 0\n",
    "    \n",
    "    if model in [\n",
    "        \"gpt-4o\",\n",
    "        \"gpt-4o-mini\"\n",
    "    ]:\n",
    "        \n",
    "        # Set function settings for the above models\n",
    "        func_init = 7\n",
    "        prop_init = 3\n",
    "        prop_key = 3\n",
    "        enum_init = -3\n",
    "        enum_item = 3\n",
    "        func_end = 12\n",
    "    elif model in [\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-4\"\n",
    "    ]:\n",
    "        # Set function settings for the above models\n",
    "        func_init = 10\n",
    "        prop_init = 3\n",
    "        prop_key = 3\n",
    "        enum_init = -3\n",
    "        enum_item = 3\n",
    "        func_end = 12\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_for_tools() is not implemented for model {model}.\"\"\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using o200k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "    \n",
    "    func_token_count = 0\n",
    "    if len(functions) > 0:\n",
    "        for f in functions:\n",
    "            func_token_count += func_init  # Add tokens for start of each function\n",
    "            function = f[\"function\"]\n",
    "            f_name = function[\"name\"]\n",
    "            f_desc = function[\"description\"]\n",
    "            if f_desc.endswith(\".\"):\n",
    "                f_desc = f_desc[:-1]\n",
    "            line = f_name + \":\" + f_desc\n",
    "            func_token_count += len(encoding.encode(line))  # Add tokens for set name and description\n",
    "            if len(function[\"parameters\"][\"properties\"]) > 0:\n",
    "                func_token_count += prop_init  # Add tokens for start of each property\n",
    "                for key in list(function[\"parameters\"][\"properties\"].keys()):\n",
    "                    func_token_count += prop_key  # Add tokens for each set property\n",
    "                    p_name = key\n",
    "                    p_type = function[\"parameters\"][\"properties\"][key][\"type\"]\n",
    "                    p_desc = function[\"parameters\"][\"properties\"][key][\"description\"]\n",
    "                    if \"enum\" in function[\"parameters\"][\"properties\"][key].keys():\n",
    "                        func_token_count += enum_init  # Add tokens if property has enum list\n",
    "                        for item in function[\"parameters\"][\"properties\"][key][\"enum\"]:\n",
    "                            func_token_count += enum_item\n",
    "                            func_token_count += len(encoding.encode(item))\n",
    "                    if p_desc.endswith(\".\"):\n",
    "                        p_desc = p_desc[:-1]\n",
    "                    line = f\"{p_name}:{p_type}:{p_desc}\"\n",
    "                    func_token_count += len(encoding.encode(line))\n",
    "        func_token_count += func_end\n",
    "        \n",
    "    messages_token_count = num_tokens_from_messages(messages, model)\n",
    "    total_tokens = messages_token_count + func_token_count\n",
    "    \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo\n",
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\n",
      "105 prompt tokens counted by num_tokens_for_tools().\n",
      "105 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4\n",
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "105 prompt tokens counted by num_tokens_for_tools().\n",
      "105 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4o\n",
      "Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\n",
      "101 prompt tokens counted by num_tokens_for_tools().\n",
      "101 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4o-mini\n",
      "Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\n",
      "101 prompt tokens counted by num_tokens_for_tools().\n",
      "101 prompt tokens counted by the OpenAI API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tools = [\n",
    "  {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"get_current_weather\",\n",
    "      \"description\": \"Get the current weather in a given location\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "          },\n",
    "          \"unit\": {\"type\": \"string\", \n",
    "                   \"description\": \"The unit of temperature to return\",\n",
    "                   \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "      },\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that can answer to questions about the weather.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in San Francisco?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for model in [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\"\n",
    "    ]:\n",
    "    print(model)\n",
    "    # example token count from the function defined above\n",
    "    print(f\"{num_tokens_for_tools(tools, example_messages, model)} prompt tokens counted by num_tokens_for_tools().\")\n",
    "    # example token count from the OpenAI API\n",
    "    response = client.chat.completions.create(model=model,\n",
    "          messages=example_messages,\n",
    "          tools=tools,\n",
    "          temperature=0)\n",
    "    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
