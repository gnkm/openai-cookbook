{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dfbaf53-32de-4b8c-bd1c-d27371a87f81",
   "metadata": {},
   "source": [
    "# Responses APIでのファイル検索ツールの使用\n",
    "\n",
    "RAGは複雑になりがちですが、PDFファイル間での検索は複雑である必要はありません。現在最も採用されている選択肢の一つは、PDFを解析し、チャンク化戦略を定義し、それらのチャンクをストレージプロバイダーにアップロードし、それらのテキストチャンクに対して埋め込みを実行し、それらの埋め込みをベクターデータベースに保存することです。そして、それはセットアップのみです — LLMワークフローでコンテンツを取得することも複数のステップが必要です。\n",
    "\n",
    "ここで、Responses APIで使用できるホスト型ツールであるファイル検索が登場します。これにより、ナレッジベースを検索し、取得したコンテンツに基づいて回答を生成することができます。このクックブックでは、これらのPDFをOpenAIのベクターストアにアップロードし、ファイル検索を使用してこのベクターストアから追加のコンテキストを取得し、最初のステップで生成した質問に答えます。その後、OpenAIのブログ（[openai.com/news](https://openai.com/news)）から抽出したPDFに基づいて、最初に小さな質問セットを作成します。\n",
    "\n",
    "_ファイル検索は以前Assistants APIで利用可能でした。現在は新しいResponses APIで利用可能になっており、このAPIはステートフルまたはステートレスにでき、メタデータフィルタリングなどの新機能も備えています_\n",
    "\n",
    "# PDFを使用したベクターストアの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47480955-9dd4-4837-8b4c-6821bb48306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2 pandas tqdm openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6feaf3e-a2be-4c74-bad5-0c37bbe110b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import concurrent\n",
    "import PyPDF2\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "dir_pdfs = 'openai_blog_pdfs' # have those PDFs stored locally here\n",
    "pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5cb9c-fc99-45e2-bd79-9c9ba5b410cc",
   "metadata": {},
   "source": [
    "OpenAI APIでVector Storeを作成し、PDFをVector Storeにアップロードします。OpenAIはそれらのPDFを読み取り、コンテンツを複数のテキストチャンクに分割し、それらに対してembeddingを実行し、そのembeddingとテキストをVector Storeに保存します。これにより、クエリに基づいて関連するコンテンツを返すためにこのVector Storeにクエリを実行できるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6823030-9110-4143-ab7c-a223182eb7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_single_pdf(file_path: str, vector_store_id: str):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        file_response = client.files.create(file=open(file_path, 'rb'), purpose=\"assistants\")\n",
    "        attach_response = client.vector_stores.files.create(\n",
    "            vector_store_id=vector_store_id,\n",
    "            file_id=file_response.id\n",
    "        )\n",
    "        return {\"file\": file_name, \"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {file_name}: {str(e)}\")\n",
    "        return {\"file\": file_name, \"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "def upload_pdf_files_to_vector_store(vector_store_id: str):\n",
    "    pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]\n",
    "    stats = {\"total_files\": len(pdf_files), \"successful_uploads\": 0, \"failed_uploads\": 0, \"errors\": []}\n",
    "    \n",
    "    print(f\"{len(pdf_files)} PDF files to process. Uploading in parallel...\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(upload_single_pdf, file_path, vector_store_id): file_path for file_path in pdf_files}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(pdf_files)):\n",
    "            result = future.result()\n",
    "            if result[\"status\"] == \"success\":\n",
    "                stats[\"successful_uploads\"] += 1\n",
    "            else:\n",
    "                stats[\"failed_uploads\"] += 1\n",
    "                stats[\"errors\"].append(result)\n",
    "\n",
    "    return stats\n",
    "\n",
    "def create_vector_store(store_name: str) -> dict:\n",
    "    try:\n",
    "        vector_store = client.vector_stores.create(name=store_name)\n",
    "        details = {\n",
    "            \"id\": vector_store.id,\n",
    "            \"name\": vector_store.name,\n",
    "            \"created_at\": vector_store.created_at,\n",
    "            \"file_count\": vector_store.file_counts.completed\n",
    "        }\n",
    "        print(\"Vector store created:\", details)\n",
    "        return details\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb6cba0-931e-426a-88aa-34a62cc7158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created: {'id': 'vs_67d06b9b9a9c8191bafd456cf2364ce3', 'name': 'openai_blog_store', 'created_at': 1741712283, 'file_count': 0}\n",
      "21 PDF files to process. Uploading in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 21/21 [00:09<00:00,  2.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_files': 21,\n",
       " 'successful_uploads': 21,\n",
       " 'failed_uploads': 0,\n",
       " 'errors': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_name = \"openai_blog_store\"\n",
    "vector_store_details = create_vector_store(store_name)\n",
    "upload_pdf_files_to_vector_store(vector_store_details[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4ade3-2b3e-4df6-a441-c1ee3ea73172",
   "metadata": {},
   "source": [
    "# スタンドアロンベクター検索\n",
    "\n",
    "ベクターストアの準備が完了したので、ベクターストアに直接クエリを実行し、特定のクエリに関連するコンテンツを取得できるようになりました。新しい[vector search API](https://platform.openai.com/docs/api-reference/vector-stores/search)を使用することで、LLMクエリに必ずしも統合することなく、ナレッジベースから関連するアイテムを見つけることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "980323d0-0112-4c9e-9b90-67719739026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's Deep Research?\"\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store_details['id'],\n",
    "    query=query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6045a2e-a75f-48c0-89f4-841ef722d24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3502 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.9813588865322393\n",
      "3493 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.9522476825143714\n",
      "3634 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.9397930296526796\n",
      "2774 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.9101975747303771\n",
      "3474 of character of content from Deep research System Card _ OpenAI.pdf with a relevant score of 0.9036647613464299\n",
      "3123 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.887120981288272\n",
      "3343 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.8448454849432881\n",
      "3262 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.791345286655509\n",
      "3271 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.7485530025091963\n",
      "2721 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.734033360849088\n"
     ]
    }
   ],
   "source": [
    "for result in search_results.data:\n",
    "    print(str(len(result.content[0].text)) + ' of character of content from ' + result.filename + ' with a relevant score of ' + str(result.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0b4ec-ea13-429a-a1b7-7bac3d2ea014",
   "metadata": {},
   "source": [
    "検索クエリから異なるサイズ（そして内部的には異なるテキスト）が返されていることがわかります。これらはすべて、ハイブリッド検索を使用するランカーによって計算された異なる関連性スコアを持っています。\n",
    "\n",
    "# 単一のAPI呼び出しでLLMと検索結果を統合する\n",
    "\n",
    "しかし、ベクトルストアにクエリを実行してからそのデータをResponsesまたはChat Completion API呼び出しに渡すのではなく、この検索結果をLLMクエリで使用するより便利な方法は、OpenAI Responses APIの一部として`file_search`ツールを使用することです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a153cb6e-e94b-4b55-a557-4f34fd3022bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files used: {'Introducing deep research _ OpenAI.pdf'}\n",
      "Response:\n",
      "Deep Research is a new capability introduced by OpenAI that allows users to conduct complex, multi-step research tasks on the internet efficiently. Key features include:\n",
      "\n",
      "1. **Autonomous Research**: Deep Research acts as an independent agent that synthesizes vast amounts of information across the web, enabling users to receive comprehensive reports similar to those produced by a research analyst.\n",
      "\n",
      "2. **Multi-Step Reasoning**: It performs deep analysis by finding, interpreting, and synthesizing data from various sources, including text, images, and PDFs.\n",
      "\n",
      "3. **Application Areas**: Especially useful for professionals in fields such as finance, science, policy, and engineering, as well as for consumers seeking detailed information for purchases.\n",
      "\n",
      "4. **Efficiency**: The output is fully documented with citations, making it easy to verify information, and it significantly speeds up research processes that would otherwise take hours for a human to complete.\n",
      "\n",
      "5. **Limitations**: While Deep Research enhances research capabilities, it is still subject to limitations, such as potential inaccuracies in information retrieval and challenges in distinguishing authoritative data from unreliable sources.\n",
      "\n",
      "Overall, Deep Research marks a significant advancement toward automated general intelligence (AGI) by improving access to thorough and precise research outputs.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's Deep Research?\"\n",
    "response = client.responses.create(\n",
    "    input= query,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store_details['id']],\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Extract annotations from the response\n",
    "annotations = response.output[1].content[0].annotations\n",
    "    \n",
    "# Get top-k retrieved filenames\n",
    "retrieved_files = set([result.filename for result in annotations])\n",
    "\n",
    "print(f'Files used: {retrieved_files}')\n",
    "print('Response:')\n",
    "print(response.output[1].content[0].text) # 0 being the filesearch call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c7b7b3-7d63-4630-95e7-76cf8080477e",
   "metadata": {},
   "source": [
    "`gpt-4o-mini`がOpenAIのDeep Researchに関するより最新で専門的な知識を必要とするクエリに回答できることがわかります。最も関連性の高いテキストのチャンクを含む`Introducing deep research _ OpenAI.pdf`ファイルのコンテンツを使用しました。取得されたテキストのチャンクをさらに深く分析したい場合は、クエリに`include=[\"output[*].file_search_call.search_results\"]`を追加することで、検索エンジンによって返された異なるテキストを分析することもできます。\n",
    "\n",
    "# パフォーマンスの評価\n",
    "\n",
    "これらの情報検索システムにとって重要なのは、回答のために取得されたファイルの関連性と品質を測定することです。このクックブックの以下のステップでは、評価データセットを生成し、この生成されたデータセットに対してさまざまなメトリクスを計算することで構成されます。これは不完全なアプローチであり、独自のユースケースには人間が検証した評価データセットを用意することを常に推奨しますが、これらを評価する方法論を示します。生成される質問の一部が汎用的（例：この文書で主要な関係者が述べていることは何か）である可能性があり、私たちの検索テストではその質問がどの文書のために生成されたかを把握するのが困難になるため、不完全になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93291578-d04a-4e71-8ecb-9f0f647e68c3",
   "metadata": {},
   "source": [
    "## 評価データの生成\n",
    "\n",
    "ローカルに保存されているPDFを読み取り、その文書でのみ回答可能な質問を生成する関数を作成します。これにより、後で使用できる評価データセットを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1274ce-a468-489a-9206-0ff6ba82e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def generate_questions(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    prompt = (\n",
    "        \"Can you generate a question that can only be answered from this document?:\\n\"\n",
    "        f\"{text}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    response = client.responses.create(\n",
    "        input=prompt,\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    question = response.output[0].content[0].text\n",
    "\n",
    "    return question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850d17f-832f-4a03-8216-5200d2db6b17",
   "metadata": {},
   "source": [
    "最初のPDFファイルに対して関数`generate_question`を実行すると、どのような質問が生成されるかを確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d19e4f5-a193-4787-aad1-8547173d36f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What new capabilities will ChatGPT have as a result of the partnership between OpenAI and Schibsted Media Group?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_questions(pdf_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e4e26-3396-4a3b-83a9-db9ae1597e41",
   "metadata": {},
   "source": [
    "これで、ローカルに保存されているすべてのPDFに対して、すべての質問を生成できるようになりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fec6e6c-13b6-4498-b49c-d20e28b39ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions for each PDF and store in a dictionary\n",
    "questions_dict = {}\n",
    "for pdf_path in pdf_files:\n",
    "    questions = generate_questions(pdf_path)\n",
    "    questions_dict[os.path.basename(pdf_path)] = questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e04371b-32ef-48f9-833a-84f53b7399fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OpenAI partners with Schibsted Media Group _ OpenAI.pdf': 'What is the purpose of the partnership between Schibsted Media Group and OpenAI announced on February 10, 2025?',\n",
       " 'OpenAI and the CSU system bring AI to 500,000 students & faculty _ OpenAI.pdf': 'What significant milestone did the California State University system achieve by partnering with OpenAI, making it the first of its kind in the United States?',\n",
       " '1,000 Scientist AI Jam Session _ OpenAI.pdf': 'What was the specific AI model used during the \"1,000 Scientist AI Jam Session\" event across the nine national labs?',\n",
       " 'Announcing The Stargate Project _ OpenAI.pdf': 'What are the initial equity funders and lead partners in The Stargate Project announced by OpenAI, and who holds the financial and operational responsibilities?',\n",
       " 'Introducing Operator _ OpenAI.pdf': 'What is the name of the new model that powers the Operator agent introduced by OpenAI?',\n",
       " 'Introducing NextGenAI _ OpenAI.pdf': 'What major initiative did OpenAI launch on March 4, 2025, and which research institution from Europe is involved as a founding partner?',\n",
       " 'Introducing the Intelligence Age _ OpenAI.pdf': \"What is the name of the video generation tool used by OpenAI's creative team to help produce their Super Bowl ad?\",\n",
       " 'Operator System Card _ OpenAI.pdf': 'What is the preparedness score for the \"Cybersecurity\" category according to the Operator System Card?',\n",
       " 'Strengthening America’s AI leadership with the U.S. National Laboratories _ OpenAI.pdf': \"What is the purpose of OpenAI's agreement with the U.S. National Laboratories as described in the document?\",\n",
       " 'OpenAI GPT-4.5 System Card _ OpenAI.pdf': 'What is the Preparedness Framework rating for \"Cybersecurity\" for GPT-4.5 according to the system card?',\n",
       " 'Partnering with Axios expands OpenAI’s work with the news industry _ OpenAI.pdf': \"What is the goal of OpenAI's new content partnership with Axios as announced in the document?\",\n",
       " 'OpenAI and Guardian Media Group launch content partnership _ OpenAI.pdf': 'What is the main purpose of the partnership between OpenAI and Guardian Media Group announced on February 14, 2025?',\n",
       " 'Introducing GPT-4.5 _ OpenAI.pdf': 'What is the release date of the GPT-4.5 research preview?',\n",
       " 'Introducing data residency in Europe _ OpenAI.pdf': 'What are the benefits of data residency in Europe for new ChatGPT Enterprise and Edu customers according to the document?',\n",
       " 'The power of personalized AI _ OpenAI.pdf': 'What is the purpose of the \"Model Spec\" document published by OpenAI for ChatGPT?',\n",
       " 'Disrupting malicious uses of AI _ OpenAI.pdf': \"What is OpenAI's mission as stated in the document?\",\n",
       " 'Sharing the latest Model Spec _ OpenAI.pdf': 'What is the release date of the latest Model Spec mentioned in the document?',\n",
       " 'Deep research System Card _ OpenAI.pdf': \"What specific publication date is mentioned in the Deep Research System Card for when the report on deep research's preparedness was released?\",\n",
       " 'Bertelsmann powers creativity and productivity with OpenAI _ OpenAI.pdf': 'What specific AI-powered solutions is Bertelsmann planning to implement for its divisions RTL Deutschland and Penguin Random House according to the document?',\n",
       " 'OpenAI’s Economic Blueprint _ OpenAI.pdf': 'What date and location is scheduled for the kickoff event of OpenAI\\'s \"Innovating for America\" initiative as mentioned in the Economic Blueprint document?',\n",
       " 'Introducing deep research _ OpenAI.pdf': 'What specific model powers the \"deep research\" capability in ChatGPT that is discussed in this document, and what are its main features designed for?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea9bd1b-f746-4442-9f1b-aa31b5c766c6",
   "metadata": {},
   "source": [
    "現在、`filename:question`の辞書があり、これをループして文書を提供することなくgpt-4o(-mini)に質問できます。gpt-4oはVector Store内の関連する文書を見つけることができるはずです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda554b-c3d4-4b07-9028-b41670c2fa20",
   "metadata": {},
   "source": [
    "## 評価\n",
    "\n",
    "辞書をデータフレームに変換し、gpt-4o-miniを使用して処理します。期待されるファイルを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "968d54af-55c0-4b21-9ed8-c57811f9700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for filename, query in questions_dict.items():\n",
    "    rows.append({\"query\": query, \"_id\": filename.replace(\".pdf\", \"\")})\n",
    "\n",
    "# Metrics evaluation parameters\n",
    "k = 5\n",
    "total_queries = len(rows)\n",
    "correct_retrievals_at_k = 0\n",
    "reciprocal_ranks = []\n",
    "average_precisions = []\n",
    "\n",
    "def process_query(row):\n",
    "    query = row['query']\n",
    "    expected_filename = row['_id'] + '.pdf'\n",
    "    # Call file_search via Responses API\n",
    "    response = client.responses.create(\n",
    "        input=query,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_details['id']],\n",
    "            \"max_num_results\": k,\n",
    "        }],\n",
    "        tool_choice=\"required\" # it will force the file_search, while not necessary, it's better to enforce it as this is what we're testing\n",
    "    )\n",
    "    # Extract annotations from the response\n",
    "    annotations = None\n",
    "    if hasattr(response.output[1], 'content') and response.output[1].content:\n",
    "        annotations = response.output[1].content[0].annotations\n",
    "    elif hasattr(response.output[1], 'annotations'):\n",
    "        annotations = response.output[1].annotations\n",
    "\n",
    "    if annotations is None:\n",
    "        print(f\"No annotations for query: {query}\")\n",
    "        return False, 0, 0\n",
    "\n",
    "    # Get top-k retrieved filenames\n",
    "    retrieved_files = [result.filename for result in annotations[:k]]\n",
    "    if expected_filename in retrieved_files:\n",
    "        rank = retrieved_files.index(expected_filename) + 1\n",
    "        rr = 1 / rank\n",
    "        correct = True\n",
    "    else:\n",
    "        rr = 0\n",
    "        correct = False\n",
    "\n",
    "    # Calculate Average Precision\n",
    "    precisions = []\n",
    "    num_relevant = 0\n",
    "    for i, fname in enumerate(retrieved_files):\n",
    "        if fname == expected_filename:\n",
    "            num_relevant += 1\n",
    "            precisions.append(num_relevant / (i + 1))\n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    \n",
    "    if expected_filename not in retrieved_files:\n",
    "        print(\"Expected file NOT found in the retrieved files!\")\n",
    "        \n",
    "    if retrieved_files and retrieved_files[0] != expected_filename:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Expected file: {expected_filename}\")\n",
    "        print(f\"First retrieved file: {retrieved_files[0]}\")\n",
    "        print(f\"Retrieved files: {retrieved_files}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    \n",
    "    return correct, rr, avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee6d3084-5fae-4a26-8fd2-d269ffbc60ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 1.0, 1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_query(rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba088faf-2945-48b3-a3de-412da1ee81fc",
   "metadata": {},
   "source": [
    "この例では、Recall & Precisionが1となっており、私たちのファイルが1位にランクされているため、この例ではMRRとMAP = 1となっています。\n",
    "\n",
    "これで、質問セットに対してこの処理を実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f1e1cc2-0128-48cc-9e4c-5eb416c21347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████▏           | 13/21 [00:07<00:03,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected file NOT found in the retrieved files!\n",
      "Query: What is OpenAI's mission as stated in the document?\n",
      "Expected file: Disrupting malicious uses of AI _ OpenAI.pdf\n",
      "First retrieved file: Introducing the Intelligence Age _ OpenAI.pdf\n",
      "Retrieved files: ['Introducing the Intelligence Age _ OpenAI.pdf']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|██████████████████████▏        | 15/21 [00:14<00:06,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected file NOT found in the retrieved files!\n",
      "Query: What is the purpose of the \"Model Spec\" document published by OpenAI for ChatGPT?\n",
      "Expected file: The power of personalized AI _ OpenAI.pdf\n",
      "First retrieved file: Sharing the latest Model Spec _ OpenAI.pdf\n",
      "Retrieved files: ['Sharing the latest Model Spec _ OpenAI.pdf', 'Sharing the latest Model Spec _ OpenAI.pdf', 'Sharing the latest Model Spec _ OpenAI.pdf', 'Sharing the latest Model Spec _ OpenAI.pdf', 'Sharing the latest Model Spec _ OpenAI.pdf']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 21/21 [00:15<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(process_query, rows), total=total_queries))\n",
    "\n",
    "correct_retrievals_at_k = 0\n",
    "reciprocal_ranks = []\n",
    "average_precisions = []\n",
    "\n",
    "for correct, rr, avg_precision in results:\n",
    "    if correct:\n",
    "        correct_retrievals_at_k += 1\n",
    "    reciprocal_ranks.append(rr)\n",
    "    average_precisions.append(avg_precision)\n",
    "\n",
    "recall_at_k = correct_retrievals_at_k / total_queries\n",
    "precision_at_k = recall_at_k  # In this context, same as recall\n",
    "mrr = sum(reciprocal_ranks) / total_queries\n",
    "map_score = sum(average_precisions) / total_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc74d02-7ee9-4cc3-b48f-5c205c3fdfcb",
   "metadata": {},
   "source": [
    "上記でログ出力された結果は、評価データセットで1位にランクされることが期待されていたファイルが実際には1位にランクされなかった場合、または全く見つからなかった場合のいずれかを示しています。不完全な評価データセットから分かるように、一部の質問は汎用的で別の文書を期待していましたが、我々の検索システムはこの質問に対してその文書を具体的に取得しませんでした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a32ec63-8f39-4085-b123-f2593eb702d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics at k=5:\n",
      "Recall@5: 0.9048\n",
      "Precision@5: 0.9048\n",
      "Mean Reciprocal Rank (MRR): 0.9048\n",
      "Mean Average Precision (MAP): 0.8954\n"
     ]
    }
   ],
   "source": [
    "# Print the metrics with k\n",
    "print(f\"Metrics at k={k}:\")\n",
    "print(f\"Recall@{k}: {recall_at_k:.4f}\")\n",
    "print(f\"Precision@{k}: {precision_at_k:.4f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d19556-8d99-4c53-8800-eec54948a674",
   "metadata": {},
   "source": [
    "このクックブックでは、以下の方法を確認することができました：\n",
    "- PDFコンテキスト詰め込み（4oのビジョンモダリティを活用）と従来のPDFリーダーを使用した評価データセットの生成\n",
    "- ベクターストアの作成とPDFでの入力\n",
    "- OpenAIのResponse APIの`file_search`ツール呼び出しで利用可能な、すぐに使えるRAGシステムを活用したクエリに対するLLM回答の取得\n",
    "- Response APIの一部として、テキストのチャンクがどのように検索、ランク付け、使用されるかの理解\n",
    "- 事前に生成された評価データセットでの精度、適合率、検索率、MRR、MAPの測定\n",
    "\n",
    "Responsesでfile searchを使用することで、RAGアーキテクチャを簡素化し、新しいResponses APIを使用して単一のAPI呼び出しでこれを活用できます。ファイルストレージ、埋め込み、検索がすべて1つのツールに統合されています！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
