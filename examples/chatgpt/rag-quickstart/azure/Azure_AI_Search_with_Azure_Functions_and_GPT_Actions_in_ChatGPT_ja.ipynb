{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ベクターデータベースとしてのAzure AI Search + ChatGPTでのGPT統合のためのAzure Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックでは、Azure AI Search（旧Azure Cognitive Search）をOpenAI埋め込みと組み合わせてベクターデータベースとして使用し、その上にAzure Functionを作成してChatGPTのCustom GPTに接続するための手順を段階的に説明します。\n",
    "\n",
    "これは、Azure内に含まれるRAGインフラストラクチャを構築し、ChatGPTなどの他のプラットフォームと統合するためのエンドポイントとして公開したいお客様向けのソリューションとなります。\n",
    "\n",
    "Azure AI Searchは、ウェブ、モバイル、エンタープライズアプリケーションにおいて、プライベートで異種混合のコンテンツに対する豊富な検索体験を構築するためのインフラストラクチャ、API、ツールを開発者に提供するクラウド検索サービスです。\n",
    "\n",
    "Azure Functionsは、イベント駆動型のコードを実行するサーバーレスコンピュートサービスで、インフラストラクチャを自動的に管理し、スケーリングを行い、他のAzureサービスと統合します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前提条件:\n",
    "この演習を行うには、以下が必要です：\n",
    "- [Azure AI Search Service](https://learn.microsoft.com/azure/search/) と Azure Function Apps を作成する権限を持つ Azure ユーザー\n",
    "- Azure サブスクリプション ID とリソースグループ\n",
    "- [OpenAI Key](https://platform.openai.com/account/api-keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# アーキテクチャ\n",
    "以下は、このソリューションのアーキテクチャの図で、ステップバイステップで説明していきます。\n",
    "\n",
    "![azure-rag-architecture.png](../../../../images/azure-rag-architecture.png)\n",
    "\n",
    "> 注意: ベクターデータストア + サーバーレス関数というこのアーキテクチャパターンは、他のベクターデータストアにも応用できます。例えば、Azure内でPostgresのようなものを使用したい場合は、[Azure AI Search設定の構成](#configure-azure-ai-search-settings)ステップを変更してPostgresの要件を設定し、[Azure AIベクター検索の作成](#create-azure-ai-vector-search)を変更してPostgres内にデータベースとテーブルを作成し、このリポジトリの`function_app.py`コードをAzure AI Searchの代わりにPostgresをクエリするように更新します。データ準備とAzure Functionの作成は一貫して同じままです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目次:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **[環境のセットアップ](#set-up-environment)**\n",
    "    必要なライブラリのインストールとインポート、およびAzure設定の構成により環境をセットアップします。以下を含みます：\n",
    "     - [必要なライブラリのインストールとインポート](#install-and-import-required-libraries)\n",
    "     - [OpenAI設定の構成](#configure-openai-settings)\n",
    "     - [Azure AI Search設定の構成](#configure-azure-ai-search-settings)\n",
    " \n",
    "\n",
    "2. **[データの準備](#prepare-data)** ドキュメントの埋め込みと追加メタデータの取得により、アップロード用のデータを準備します。この例では、OpenAIのドキュメントのサブセットをサンプルデータとして使用します。\n",
    " \n",
    "3. **[Azure AI Vector Searchの作成](#create-azure-ai-vector-search)** Azure AI Vector Searchを作成し、準備したデータをアップロードします。以下を含みます：\n",
    "     - [インデックスの作成](#create-index): Azure AI Searchでインデックスを作成する手順。\n",
    "     - [データのアップロード](#upload-data): Azure AI Searchにデータをアップロードする手順。\n",
    "     - [検索のテスト](#test-search): 検索機能をテストする手順。\n",
    " \n",
    "4. **[Azure Functionの作成](#create-azure-function)** Azure AI Vector Searchと連携するAzure Functionを作成します。以下を含みます：\n",
    "     - [ストレージアカウントの作成](#create-storage-account): Azure Function用のストレージアカウントを作成する手順。\n",
    "     - [Function Appの作成](#create-function-app): AzureでFunction Appを作成する手順。\n",
    " \n",
    "5. **[ChatGPTのカスタムGPTでの入力](#input-in-a-custom-gpt-in-chatgpt)** Azure FunctionをChatGPTのカスタムGPTと統合します。以下を含みます：\n",
    "     - [OpenAPI仕様の作成](#create-openapi-spec): Azure Function用のOpenAPI仕様を作成する手順。\n",
    "     - [GPT指示の作成](#create-gpt-instructions): 統合用のGPT固有の指示を作成する手順。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 環境のセットアップ\n",
    "必要なライブラリをインポートし、Azureの設定を構成して環境をセットアップします。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なライブラリのインストールとインポート\n",
    "可読性を向上させるため、これらのライブラリを標準Pythonライブラリ、サードパーティライブラリ、Azure関連ライブラリに分類します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q wget\n",
    "! pip install -q azure-search-documents \n",
    "! pip install -q azure-identity\n",
    "! pip install -q openai\n",
    "! pip install -q azure-mgmt-search\n",
    "! pip install -q pandas\n",
    "! pip install -q azure-mgmt-resource \n",
    "! pip install -q azure-mgmt-storage\n",
    "! pip install -q pyperclip\n",
    "! pip install -q PyPDF2\n",
    "! pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import json  \n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import csv\n",
    "from itertools import islice\n",
    "import uuid\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import pyperclip\n",
    "\n",
    "# OpenAI Libraries (note we use OpenAI directly here, but you can replace with Azure OpenAI as needed)\n",
    "from openai import OpenAI\n",
    "\n",
    "# Azure Identity and Credentials\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "# Azure Search Documents\n",
    "from azure.search.documents import SearchClient, SearchIndexingBufferedSender  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery\n",
    ")\n",
    "from azure.search.documents.indexes.models import (\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    SearchField,\n",
    "    SearchableField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "\n",
    "# Azure Management Clients\n",
    "from azure.mgmt.search import SearchManagementClient\n",
    "from azure.mgmt.resource import ResourceManagementClient, SubscriptionClient\n",
    "from azure.mgmt.storage import StorageManagementClient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI設定の構成\n",
    "\n",
    "このセクションを進める前に、OpenAI APIキーを取得していることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as an env var>\") # Saving this as a variable to reference in function app in later step\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "embeddings_model = \"text-embedding-3-small\" # We'll use this by default, but you can change to your text-embedding-3-large if desired"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure AI Search設定の構成\n",
    "Azure AI Searchサービスの詳細は、Azure Portalまたは[Search Management SDK](https://learn.microsoft.com/rest/api/searchmanagement/)を使用してプログラム的に確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前提条件：\n",
    "- AzureのサブスクリプションID\n",
    "- Azureのリソースグループ名\n",
    "- Azureのリージョン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the below with your values\n",
    "subscription_id=\"<enter_your_subscription_id>\"\n",
    "resource_group=\"<enter_your_resource_group>\"\n",
    "\n",
    "## Make sure to choose a region that supports the proper products. We've defaulted to \"eastus\" below. https://azure.microsoft.com/en-us/explore/global-infrastructure/products-by-region/#products-by-region_tab5\n",
    "region = \"eastus\"\n",
    "credential = InteractiveBrowserCredential()\n",
    "subscription_client = SubscriptionClient(credential)\n",
    "subscription = next(subscription_client.subscriptions.list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Azure AI Search サービスの作成と設定\n",
    "以下では、検索サービスの一意の名前を生成し、サービスのプロパティを設定して、検索サービスを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SearchManagementClient with the provided credentials and subscription ID\n",
    "search_management_client = SearchManagementClient(\n",
    "    credential=credential,\n",
    "    subscription_id=subscription_id,\n",
    ")\n",
    "\n",
    "# Generate a unique name for the search service using UUID, but you can change this if you'd like.\n",
    "generated_uuid = str(uuid.uuid4())\n",
    "search_service_name = \"search-service-gpt-demo\" + generated_uuid\n",
    "## The below is the default endpoint structure that is created when you create a search service. This may differ based on your Azure settings.\n",
    "search_service_endpoint = 'https://'+search_service_name+'.search.windows.net'\n",
    "\n",
    "# Create or update the search service with the specified parameters\n",
    "response = search_management_client.services.begin_create_or_update(\n",
    "    resource_group_name=resource_group,\n",
    "    search_service_name=search_service_name,\n",
    "    service={\n",
    "        \"location\": region,\n",
    "        \"properties\": {\"hostingMode\": \"default\", \"partitionCount\": 1, \"replicaCount\": 1},\n",
    "        # We are using the free pricing tier for this demo. You are only allowed one free search service per subscription.\n",
    "        \"sku\": {\"name\": \"free\"},\n",
    "        \"tags\": {\"app-name\": \"Search service demo\"},\n",
    "    },\n",
    ").result()\n",
    "\n",
    "# Convert the response to a dictionary and then to a pretty-printed JSON string\n",
    "response_dict = response.as_dict()\n",
    "response_json = json.dumps(response_dict, indent=4)\n",
    "\n",
    "print(response_json)\n",
    "print(\"Search Service Name:\" + search_service_name)\n",
    "print(\"Search Service Endpoint:\" + search_service_endpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Service API キーの取得\n",
    "検索サービスが稼働している状態で、[Search Service API キー](https://learn.microsoft.com/en-us/azure/search/search-security-api-keys?tabs=rest-use,portal-find,portal-query)を取得する必要があります。このキーはインデックスの作成を開始し、後で検索を実行するために使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the admin keys for the search service\n",
    "try:\n",
    "    response = search_management_client.admin_keys.get(\n",
    "        resource_group_name=resource_group,\n",
    "        search_service_name=search_service_name,\n",
    "    )\n",
    "    # Extract the primary API key from the response and save as a variable to be used later\n",
    "    search_service_api_key = response.primary_key\n",
    "    print(\"Successfully retrieved the API key.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to retrieve the API key: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備\n",
    "OpenAIドキュメントの数ページをoai_docsフォルダに埋め込んで保存します。まず各ページを埋め込み、CSVに追加し、そのCSVを使用してインデックスにアップロードします。\n",
    "\n",
    "8191トークンのコンテキストを超える長いテキストファイルを処理するために、チャンク埋め込みを個別に使用するか、各チャンクのサイズで重み付けした平均化などの方法で組み合わせることができます。\n",
    "\n",
    "シーケンスをチャンクに分割するPythonの公式クックブックの関数を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n",
    "    # batched('ABCDEFG', 3) --> ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while (batch := tuple(islice(it, n))):\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、文字列をトークンにエンコードし、それをチャンクに分割する関数を定義します。OpenAIによる高速なオープンソーストークナイザーであるtiktokenを使用します。\n",
    "\n",
    "Tiktokenを使用したトークンのカウントについて詳しく知りたい場合は、[このクックブック](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)をご確認ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_tokens(text, chunk_length, encoding_name='cl100k_base'):\n",
    "    # Get the encoding object for the specified encoding name. OpenAI's tiktoken library, which is used in this notebook, currently supports two encodings: 'bpe' and 'cl100k_base'. The 'bpe' encoding is used for GPT-3 and earlier models, while 'cl100k_base' is used for newer models like GPT-4.\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    # Encode the input text into tokens\n",
    "    tokens = encoding.encode(text)\n",
    "    # Create an iterator that yields chunks of tokens of the specified length\n",
    "    chunks_iterator = batched(tokens, chunk_length)\n",
    "    # Yield each chunk from the iterator\n",
    "    yield from chunks_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、入力テキストが最大コンテキスト長より長い場合でも、入力トークンをチャンクに分割し、各チャンクを個別に埋め込むことで、埋め込みリクエストを安全に処理する関数を書くことができます。averageフラグをTrueに設定すると、チャンク埋め込みの重み付き平均を返し、Falseに設定すると、変更されていないチャンク埋め込みのリストを単純に返します。\n",
    "\n",
    "> 注意：ここでは、以下を含む他のより洗練された技術を使用することもできます：\n",
    "> - GPT-4oを使用して画像/チャートの説明を埋め込み用にキャプチャする\n",
    "> - 重要なコンテキストが切り取られることを最小限に抑えるため、チャンク間でテキストの重複を保持する\n",
    "> - 段落やセクションに基づいてチャンクを分割する\n",
    "> - 各記事についてより説明的なメタデータを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change the below based on model. The below is for the latest embeddings models from OpenAI, so you can leave as is unless you are using a different embedding model..\n",
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING='cl100k_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, model):\n",
    "    # Generate embeddings for the provided text using the specified model\n",
    "    embeddings_response = openai_client.embeddings.create(model=model, input=text)\n",
    "    # Extract the embedding data from the response\n",
    "    embedding = embeddings_response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "def len_safe_get_embedding(text, model=embeddings_model, max_tokens=EMBEDDING_CTX_LENGTH, encoding_name=EMBEDDING_ENCODING):\n",
    "    # Initialize lists to store embeddings and corresponding text chunks\n",
    "    chunk_embeddings = []\n",
    "    chunk_texts = []\n",
    "    # Iterate over chunks of tokens from the input text\n",
    "    for chunk in chunked_tokens(text, chunk_length=max_tokens, encoding_name=encoding_name):\n",
    "        # Generate embeddings for each chunk and append to the list\n",
    "        chunk_embeddings.append(generate_embeddings(chunk, model=model))\n",
    "        # Decode the chunk back to text and append to the list\n",
    "        chunk_texts.append(tiktoken.get_encoding(encoding_name).decode(chunk))\n",
    "    # Return the list of chunk embeddings and the corresponding text chunks\n",
    "    return chunk_embeddings, chunk_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、ドキュメントに関する追加のメタデータをキャプチャするヘルパー関数を定義できます。これは検索クエリのメタデータフィルターとして使用したり、検索のためのより豊富なデータをキャプチャしたりするのに便利です。\n",
    "\n",
    "この例では、後でメタデータフィルターで使用するカテゴリのリストから選択します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These are the categories I will be using for the categorization task. You can change these as needed based on your use case.\n",
    "categories = ['authentication','models','techniques','tools','setup','billing_limits','other']\n",
    "\n",
    "def categorize_text(text, categories):\n",
    "    # Create a prompt for categorization\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"\"\"You are an expert in LLMs, and you will be given text that corresponds to an article in OpenAI's documentation.\n",
    "         Categorize the document into one of these categories: {', '.join(categories)}. Only respond with the category name and nothing else.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    try:\n",
    "        # Call the OpenAI API to categorize the text\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages\n",
    "        )\n",
    "        # Extract the category from the response\n",
    "        category = response.choices[0].message.content\n",
    "        return category\n",
    "    except Exception as e:\n",
    "        print(f\"Error categorizing text: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、dataフォルダ内のoai_docsフォルダにある.txtファイルを処理するためのヘルパー関数を定義できます。これは独自のデータでも使用でき、.txtファイルと.pdfファイルの両方をサポートしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Initialize the PDF reader\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    # Iterate through each page in the PDF and extract text\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def process_file(file_path, idx, categories, embeddings_model):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"Processing file {idx + 1}: {file_name}\")\n",
    "    \n",
    "    # Read text content from .txt files\n",
    "    if file_name.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "    # Extract text content from .pdf files\n",
    "    elif file_name.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    \n",
    "    title = file_name\n",
    "    # Generate embeddings for the title\n",
    "    title_vectors, title_text = len_safe_get_embedding(title, embeddings_model)\n",
    "    print(f\"Generated title embeddings for {file_name}\")\n",
    "    \n",
    "    # Generate embeddings for the content\n",
    "    content_vectors, content_text = len_safe_get_embedding(text, embeddings_model)\n",
    "    print(f\"Generated content embeddings for {file_name}\")\n",
    "    \n",
    "    category = categorize_text(' '.join(content_text), categories)\n",
    "    print(f\"Categorized {file_name} as {category}\")\n",
    "    \n",
    "    # Prepare the data to be appended\n",
    "    data = []\n",
    "    for i, content_vector in enumerate(content_vectors):\n",
    "        data.append({\n",
    "            \"id\": f\"{idx}_{i}\",\n",
    "            \"vector_id\": f\"{idx}_{i}\",\n",
    "            \"title\": title_text[0],\n",
    "            \"text\": content_text[i],\n",
    "            \"title_vector\": json.dumps(title_vectors[0]),  # Assuming title is short and has only one chunk\n",
    "            \"content_vector\": json.dumps(content_vector),\n",
    "            \"category\": category\n",
    "        })\n",
    "        print(f\"Appended data for chunk {i + 1}/{len(content_vectors)} of {file_name}\")\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、このヘルパー関数を使用してOpenAIドキュメントを処理します。以下の`process_files`でフォルダを変更することで、独自のデータを使用するように自由に更新してください。\n",
    "\n",
    "なお、これは選択したフォルダ内のドキュメントを並行処理するため、txtファイルを使用する場合は30秒未満で完了し、PDFを使用する場合は少し長くかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Customize the location below if you are using different data besides the OpenAI documentation. Note that if you are using a different dataset, you will need to update the categories list as well.\n",
    "folder_name = \"../../../data/oai_docs\"\n",
    "\n",
    "files = [os.path.join(folder_name, f) for f in os.listdir(folder_name) if f.endswith('.txt') or f.endswith('.pdf')]\n",
    "data = []\n",
    "\n",
    "# Process each file concurrently\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_file, file_path, idx, categories, embeddings_model): idx for idx, file_path in enumerate(files)}\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            data.extend(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {str(e)}\")\n",
    "\n",
    "# Write the data to a CSV file\n",
    "csv_file = os.path.join(\"..\", \"embedded_data.csv\")\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = [\"id\", \"vector_id\", \"title\", \"text\", \"title_vector\", \"content_vector\",\"category\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n",
    "        print(f\"Wrote row with id {row['id']} to CSV\")\n",
    "\n",
    "# Convert the CSV file to a Dataframe\n",
    "article_df = pd.read_csv(\"../embedded_data.csv\")\n",
    "# Read vectors from strings back into a list using json.loads\n",
    "article_df[\"title_vector\"] = article_df.title_vector.apply(json.loads)\n",
    "article_df[\"content_vector\"] = article_df.content_vector.apply(json.loads)\n",
    "article_df[\"vector_id\"] = article_df[\"vector_id\"].apply(str)\n",
    "article_df[\"category\"] = article_df[\"category\"].apply(str)\n",
    "article_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、ベクターデータベースにアップロードできる6つの列を持つ`embedded_data.csv`ファイルが完成しました！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Vector Search の作成"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インデックスの作成\n",
    "Azure AI Search Python SDKの`SearchIndexClient`を使用して検索インデックスを定義し、作成します。このインデックスは、ベクトル検索とハイブリッド検索の両方の機能を組み込んでいます。詳細については、Microsoftの[ベクトルインデックスの作成](https://learn.microsoft.com/azure/search/vector-search-how-to-create-index?.tabs=config-2023-11-01%2Crest-2023-11-01%2Cpush%2Cportal-check-index)に関するドキュメントをご覧ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"azure-ai-search-openai-cookbook-demo\"\n",
    "# index_name = \"<insert_name_for_index>\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=search_service_endpoint, credential=AzureKeyCredential(search_service_api_key)\n",
    ")\n",
    "# Define the fields for the index. Update these based on your data.\n",
    "# Each field represents a column in the search index\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String),  # Simple string field for document ID\n",
    "    SimpleField(name=\"vector_id\", type=SearchFieldDataType.String, key=True),  # Key field for the index\n",
    "    # SimpleField(name=\"url\", type=SearchFieldDataType.String),  # URL field (commented out)\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),  # Searchable field for document title\n",
    "    SearchableField(name=\"text\", type=SearchFieldDataType.String),  # Searchable field for document text\n",
    "    SearchField(\n",
    "        name=\"title_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),  # Collection of single values for title vector\n",
    "        vector_search_dimensions=1536,  # Number of dimensions in the vector\n",
    "        vector_search_profile_name=\"my-vector-config\",  # Profile name for vector search configuration\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"content_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),  # Collection of single values for content vector\n",
    "        vector_search_dimensions=1536,  # Number of dimensions in the vector\n",
    "        vector_search_profile_name=\"my-vector-config\",  # Profile name for vector search configuration\n",
    "    ),\n",
    "    SearchableField(name=\"category\", type=SearchFieldDataType.String, filterable=True),  # Searchable field for document category\n",
    "]\n",
    "\n",
    "# This configuration defines the algorithm and parameters for vector search\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"my-hnsw\",  # Name of the HNSW algorithm configuration\n",
    "            kind=VectorSearchAlgorithmKind.HNSW,  # Type of algorithm\n",
    "            parameters=HnswParameters(\n",
    "                m=4,  # Number of bi-directional links created for every new element\n",
    "                ef_construction=400,  # Size of the dynamic list for the nearest neighbors during construction\n",
    "                ef_search=500,  # Size of the dynamic list for the nearest neighbors during search\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE,  # Distance metric used for the search\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"my-vector-config\",  # Name of the vector search profile\n",
    "            algorithm_configuration_name=\"my-hnsw\",  # Reference to the algorithm configuration\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create the search index with the vector search configuration\n",
    "# This combines all the configurations into a single search index\n",
    "index = SearchIndex(\n",
    "    name=index_name,  # Name of the index\n",
    "    fields=fields,  # Fields defined for the index\n",
    "    vector_search=vector_search  # Vector search configuration\n",
    "\n",
    ")\n",
    "\n",
    "# Create or update the index\n",
    "# This sends the index definition to the Azure Search service\n",
    "result = index_client.create_index(index)\n",
    "print(f\"{result.name} created\")  # Output the name of the created index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのアップロード\n",
    "\n",
    "次に、上記で`embedded_data.csv`にpandas DataFrameから保存した記事をAzure AI Searchインデックスにアップロードします。データインポート戦略とベストプラクティスの詳細なガイドについては、[Azure AI Searchでのデータインポート](https://learn.microsoft.com/azure/search/search-what-is-data-import)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'id' and 'vector_id' columns to string so one of them can serve as our key field\n",
    "article_df[\"id\"] = article_df[\"id\"].astype(str)\n",
    "article_df[\"vector_id\"] = article_df[\"vector_id\"].astype(str)\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "documents = article_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Log the number of documents to be uploaded\n",
    "print(f\"Number of documents to upload: {len(documents)}\")\n",
    "\n",
    "# Create a SearchIndexingBufferedSender\n",
    "batch_client = SearchIndexingBufferedSender(\n",
    "    search_service_endpoint, index_name, AzureKeyCredential(search_service_api_key)\n",
    ")\n",
    "# Get the first document to check its schema\n",
    "first_document = documents[0]\n",
    "\n",
    "# Get the index schema\n",
    "index_schema = index_client.get_index(index_name)\n",
    "\n",
    "# Get the field names from the index schema\n",
    "index_fields = {field.name: field.type for field in index_schema.fields}\n",
    "\n",
    "# Check each field in the first document\n",
    "for field, value in first_document.items():\n",
    "    if field not in index_fields:\n",
    "        print(f\"Field '{field}' is not in the index schema.\")\n",
    "\n",
    "# Check for any fields in the index schema that are not in the documents\n",
    "for field in index_fields:\n",
    "    if field not in first_document:\n",
    "        print(f\"Field '{field}' is in the index schema but not in the documents.\")\n",
    "\n",
    "try:\n",
    "    if documents:\n",
    "        # Add upload actions for all documents in a single call\n",
    "        upload_result = batch_client.upload_documents(documents=documents)\n",
    "\n",
    "        # Check if the upload was successful\n",
    "        # Manually flush to send any remaining documents in the buffer\n",
    "        batch_client.flush()\n",
    "        \n",
    "        print(f\"Uploaded {len(documents)} documents in total\")\n",
    "    else:\n",
    "        print(\"No documents to upload.\")\n",
    "except HttpResponseError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    raise  # Re-raise the exception to ensure it errors out\n",
    "finally:\n",
    "    # Clean up resources\n",
    "    batch_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト検索\n",
    "データがアップロードされたので、期待通りに動作することを確認するために、以下でベクトル類似度検索とハイブリッド検索の両方をローカルでテストします。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "純粋なベクトル検索とハイブリッド検索の両方をテストできます。純粋なベクトル検索では、以下の`search_text`に`None`を渡すことで、ベクトル類似度のみで検索を行います。ハイブリッド検索では、クエリテキスト`query`を`search_text`に渡すことで、従来のキーワードベース検索の機能とベクトルベースの類似度検索を組み合わせ、より関連性が高く文脈に適した結果を提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What model should I use to embed?\"\n",
    "# Note: we'll have the GPT choose the category automatically once we put it in ChatGPT\n",
    "category =\"models\"\n",
    "\n",
    "search_client = SearchClient(search_service_endpoint, index_name, AzureKeyCredential(search_service_api_key))\n",
    "vector_query = VectorizedQuery(vector=generate_embeddings(query, embeddings_model), k_nearest_neighbors=3, fields=\"content_vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None, # Pass in None if you want to use pure vector search, and `query` if you want to use hybrid search\n",
    "    vector_queries= [vector_query], \n",
    "    select=[\"title\", \"text\"],\n",
    "    filter=f\"category eq '{category}'\" \n",
    ")\n",
    "\n",
    "for result in results:  \n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Function の作成\n",
    "\n",
    "Azure Functions は、新しい AI 検索の上に API を構築する簡単な方法です。私たちのコード（このフォルダの `function_app.py` ファイル、または[こちら](https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/azure/function_app.py)のリンクを参照）は以下の処理を行います：\n",
    "\n",
    "1. ユーザーのクエリ、検索インデックスエンドポイント、インデックス名、k_nearest_neighbors*、使用する検索列（content_vector または title_vector のいずれか）、およびハイブリッドクエリを使用するかどうかの入力を受け取ります\n",
    "2. ユーザーのクエリを取得し、それを埋め込みます\n",
    "3. ベクトル検索を実行し、関連するテキストチャンクを取得します\n",
    "4. それらの関連するテキストチャンクをレスポンスボディとして返します\n",
    "\n",
    "*ベクトル検索の文脈において、k_nearest_neighbors は検索が返すべき「最も近い」ベクトル（コサイン類似度の観点で）の数を指定します。例えば、k_nearest_neighbors が 3 に設定されている場合、検索はクエリベクトルに最も類似しているインデックス内の 3 つのベクトルを返します。\n",
    "\n",
    "> この Azure Function には_認証機能がありません_。ただし、[こちら](https://learn.microsoft.com/en-us/azure/azure-functions/security-concepts?tabs=v4)のドキュメントに従って認証を設定することができます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ストレージアカウントの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードを使用して新しいストレージアカウントを作成できますが、このブロックをスキップして、既存のストレージアカウントを使用するように後続の手順を変更しても構いません。これには最大30秒かかる場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update below with a different name\n",
    "storage_account_name = \"<enter-storage-account-name>\"\n",
    "\n",
    "## Use below SKU or any other SKU as per your requirement\n",
    "sku = \"Standard_LRS\"\n",
    "resource_client = ResourceManagementClient(credential, subscription_id)\n",
    "storage_client = StorageManagementClient(credential, subscription_id)\n",
    "\n",
    "# Create resource group if it doesn't exist\n",
    "rg_result = resource_client.resource_groups.create_or_update(resource_group, {\"location\": region})\n",
    "\n",
    "# Create storage account\n",
    "storage_async_operation = storage_client.storage_accounts.begin_create(\n",
    "    resource_group,\n",
    "    storage_account_name,\n",
    "    {\n",
    "        \"sku\": {\"name\": sku},\n",
    "        \"kind\": \"StorageV2\",\n",
    "        \"location\": region,\n",
    "    },\n",
    ")\n",
    "storage_account = storage_async_operation.result()\n",
    "\n",
    "print(f\"Storage account {storage_account.name} created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function App の作成\n",
    "この Function App は、GPT Action によってトリガーされた際に Python コードが実行される場所です。Function App について詳しく読むには、[こちら](https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview?pivots=programming-language-csharp)のドキュメントを参照してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Appsをデプロイするには、Azure CLIとAzure Functions Core Toolsを使用する必要があります。\n",
    "\n",
    "> 以下は、あなたの仮想環境でプラットフォームタイプに基づいてインストールと実行を試みますが、うまくいかない場合は、Azureドキュメントを読んで[Azure Function Core Tools](https://learn.microsoft.com/en-us/azure/azure-functions/create-first-function-cli-python?tabs=linux,bash,azure-cli,browser)と[Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)のインストール方法を確認してください。その後、このフォルダに移動してから、以下の`subprocess.run`コマンドをターミナルで実行してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、必要なAzureコマンドを実行するために、環境に関連するツールが揃っていることを確認します。インストールには数分かかる場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_type = platform.system()\n",
    "\n",
    "if os_type == \"Windows\":\n",
    "    # Install Azure Functions Core Tools on Windows\n",
    "    subprocess.run([\"npm\", \"install\", \"-g\", \"azure-functions-core-tools@3\", \"--unsafe-perm\", \"true\"], check=True)\n",
    "    # Install Azure CLI on Windows\n",
    "    subprocess.run([\"powershell\", \"-Command\", \"Invoke-WebRequest -Uri https://aka.ms/installazurecliwindows -OutFile .\\\\AzureCLI.msi; Start-Process msiexec.exe -ArgumentList '/I AzureCLI.msi /quiet' -Wait\"], check=True)\n",
    "elif os_type == \"Darwin\":  # MacOS\n",
    "    # Install Azure Functions Core Tools on MacOS\n",
    "    if platform.machine() == 'arm64':\n",
    "        # For M1 Macs\n",
    "        subprocess.run([\"arch\", \"-arm64\", \"brew\", \"install\", \"azure-functions-core-tools@3\"], check=True)\n",
    "    else:\n",
    "        # For Intel Macs\n",
    "        subprocess.run([\"brew\", \"install\", \"azure-functions-core-tools@3\"], check=True)\n",
    "    # Install Azure CLI on MacOS\n",
    "    subprocess.run([\"brew\", \"update\"], check=True)\n",
    "    subprocess.run([\"brew\", \"install\", \"azure-cli\"], check=True)\n",
    "elif os_type == \"Linux\":\n",
    "    # Install Azure Functions Core Tools on Linux\n",
    "    subprocess.run([\"curl\", \"https://packages.microsoft.com/keys/microsoft.asc\", \"|\", \"gpg\", \"--dearmor\", \">\", \"microsoft.gpg\"], check=True, shell=True)\n",
    "    subprocess.run([\"sudo\", \"mv\", \"microsoft.gpg\", \"/etc/apt/trusted.gpg.d/microsoft.gpg\"], check=True)\n",
    "    subprocess.run([\"sudo\", \"sh\", \"-c\", \"'echo \\\"deb [arch=amd64] https://packages.microsoft.com/repos/microsoft-ubuntu-$(lsb_release -cs)-prod $(lsb_release -cs) main\\\" > /etc/apt/sources.list.d/dotnetdev.list'\"], check=True, shell=True)\n",
    "    subprocess.run([\"sudo\", \"apt-get\", \"update\"], check=True)\n",
    "    subprocess.run([\"sudo\", \"apt-get\", \"install\", \"azure-functions-core-tools-3\"], check=True)\n",
    "    # Install Azure CLI on Linux\n",
    "    subprocess.run([\"curl\", \"-sL\", \"https://aka.ms/InstallAzureCLIDeb\", \"|\", \"sudo\", \"bash\"], check=True, shell=True)\n",
    "else:\n",
    "    # Raise an error if the operating system is not supported\n",
    "    raise OSError(\"Unsupported operating system\")\n",
    "\n",
    "# Verify the installation of Azure Functions Core Tools\n",
    "subprocess.run([\"func\", \"--version\"], check=True)\n",
    "# Verify the installation of Azure CLI\n",
    "subprocess.run([\"az\", \"--version\"], check=True)\n",
    "\n",
    "subprocess.run([\n",
    "    \"az\", \"login\"\n",
    "], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、Azure用のキー環境変数を含む`local.settings.json`ファイルを作成する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_settings_content = f\"\"\"\n",
    "{{\n",
    "  \"IsEncrypted\": false,\n",
    "  \"Values\": {{\n",
    "    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n",
    "    \"FUNCTIONS_WORKER_RUNTIME\": \"python\",\n",
    "    \"OPENAI_API_KEY\": \"{openai_api_key}\",\n",
    "    \"EMBEDDINGS_MODEL\": \"{embeddings_model}\",\n",
    "    \"SEARCH_SERVICE_API_KEY\": \"{search_service_api_key}\",\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"local.settings.json\", \"w\") as file:\n",
    "    file.write(local_settings_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`local.settings.json`ファイルを確認し、環境変数が期待する値と一致していることを確認してください。\n",
    "\n",
    "次に、以下でアプリに名前を付けると、Function Appを作成して関数を公開する準備が整います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your own values. This name will appear in the URL of the API call https://<app_name>.azurewebsites.net\n",
    "app_name = \"<app-name>\"\n",
    "\n",
    "subprocess.run([\n",
    "    \"az\", \"functionapp\", \"create\",\n",
    "    \"--resource-group\", resource_group,\n",
    "    \"--consumption-plan-location\", region,\n",
    "    \"--runtime\", \"python\",\n",
    "    \"--name\", app_name,\n",
    "    \"--storage-account\", storage_account_name,\n",
    "    \"--os-type\", \"Linux\",\n",
    "], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Appを作成したら、次に関数で使用するための設定変数をFunction Appに追加します。具体的には、`OPENAI_API_KEY`、`SEARCH_SERVICE_API_KEY`、および`EMBEDDINGS_MODEL`が必要です。これらはすべて`function_app.py`コードで使用されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the relevant environment variables \n",
    "env_vars = {\n",
    "    \"OPENAI_API_KEY\": openai_api_key,\n",
    "    \"SEARCH_SERVICE_API_KEY\": search_service_api_key,\n",
    "    \"EMBEDDINGS_MODEL\": embeddings_model\n",
    "}\n",
    "\n",
    "# Create the settings argument for the az functionapp create command\n",
    "settings_args = []\n",
    "for key, value in env_vars.items():\n",
    "    settings_args.append(f\"{key}={value}\")\n",
    "\n",
    "subprocess.run([\n",
    "    \"az\", \"functionapp\", \"config\", \"appsettings\", \"set\",\n",
    "    \"--name\", app_name,\n",
    "    \"--resource-group\", resource_group,\n",
    "    \"--settings\", *settings_args\n",
    "], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、関数コード `function_app.py` をAzure Functionに公開する準備が整いました。デプロイには最大10分かかる場合があります。これが完了すると、Azure AI Searchの上にAzure Functionを使用したAPIエンドポイントが作成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\n",
    "    \"func\", \"azure\", \"functionapp\", \"publish\", app_name\n",
    "], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPTのカスタムGPTでの入力\n",
    "Vector Search Indexにクエリを実行するAzure Functionが用意できたので、これをGPT Actionとして設定しましょう！\n",
    "\n",
    "GPTについてのドキュメントは[こちら](https://openai.com/index/introducing-gpts/)、GPT Actionsについては[こちら](https://platform.openai.com/docs/actions)をご覧ください。以下をGPTの指示として、またGPT ActionのOpenAPI仕様として使用してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAPI仕様の作成\n",
    "以下はOpenAPI仕様のサンプルです。下記のブロックを実行すると、機能的な仕様がクリップボードにコピーされ、GPT Actionに貼り付けることができます。\n",
    "\n",
    "なお、これはデフォルトでは認証機能がありませんが、認証セクションの[このクックブック](https://cookbook.openai.com/examples/chatgpt/gpt_actions_library/gpt_middleware_azure_function#part-2-set-up-auth)のパターンに従うか、[こちら](https://learn.microsoft.com/en-us/azure/app-service/overview-authentication-authorization)のドキュメントを参照することで、OAuthを使用したAzure Functionsを設定できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spec = f\"\"\"\n",
    "openapi: 3.1.0\n",
    "info:\n",
    "  title: Vector Similarity Search API\n",
    "  description: API for performing vector similarity search.\n",
    "  version: 1.0.0\n",
    "servers:\n",
    "  - url: https://{app_name}.azurewebsites.net/api\n",
    "    description: Main (production) server\n",
    "paths:\n",
    "  /vector_similarity_search:\n",
    "    post:\n",
    "      operationId: vectorSimilaritySearch\n",
    "      summary: Perform a vector similarity search.\n",
    "      requestBody:\n",
    "        required: true\n",
    "        content:\n",
    "          application/json:\n",
    "            schema:\n",
    "              type: object\n",
    "              properties:\n",
    "                search_service_endpoint:\n",
    "                  type: string\n",
    "                  description: The endpoint of the search service.\n",
    "                index_name:\n",
    "                  type: string\n",
    "                  description: The name of the search index.\n",
    "                query:\n",
    "                  type: string\n",
    "                  description: The search query.\n",
    "                k_nearest_neighbors:\n",
    "                  type: integer\n",
    "                  description: The number of nearest neighbors to return.\n",
    "                search_column:\n",
    "                  type: string\n",
    "                  description: The name of the search column.\n",
    "                use_hybrid_query:\n",
    "                  type: boolean\n",
    "                  description: Whether to use a hybrid query.\n",
    "                category:\n",
    "                  type: string\n",
    "                  description: category to filter.\n",
    "              required:\n",
    "                - search_service_endpoint\n",
    "                - index_name\n",
    "                - query\n",
    "                - k_nearest_neighbors\n",
    "                - search_column\n",
    "                - use_hybrid_query\n",
    "      responses:\n",
    "        '200':\n",
    "          description: A successful response with the search results.\n",
    "          content:\n",
    "            application/json:\n",
    "              schema:\n",
    "                type: object\n",
    "                properties:\n",
    "                  results:\n",
    "                    type: array\n",
    "                    items:\n",
    "                      type: object\n",
    "                      properties:\n",
    "                        id:\n",
    "                          type: string\n",
    "                          description: The identifier of the result item.\n",
    "                        score:\n",
    "                          type: number\n",
    "                          description: The similarity score of the result item.\n",
    "                        content:\n",
    "                          type: object\n",
    "                          description: The content of the result item.\n",
    "        '400':\n",
    "          description: Bad request due to missing or invalid parameters.\n",
    "        '500':\n",
    "          description: Internal server error.\n",
    "\"\"\"\n",
    "pyperclip.copy(spec)\n",
    "print(\"OpenAPI spec copied to clipboard\")\n",
    "print(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT指示の作成\n",
    "\n",
    "必要に応じて指示を自由に修正してください。プロンプトエンジニアリングのヒントについては、[こちら](https://platform.openai.com/docs/guides/prompt-engineering)のドキュメントをご確認ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = f'''\n",
    "You are an OAI docs assistant. You have an action in your knowledge base where you can make a POST request to search for information. The POST request should always include: {{\n",
    "    \"search_service_endpoint\": \"{search_service_endpoint}\",\n",
    "    \"index_name\": {index_name},\n",
    "    \"query\": \"<user_query>\",\n",
    "    \"k_nearest_neighbors\": 1,\n",
    "    \"search_column\": \"content_vector\",\n",
    "    \"use_hybrid_query\": true,\n",
    "    \"category\": \"<category>\"\n",
    "}}. Only the query and category change based on the user's request. Your goal is to assist users by performing searches using this POST request and providing them with relevant information based on the query.\n",
    "\n",
    "You must only include knowledge you get from your action in your response.\n",
    "The category must be from the following list: {categories}, which you should determine based on the user's query. If you cannot determine, then do not include the category in the POST request.\n",
    "'''\n",
    "pyperclip.copy(instructions)\n",
    "print(\"GPT Instructions copied to clipboard\")\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、ベクターデータベースにクエリを実行するGPTができました！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "以下の手順により、Azure AI SearchとChatGPTのGPT Actionsの統合を正常に完了しました：\n",
    "1. OpenAIの埋め込みを使用してデータを埋め込み、gpt-4oを使用して追加のメタデータを加えました。\n",
    "2. そのデータをAzure AI Searchにアップロードしました。\n",
    "3. Azure Functionsを使用してクエリを実行するエンドポイントを作成しました。\n",
    "4. それをカスタムGPTに組み込みました。\n",
    "\n",
    "これで私たちのGPTは、ユーザーのクエリに回答するための情報を取得できるようになり、データに対してより正確でカスタマイズされた応答が可能になりました。以下がGPTの動作例です："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![azure-rag-quickstart-gpt.png](../../../../images/azure-rag-quickstart-gpt.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
