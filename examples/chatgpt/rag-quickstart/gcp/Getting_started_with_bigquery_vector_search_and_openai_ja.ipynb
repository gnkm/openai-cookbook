{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP BigQueryとGCP Functions、ChatGPTのGPTアクションを使用した構成\n",
    "\n",
    "このノートブックでは、OpenAI埋め込みを使用してGoogle Cloud BigQueryをベクトル検索機能付きデータベースとして使用し、その上にGoogle Cloud Functionを作成してChatGPTのカスタムGPTに接続する手順を段階的に説明します。\n",
    "\n",
    "これは、Google Cloud Platform（GCP）内に含まれるRAGインフラストラクチャを設定し、ChatGPTなどの他のプラットフォームと統合するためのエンドポイントとして公開したい顧客向けのソリューションとなります。\n",
    "\n",
    "Google Cloud BigQueryは、Googleのインフラストラクチャの処理能力を使用して超高速SQLクエリを可能にする、完全管理型のサーバーレスデータウェアハウスです。開発者は大規模なデータセットを簡単に保存・分析できます。\n",
    "\n",
    "Google Cloud Functionsは、サーバーやランタイム環境を管理することなく、クラウドイベントに応答する小さな単一目的の関数を作成できる、軽量でイベントベースの非同期コンピューティングソリューションです。\n",
    "\n",
    "## 前提条件：\n",
    "\n",
    "このクックブックを実行するには、以下が必要です：\n",
    "- アクセス権限のあるGCPプロジェクト\n",
    "- BigQueryデータセットとGoogle Cloud Functionを作成する権限を持つGCPユーザー\n",
    "- [GCP CLI](https://cloud.google.com/sdk/docs/downloads-interactive)がインストールされ、接続されていること\n",
    "- OpenAI APIキー\n",
    "- ChatGPT Plus、Teams、またはEnterpriseサブスクリプション\n",
    "\n",
    "## アーキテクチャ\n",
    "\n",
    "以下は、このソリューションのアーキテクチャ図です。段階的に説明していきます：\n",
    "\n",
    "![bigquery-rag-architecture.png](../../../../images/bigquery_rag_architecture.png)\n",
    "\n",
    "## 目次\n",
    "\n",
    "1. **[環境のセットアップ](#set-up-environment)** 必要なライブラリのインストールとインポート、GCP設定の構成により環境をセットアップします。含まれる内容：\n",
    "    - [必要なライブラリのインストールとインポート](#install-and-import-required-libraries)\n",
    "    - [GCPプロジェクトの設定](#configure-gcp-project)\n",
    "    - [OpenAI設定の構成](#configure-openai-settings)\n",
    "\n",
    "2. **[データの準備](#prepare-data)** ドキュメントを埋め込み、追加のメタデータを取得してアップロード用のデータを準備します。この例では、OpenAIのドキュメントのサブセットをサンプルデータとして使用します。\n",
    "\n",
    "3. **[ベクトル検索付きBigQueryテーブルの作成](#create-bigquery-table-with-vector-search)**  \n",
    "準備したデータをアップロードするBigQueryテーブルを作成します。含まれる内容：\n",
    "\n",
    "    - [データセットの作成](#create-bigquery-dataset)：BigQueryでデータセットを作成する手順。\n",
    "    - [テーブルの作成とデータのアップロード](#creating-table-and-upload-data)：BigQueryでテーブルを作成する手順。\n",
    "\n",
    "4. **[GCP Functionの作成](#create-gcp-function)** gcloud CLIと事前に計算された環境変数を使用\n",
    "\n",
    "5. **[ChatGPTのカスタムGPTへの入力](#input-in-a-custom-gpt-in-chatgpt)** BigQueryの埋め込みデータに対して検索を実行：\n",
    "\n",
    "    - [ベクトル検索](#test-search)：ベクトルベースの検索クエリを実行する手順。\n",
    "    - [メタデータフィルタリング検索](#perform-search-with-metadata-filtering)：メタデータフィルタリングを実行する手順。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 環境のセットアップ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なライブラリのインストールとインポート\n",
    "以下のライブラリは、標準Pythonライブラリ、サードパーティライブラリ、およびGCP関連ライブラリに分類できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q google-auth\n",
    "! pip install -q openai\n",
    "! pip install -q pandas\n",
    "! pip install -q google-cloud-functions\n",
    "! pip install -q python-dotenv\n",
    "! pip install -q pyperclip\n",
    "! pip install -q PyPDF2\n",
    "! pip install -q tiktoken\n",
    "! pip install -q google-cloud-bigquery\n",
    "! pip install -q pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import json  \n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "from itertools import islice\n",
    "import concurrent.futures\n",
    "import yaml\n",
    "\n",
    "# Third-Party Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import pyperclip\n",
    "\n",
    "# OpenAI Libraries\n",
    "from openai import OpenAI\n",
    "\n",
    "# Google Cloud Identity and Credentials\n",
    "from google.auth import default\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import functions_v1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCPプロジェクトの設定\n",
    "\n",
    "まだセットアップされていない場合は、GCP CLIをインストールし、GCPに認証を行い、デフォルトプロジェクトを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gcloud to PATH\n",
    "os.environ['PATH'] += os.pathsep + os.path.expanduser('~/google-cloud-sdk/bin')\n",
    "\n",
    "# Verify gcloud is in PATH\n",
    "! gcloud --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"<insert_project_id>\"  # Replace with your actual project ID\n",
    "! gcloud config set project {project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud services enable cloudfunctions.googleapis.com\n",
    "! gcloud services enable cloudbuild.googleapis.com\n",
    "! gcloud services enable bigquery.googleapis.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI設定の構成\n",
    "\n",
    "このセクションでは、OpenAIの認証設定について説明します。このセクションを進める前に、OpenAI APIキーを取得していることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as an env var>\") # Saving this as a variable to reference in function app in later step\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "embeddings_model = \"text-embedding-3-small\" # We'll use this by default, but you can change to your text-embedding-3-large if desired"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベクター検索機能を持つGCP BigQueryの設定\n",
    "\n",
    "このセクションでは、BigQueryでデータセットを作成し、埋め込みとベクター検索に使用される浮動小数点のベクターを保存する方法について説明します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth import default\n",
    "\n",
    "# Use default credentials\n",
    "credentials, project_id = default()\n",
    "region = \"us-central1\" # e.g: \"us-central1\"\n",
    "print(\"Default Project ID:\", project_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備\n",
    "oai_docsフォルダ内のOpenAI docsの数ページを埋め込み、保存します。まず各ページを埋め込み、CSVに追加し、そのCSVを使用してインデックスにアップロードします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[このクックブック](khttps://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb)で紹介されているいくつかの技術を使用します。これは、セクション、画像/グラフ/図表を説明するためのビジョンモデルの使用、長い文書のチャンク間でのテキストの重複などの変数を考慮せずに、テキストを埋め込む簡単な方法です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8191トークンのコンテキストを超える長いテキストファイルを処理するために、チャンクの埋め込みを個別に使用するか、または平均化（各チャンクのサイズで重み付け）などの方法で組み合わせることができます。\n",
    "\n",
    "Pythonの公式クックブックから、シーケンスをチャンクに分割する関数を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n",
    "    # batched('ABCDEFG', 3) --> ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError('n must be at least one')\n",
    "    it = iter(iterable)\n",
    "    while (batch := tuple(islice(it, n))):\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、文字列をトークンにエンコードし、それをチャンクに分割する関数を定義します。OpenAIによる高速なオープンソーストークナイザーであるtiktokenを使用します。\n",
    "\n",
    "Tiktokenを使ったトークンの数え方について詳しく知りたい場合は、[このクックブック](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)をご確認ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_tokens(text, chunk_length, encoding_name='cl100k_base'):\n",
    "    # Get the encoding object for the specified encoding name. OpenAI's tiktoken library, which is used in this notebook, currently supports two encodings: 'bpe' and 'cl100k_base'. The 'bpe' encoding is used for GPT-3 and earlier models, while 'cl100k_base' is used for newer models like GPT-4.\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    # Encode the input text into tokens\n",
    "    tokens = encoding.encode(text)\n",
    "    # Create an iterator that yields chunks of tokens of the specified length\n",
    "    chunks_iterator = batched(tokens, chunk_length)\n",
    "    # Yield each chunk from the iterator\n",
    "    yield from chunks_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、入力テキストが最大コンテキスト長よりも長い場合でも、入力トークンをチャンクに分割し、各チャンクを個別に埋め込むことで、埋め込みリクエストを安全に処理する関数を書くことができます。`average`フラグを`True`に設定すると、チャンク埋め込みの重み付き平均を返し、`False`に設定すると、変更されていないチャンク埋め込みのリストを単純に返します。\n",
    "\n",
    "> 注意：ここでは他の手法も使用できます：\n",
    "> - GPT-4oを使用して画像/チャートの説明を埋め込み用にキャプチャする\n",
    "> - 段落やセクションに基づいてチャンクに分割する\n",
    "> - 各記事についてより詳細なメタデータを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING='cl100k_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text, model):\n",
    "    # Generate embeddings for the provided text using the specified model\n",
    "    embeddings_response = openai_client.embeddings.create(model=model, input=text)\n",
    "    # Extract the embedding data from the response\n",
    "    embedding = embeddings_response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "def len_safe_get_embedding(text, model=embeddings_model, max_tokens=EMBEDDING_CTX_LENGTH, encoding_name=EMBEDDING_ENCODING):\n",
    "    # Initialize lists to store embeddings and corresponding text chunks\n",
    "    chunk_embeddings = []\n",
    "    chunk_texts = []\n",
    "    # Iterate over chunks of tokens from the input text\n",
    "    for chunk in chunked_tokens(text, chunk_length=max_tokens, encoding_name=encoding_name):\n",
    "        # Generate embeddings for each chunk and append to the list\n",
    "        chunk_embeddings.append(generate_embeddings(chunk, model=model))\n",
    "        # Decode the chunk back to text and append to the list\n",
    "        chunk_texts.append(tiktoken.get_encoding(encoding_name).decode(chunk))\n",
    "    # Return the list of chunk embeddings and the corresponding text chunks\n",
    "    return chunk_embeddings, chunk_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、ドキュメントに関する追加のメタデータをキャプチャするヘルパー関数を定義できます。この例では、後でメタデータフィルターで使用するために、カテゴリのリストから選択します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['authentication','models','techniques','tools','setup','billing_limits','other']\n",
    "\n",
    "def categorize_text(text, categories):\n",
    "\n",
    "    # Create a prompt for categorization\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"\"\"You are an expert in LLMs, and you will be given text that corresponds to an article in OpenAI's documentation.\n",
    "         Categorize the document into one of these categories: {', '.join(categories)}. Only respond with the category name and nothing else.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    try:\n",
    "        # Call the OpenAI API to categorize the text\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        # Extract the category from the response\n",
    "        category = response.choices[0].message.content\n",
    "        return category\n",
    "    except Exception as e:\n",
    "        print(f\"Error categorizing text: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、`oai_docs`フォルダ内の.txtファイルを処理するためのヘルパー関数を定義できます。ご自身のデータでも自由にお使いください。これは.txtファイルと.pdfファイルの両方をサポートしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Initialize the PDF reader\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    # Iterate through each page in the PDF and extract text\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def process_file(file_path, idx, categories, embeddings_model):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"Processing file {idx + 1}: {file_name}\")\n",
    "    \n",
    "    # Read text content from .txt files\n",
    "    if file_name.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "    # Extract text content from .pdf files\n",
    "    elif file_name.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    \n",
    "    title = file_name\n",
    "    # Generate embeddings for the title\n",
    "    title_vectors, title_text = len_safe_get_embedding(title, embeddings_model)\n",
    "    print(f\"Generated title embeddings for {file_name}\")\n",
    "    \n",
    "    # Generate embeddings for the content\n",
    "    content_vectors, content_text = len_safe_get_embedding(text, embeddings_model)\n",
    "    print(f\"Generated content embeddings for {file_name}\")\n",
    "    \n",
    "    category = categorize_text(' '.join(content_text), categories)\n",
    "    print(f\"Categorized {file_name} as {category}\")\n",
    "    \n",
    "    # Prepare the data to be appended\n",
    "    data = []\n",
    "    for i, content_vector in enumerate(content_vectors):\n",
    "        data.append({\n",
    "            \"id\": f\"{idx}_{i}\",\n",
    "            \"vector_id\": f\"{idx}_{i}\",\n",
    "            \"title\": title_text[0],\n",
    "            \"text\": content_text[i],\n",
    "            \"title_vector\": json.dumps(title_vectors[0]),  # Assuming title is short and has only one chunk\n",
    "            \"content_vector\": json.dumps(content_vector),\n",
    "            \"category\": category\n",
    "        })\n",
    "        print(f\"Appended data for chunk {i + 1}/{len(content_vectors)} of {file_name}\")\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、このヘルパー関数を使用してOpenAIドキュメントを処理します。以下の`process_files`でフォルダを変更することで、独自のデータを使用するように自由に更新してください。\n",
    "\n",
    "なお、これは選択したフォルダ内のドキュメントを並行処理するため、txtファイルを使用する場合は30秒未満で完了し、PDFを使用する場合は少し長くかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Customize the location below if you are using different data besides the OpenAI documentation. Note that if you are using a different dataset, you will need to update the categories list as well.\n",
    "folder_name = \"../../../data/oai_docs\"\n",
    "\n",
    "files = [os.path.join(folder_name, f) for f in os.listdir(folder_name) if f.endswith('.txt') or f.endswith('.pdf')]\n",
    "data = []\n",
    "\n",
    "# Process each file concurrently\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_file, file_path, idx, categories, embeddings_model): idx for idx, file_path in enumerate(files)}\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            data.extend(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {str(e)}\")\n",
    "\n",
    "# Write the data to a CSV file\n",
    "csv_file = os.path.join(\"..\", \"embedded_data.csv\")\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = [\"id\", \"vector_id\", \"title\", \"text\", \"title_vector\", \"content_vector\",\"category\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in data:\n",
    "        writer.writerow(row)\n",
    "        print(f\"Wrote row with id {row['id']} to CSV\")\n",
    "\n",
    "# Convert the CSV file to a Dataframe\n",
    "article_df = pd.read_csv(\"../embedded_data.csv\")\n",
    "# Read vectors from strings back into a list using json.loads\n",
    "article_df[\"title_vector\"] = article_df.title_vector.apply(json.loads)\n",
    "article_df[\"content_vector\"] = article_df.content_vector.apply(json.loads)\n",
    "article_df[\"vector_id\"] = article_df[\"vector_id\"].apply(str)\n",
    "article_df[\"category\"] = article_df[\"category\"].apply(str)\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、ベクターデータベースにアップロードできる6つの列を持つ`embedded_data.csv`ファイルが完成しました！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search機能付きBigQueryテーブルの作成\n",
    "\n",
    "## BigQueryデータセットの作成\n",
    "\n",
    "Google SDKを活用して、「oai_docs」という名前のデータセットと「embedded_data」というテーブル名を作成しますが、これらの変数は自由に変更してください（リージョンも変更可能です）。\n",
    "\n",
    "*注意: BigQueryインデックスは作成しません。これはベクトル検索のパフォーマンスを向上させる可能性がありますが、そのようなインデックスにはデータセット内に1,000行以上が必要であり、この例では該当しません。ただし、独自のユースケースでは自由に活用してください。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigquery table\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import Conflict\n",
    "\n",
    "# Define the dataset ID (project_id.dataset_id)\n",
    "raw_dataset_id = 'oai_docs'\n",
    "dataset_id = project_id + '.' + raw_dataset_id\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Construct a full Dataset object to send to the API\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "# Specify the geographic location where the dataset should reside\n",
    "dataset.location = \"US\"\n",
    "\n",
    "# Send the dataset to the API for creation\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print(f\"Created dataset {client.project}.{dataset.dataset_id}\")\n",
    "except Conflict:\n",
    "    print(f\"dataset {dataset.dataset_id } already exists\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file, properly handling multiline fields\n",
    "csv_file_path = \"../embedded_data.csv\"\n",
    "df = pd.read_csv(csv_file_path, engine='python', quotechar='\"', quoting=1)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テーブルの作成とデータのアップロード\n",
    "\n",
    "属性名と型を指定してテーブルを作成します。単一行に対してfloatのベクトルを格納できる'content_vector'属性に注目してください。これをベクトル検索で使用します。\n",
    "\n",
    "このコードは、以前に作成したCSVをループして、Bigqueryに行を挿入します。\n",
    "このコードを複数回実行すると、同一の行が複数回挿入され、検索時の精度が低下する可能性があります（IDに一意性制約を設けるか、毎回DBをクリーンアップすることをお勧めします）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file, properly handling multiline fields\n",
    "dataset_id = project_id + '.' + raw_dataset_id\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "csv_file_path = \"../embedded_data.csv\"\n",
    "df = pd.read_csv(csv_file_path, engine='python', quotechar='\"', quoting=1)\n",
    "\n",
    "# Preprocess the data to ensure content_vector is correctly formatted\n",
    "# removing last and first character which are brackets [], comma splitting and converting to float\n",
    "def preprocess_content_vector(row):\n",
    "    row['content_vector'] = [float(x) for x in row['content_vector'][1:-1].split(',')]\n",
    "    return row\n",
    "\n",
    "# Apply preprocessing to the dataframe\n",
    "df = df.apply(preprocess_content_vector, axis=1)\n",
    "\n",
    "# Define the schema of the final table\n",
    "final_schema = [\n",
    "    bigquery.SchemaField(\"id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"vector_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"title\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"text\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"title_vector\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"content_vector\", \"FLOAT64\", mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"category\", \"STRING\"),\n",
    "]\n",
    "\n",
    "# Define the final table ID\n",
    "raw_table_id = 'embedded_data'\n",
    "final_table_id = f'{dataset_id}.' + raw_table_id\n",
    "\n",
    "# Create the final table object\n",
    "final_table = bigquery.Table(final_table_id, schema=final_schema)\n",
    "\n",
    "# Send the table to the API for creation\n",
    "final_table = client.create_table(final_table, exists_ok=True)  # API request\n",
    "print(f\"Created final table {project_id}.{final_table.dataset_id}.{final_table.table_id}\")\n",
    "\n",
    "# Convert DataFrame to list of dictionaries for BigQuery insertion\n",
    "rows_to_insert = df.to_dict(orient='records')\n",
    "\n",
    "# Upload data to the final table\n",
    "errors = client.insert_rows_json(f\"{final_table.dataset_id}.{final_table.table_id}\", rows_to_insert)  # API request\n",
    "\n",
    "if errors:\n",
    "    print(f\"Encountered errors while inserting rows: {errors}\")\n",
    "else:\n",
    "    print(f\"Successfully loaded data into {dataset_id}:{final_table_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検索のテスト\n",
    "データがアップロードされたので、以下でピュアなベクトル類似度検索とメタデータフィルタリングの両方をローカルでテストして、期待通りに動作することを確認します。\n",
    "\n",
    "ピュアなベクトル検索とメタデータフィルタリングの両方をテストできます。\n",
    "\n",
    "以下のクエリはピュアなベクトル検索で、カテゴリでフィルタリングを行いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What model should I use to embed?\"\n",
    "category = \"models\"\n",
    "\n",
    "embedding_query = generate_embeddings(query, embeddings_model)\n",
    "embedding_query_list = ', '.join(map(str, embedding_query))\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH search_results AS (\n",
    "  SELECT query.id AS query_id, base.id AS base_id, distance\n",
    "  FROM VECTOR_SEARCH(\n",
    "    TABLE oai_docs.embedded_data, 'content_vector',\n",
    "    (SELECT ARRAY[{embedding_query_list}] AS content_vector, 'query_vector' AS id),\n",
    "    top_k => 2, distance_type => 'COSINE', options => '{{\"use_brute_force\": true}}')\n",
    ")\n",
    "SELECT sr.query_id, sr.base_id, sr.distance, ed.text, ed.title\n",
    "FROM search_results sr\n",
    "JOIN oai_docs.embedded_data ed ON sr.base_id = ed.id\n",
    "ORDER BY sr.distance ASC\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()  # Wait for the job to complete\n",
    "\n",
    "for row in results:\n",
    "    print(f\"query_id: {row['query_id']}, base_id: {row['base_id']}, distance: {row['distance']}, text_truncated: {row['text'][0:100]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メタデータフィルタリングを使用した検索の実行\n",
    "メタデータフィルタリングにより、ベクトル検索の意味的に最も近い結果を取得することに加えて、特定の属性を持つ結果に絞り込むことができます。\n",
    "\n",
    "提供されたコードスニペットは、メタデータフィルタリングを使用してクエリを実行する方法を示しています："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What model should I use to embed?\"\n",
    "category = \"models\"\n",
    "\n",
    "embedding_query = generate_embeddings(query, embeddings_model)\n",
    "embedding_query_list = ', '.join(map(str, embedding_query))\n",
    "\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH search_results AS (\n",
    "  SELECT query.id AS query_id, base.id AS base_id, distance\n",
    "  FROM VECTOR_SEARCH(\n",
    "    (SELECT * FROM oai_docs.embedded_data WHERE category = '{category}'), \n",
    "    'content_vector',\n",
    "    (SELECT ARRAY[{embedding_query_list}] AS content_vector, 'query_vector' AS id),\n",
    "    top_k => 4, distance_type => 'COSINE', options => '{{\"use_brute_force\": true}}')\n",
    ")\n",
    "SELECT sr.query_id, sr.base_id, sr.distance, ed.text, ed.title, ed.category\n",
    "FROM search_results sr\n",
    "JOIN oai_docs.embedded_data ed ON sr.base_id = ed.id\n",
    "ORDER BY sr.distance ASC\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()  # Wait for the job to complete\n",
    "\n",
    "for row in results:\n",
    "    print(f\"category: {row['category']}, title: {row['title']}, base_id: {row['base_id']}, distance: {row['distance']}, text_truncated: {row['text'][0:100]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP関数の作成\n",
    "\n",
    "## 変数のエクスポート\n",
    "\n",
    "このフォルダ内の`main.py`にある関数をデプロイします（[こちら](https://github.com/openai/openai-cookbook/blob/main/examples/chatgpt/rag-quickstart/gcp/main.py)でも利用可能）。\n",
    "\n",
    "最初のステップとして、テーブル/データセットをターゲットにし、OpenAIのAPIを使用してEmbeddingsを生成するための変数をエクスポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the environment variables (they were used previously and are just retrieved)\n",
    "env_variables = {\n",
    "    'OPENAI_API_KEY': openai_api_key,\n",
    "    'EMBEDDINGS_MODEL': embeddings_model,\n",
    "    'PROJECT_ID': project_id,\n",
    "    'DATASET_ID': raw_dataset_id,\n",
    "    'TABLE_ID': raw_table_id\n",
    "}\n",
    "\n",
    "# Write the environment variables to a YAML file\n",
    "with open('env.yml', 'w') as yaml_file:\n",
    "    yaml.dump(env_variables, yaml_file, default_flow_style=False)\n",
    "\n",
    "print(\"env.yml file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数のデプロイ\n",
    "\n",
    "現在のプロジェクト用に「openai_docs_search」というGoogle関数を作成します。そのために、以下のCLIコマンドを実行し、先ほど作成した環境変数を活用します。この関数は認証なしでどこからでも呼び出すことができることに注意してください。本番環境では使用しないか、追加の認証メカニズムを追加してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud functions deploy openai_docs_search \\\n",
    "  --runtime python39 \\\n",
    "  --trigger-http \\\n",
    "  --allow-unauthenticated \\\n",
    "  --env-vars-file env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPTのカスタムGPTでの入力\n",
    "\n",
    "Vector Search Indexにクエリを実行するGCP Functionができたので、これをGPT Actionとして設定しましょう！\n",
    "\n",
    "GPTについてのドキュメントは[こちら](https://openai.com/index/introducing-gpts/)、GPT Actionsについては[こちら](https://platform.openai.com/docs/actions)を参照してください。以下をGPTの指示として、またGPT ActionのOpenAPI仕様として使用してください。\n",
    "\n",
    "## OpenAPI仕様の作成\n",
    "\n",
    "以下はサンプルのOpenAPI仕様です。下記のブロックを実行すると、機能的な仕様がクリップボードにコピーされ、GPT Actionに貼り付けることができます。\n",
    "\n",
    "なお、これはデフォルトでは認証機能がありませんが、GCPのドキュメント[こちら](https://cloud.google.com/functions/docs/securing/authenticating)に従ってGCP Functionsに認証を設定することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = f\"\"\"\n",
    "openapi: 3.1.0\n",
    "info:\n",
    "  title: OpenAI API documentation search\n",
    "  description: API to perform a semantic search over OpenAI APIs\n",
    "  version: 1.0.0\n",
    "servers:\n",
    "  - url: https://{region}-{project_id}.cloudfunctions.net\n",
    "    description: Main (production) server\n",
    "paths:\n",
    "  /openai_docs_search:\n",
    "    post:\n",
    "      operationId: openai_docs_search\n",
    "      summary: Perform a search\n",
    "      description: Returns search results for the given query parameters.\n",
    "      requestBody:\n",
    "        required: true\n",
    "        content:\n",
    "          application/json:\n",
    "            schema:\n",
    "              type: object\n",
    "              properties:\n",
    "                query:\n",
    "                  type: string\n",
    "                  description: The search query string\n",
    "                top_k:\n",
    "                  type: integer\n",
    "                  description: Number of top results to return. Maximum is 3.\n",
    "                category:\n",
    "                  type: string\n",
    "                  description: The category to filter on, on top of similarity search (used for metadata filtering). Possible values are {categories}.\n",
    "      responses:\n",
    "        '200':\n",
    "          description: A JSON response with the search results\n",
    "          content:\n",
    "            application/json:\n",
    "              schema:\n",
    "                type: object\n",
    "                properties:\n",
    "                  items:\n",
    "                    type: array\n",
    "                    items:\n",
    "                      type: object\n",
    "                      properties:\n",
    "                        text:\n",
    "                          type: string\n",
    "                          example: \"Learn how to turn text into numbers, unlocking use cases like search...\"\n",
    "                        title:\n",
    "                          type: string\n",
    "                          example: \"embeddings.txt\"\n",
    "                        distance:\n",
    "                          type: number\n",
    "                          format: float\n",
    "                          example: 0.484939891778730\n",
    "                        category:\n",
    "                          type: string\n",
    "                          example: \"models\"\n",
    "\"\"\"\n",
    "print(spec)\n",
    "pyperclip.copy(spec)\n",
    "print(\"OpenAPI spec copied to clipboard\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT指示の作成\n",
    "\n",
    "指示は必要に応じて自由に修正してください。プロンプトエンジニアリングのヒントについては、[こちら](https://platform.openai.com/docs/guides/prompt-engineering)のドキュメントをご確認ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = f'''\n",
    "You are an OpenAI docs assistant. You have an action in your knowledge base where you can make a POST request to search for information. The POST request should always include: {{\n",
    "    \"query\": \"<user_query>\",\n",
    "    \"k_\": <integer>,\n",
    "    \"category\": <string, but optional>\n",
    "}}. Your goal is to assist users by performing searches using this POST request and providing them with relevant information based on the query.\n",
    "\n",
    "You must only include knowledge you get from your action in your response.\n",
    "The category must be from the following list: {categories}, which you should determine based on the user's query. If you cannot determine, then do not include the category in the POST request.\n",
    "'''\n",
    "pyperclip.copy(instructions)\n",
    "print(\"GPT Instructions copied to clipboard\")\n",
    "print(instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "\n",
    "以下の手順により、GCP BigQuery Vector SearchとChatGPTのGPT Actionsの統合に成功しました：\n",
    "\n",
    "1. OpenAIの埋め込みを使用してドキュメントを埋め込み、gpt-4oを使用して追加のメタデータを付与しました。\n",
    "2. そのデータをGCP BigQuery（生データと埋め込みベクトル）にアップロードしました。\n",
    "3. それらを取得するためのエンドポイントをGCP Functions上に作成しました。\n",
    "4. カスタムGPTに組み込みました。\n",
    "\n",
    "私たちのGPTは、ユーザーのクエリに答えるための情報を取得できるようになり、データに対してより正確でカスタマイズされた回答が可能になりました。以下がGPTの動作例です："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gcp-rag-quickstart-gpt.png](../../../../images/gcp_rag_quickstart_gpt.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
