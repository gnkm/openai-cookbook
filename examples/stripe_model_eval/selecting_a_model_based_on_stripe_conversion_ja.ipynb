{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stripe コンバージョンに基づくモデル選択：スタートアップのための実践的評価\n",
    "\n",
    "## 概要\n",
    "\n",
    "最適なモデルは、あなたのビジネス目標によって決まります。多くのスタートアップは、オフライン評価や公開ベンチマークに基づいて大規模言語モデル（LLM）を選択しています。しかし、ベンチマークで高いスコアを獲得するモデルが、必ずしもユーザーに支払い、購読、または製品の継続使用を促すとは限りません。紙面上では強力に見えるモデルも、実際のビジネス成果で測定すると期待を下回ることがあります。\n",
    "\n",
    "このガイドでは、スタートアップにとって最も重要なビジネス成果の一つに基づく評価アプローチを説明します：人々があなたの製品にお金を払う意思があるかどうかです。\n",
    "\n",
    "HyperWriteのモデル評価プロセスを通じて、実際の支払いコンバージョン、特に一回限りの購入や月次継続収益（MRR）サブスクリプションのStripe支払いに焦点を当てて説明します。コンバージョン率の改善、またはより安価なモデルに切り替えながらコンバージョン率を維持することが目標の場合、この評価例は有用なパターンとなるでしょう。\n",
    "\n",
    "## 前提条件と範囲\n",
    "\n",
    "このガイドをあなたのビジネスに適用するには、以下が必要です：\n",
    "\n",
    "- **決済プロセッサー。** この例ではStripeを使用していますが、わずかな調整で他の決済プロバイダーでも同じアプローチを使用できます。\n",
    "- **意味のあるシグナルを得るのに十分なユーザー数。** テストバリアントごとに少なくとも1,000人のユーザーを目標にしてください。統計的有意性を高めるには、より多くのユーザーが必要です。\n",
    "- **コンバージョンイベントを持つAI搭載製品。** 私たちはLLMアプリケーションを使用し、コンバージョンイベントは支払いです。同じテストアプローチは、音声、動画、その他のモダリティを中心に構築されたアプリにも適用されます。\n",
    "\n",
    "## 実際の目標に基づくモデル選択\n",
    "\n",
    "HyperWriteは、AI搭載のライティングツールとリサーチアシスタントを構築しています。同社の中核サービスは、高度なリサーチ機能を持つライティングアシスタントです。\n",
    "\n",
    "オフラインベンチマークは、HyperWriteにとって最も重要なことを予測できませんでした：ユーザーがライティングアシスタントと関わり、サブスクリプションを購入して製品を継続使用するかどうかです。HyperWriteチームは関心のある成果（コンバージョン）に焦点を移し、Stripeコンバージョン率を比較する実世界のA/Bテストに基づいてAIモデル間の選択を始めました。\n",
    "\n",
    "## スタートアップにとって重要なもの：コンバージョン\n",
    "\n",
    "多くのスタートアップでは、ユーザーが製品にサインアップし、継続使用することが目標です。科学者が数十年間頼りにしてきた同じ統計手法を使用した古典的なA/Bテストを使って、モデル評価プロセスを設計できます：\n",
    "\n",
    "- 新規ユーザーをバッチ化し、各バッチに異なるAIモデルを提供します。\n",
    "- ユーザーがアップグレードプロンプトに遭遇するタイミングを標準化するため、ユーザーがアシスタントに設定された数のメッセージを送信した後（意味のあるアップグレードの瞬間を作るのに十分な数）に、一貫したレート制限が適用されます。\n",
    "- 各グループの有料サブスクリプション（Stripe経由）へのコンバージョンを追跡します。\n",
    "\n",
    "ユーザーのモデルへのランダム割り当てと他の要因（オンボーディング、機能、プロンプトなど）の制御により、コンバージョン率の違いを外部変動ではなく、テストされているモデルに帰属させることができます。統計は、観察された違いが偶然によるものである可能性が低いという信頼性を提供します。\n",
    "\n",
    "真の非ランダムな改善が見つかった場合（例：あるモデルがより高いコンバージョン率をもたらす）、その影響は具体的です：Stripeコンバージョンの増加、より多くの有料ユーザー、そしてモデルがより効率的であれば、しばしばコストの削減です。\n",
    "\n",
    "## モデル選択のためのA/Bテストの方法\n",
    "\n",
    "A/Bテストは、モデル選択のための実世界評価ツールとして機能できます。ユーザーをランダムにグループに分け、各グループに異なる体験（ここでは異なるAIモデル）を提供し、どのグループが主要指標（この場合はStripeコンバージョン）でより良いパフォーマンスを示すかを観察します。\n",
    "\n",
    "### 基本：あるモデル対別のモデル\n",
    "\n",
    "標準的なセットアップには、「コントロール」（現在のモデル）と「バリアント」（挑戦者）が含まれます。ユーザーはランダムにどちらかのグループに割り当てられます。テストがモデルの効果を分離することを確実にするため、他のすべては同じに保たれます：オンボーディング、機能、プロンプト、そしてコンバージョンの機会。事前に決められた期間またはユーザー数の後、コンバージョン率が比較されます：モデルAまたはモデルBを使用した時により多くの人が支払ったでしょうか？\n",
    "\n",
    "### 実世界の例：HyperWriteのモデル交換テスト\n",
    "\n",
    "HyperWriteの目標は、収益化を大幅に減少させることなく、より安価なLLMを展開することでした。これは非劣性シナリオでした：新しいモデルがコントロールより有意に悪くないことを確実にすることに関心がありました。コスト削減を念頭に置いて、片側非劣性テストが設計されました。\n",
    "\n",
    "- **テスト焦点：** Stripeコンバージョンを害することなくコスト削減。\n",
    "- **設計：** 片側、二比率Z検定（新しいモデルが悪いかどうかを検出することに焦点）。\n",
    "- **アルファ（第一種過誤率）：** 0.15（つまり85%信頼度）。このスタートアップでは、非常に厳格な有意性閾値よりも反復速度が優先されました。\n",
    "- **検出力：** 0.60（意味のある低下を捉えるのに十分、トラフィック制約とのバランス）。\n",
    "- **最小検出可能効果（MDE）：** コンバージョンの30%低下—これより小さい低下は、コスト削減が正当化されれば「十分に近い」と見なされます。\n",
    "- **母集団：** 定義された期間の新規サインアップのセグメント、サインアップ時に`user_id`でランダム化。\n",
    "- **トリガー：** ユーザーがメッセージを送信し、アップグレードペイウォールに到達し、Stripeチェックアウト経由でコンバージョンする可能性があります。\n",
    "\n",
    "## パラメータの設定：何を勝利とするか？\n",
    "\n",
    "観察されたすべての違いが意味があるわけではありません—一部の違いは偶然に起こります。A/Bテストは、実際の効果をランダムノイズから分離するのに役立ちます。ここで一般的に使用される統計ツールは「二比率Z検定」で、二つのグループ間のコンバージョン率の違いが統計的に有意と見なされるほど大きいかどうかをチェックします。\n",
    "\n",
    "このテストにはいくつかのバリエーションがあります：\n",
    "\n",
    "- **片側検定：** 新しいモデルがコントロールより良いか（または設計によっては悪くないか）をチェック\n",
    "- **両側検定：** 上下どちらの方向でも違いをチェック\n",
    "- **多変量テスト（A/B/n）：** 三つ以上のモデルを同時に比較\n",
    "\n",
    "選択はあなたの目標によって決まります。コンバージョンの明確な向上が必要な場合、改善を探す片側検定で十分かもしれません。悪くないがより安価なモデルを採用する意思がある場合、新しいモデルが有意に悪くないことを確実にするために非劣性（片側）テストを設計するかもしれません。\n",
    "\n",
    "### 主要用語\n",
    "\n",
    "- **第一種過誤（偽陽性）：** 効果がないのに効果があると結論すること\n",
    "- **第二種過誤（偽陰性）：** 実際の効果を検出できないこと\n",
    "- **アルファ（α）：** 第一種過誤の許容リスク（しばしば0.05、つまり5%に設定）\n",
    "- **検出力：** 真の効果を検出する確率（80%が一般的な目標）\n",
    "\n",
    "### 例：実際のモデルテストの実行\n",
    "\n",
    "現在のモデル（コントロール）と新しいバリアント（モデルX）の間で選択することを考えてみましょう。モデルXがコントロールよりも良くコンバージョンするかどうかを見るために片側二比率Z検定を実行するとします。α = 0.05を設定し、ベースラインコンバージョン率と希望する最小検出可能効果で検出力計算を行った後、グループあたり約1,500人のユーザーが約75%の検出力を提供することを決定します—より速い方向性のある洞察を可能にする妥協案です。\n",
    "\n",
    "両グループが必要なサンプルサイズに達した後、データは次のようになるかもしれません：\n",
    "\n",
    "| グループ                    | 割り当てユーザー | コンバージョン | コンバージョン率 | p値   | 統計的有意？ | 勝者？ | 第一種過誤保護？ | 第二種過誤保護？ |\n",
    "|----------------------------|-----------------|---------------|-----------------|-------|-------------|-------|----------------|----------------|\n",
    "| コントロール（現在のモデル）  | 1500            | 15            | 1.0%            | --    | 基準        | いいえ | はい           | はい           |\n",
    "| モデルX（バリアント）        | 1500            | 30            | 2.0%            | 0.012 | はい        | はい  | はい           | はい           |\n",
    "\n",
    "- **割り当てユーザー：** 各グループにランダムに配置されたユーザー数。\n",
    "- **コンバージョン：** 各グループでStripe経由で支払ったユーザー数。\n",
    "- **コンバージョン率：** コンバージョンを割り当てユーザーで割った値。\n",
    "- **p値：** 片側二比率Z検定の結果、モデルXの高い率が偶然によるものでない可能性を示す。\n",
    "- **統計的有意？：** p値があなたのアルファ（ここでは0.05）を下回るか？\n",
    "- **勝者？：** 統計的に有意な場合、モデルXが新しい勝者。\n",
    "- **第一種過誤保護？：** 偽陽性リスクをアルファ閾値内に保ったか？\n",
    "- **第二種過誤保護？：** サンプルサイズは実際の効果を検出するのに十分な検出力を与えたか？\n",
    "\n",
    "この実行では、モデルXのコンバージョン率はコントロールより1パーセントポイント高く（2.0% 対 1.0%）—100%の相対的増加です。p値0.012は0.05を大幅に下回るため、統計的に有意とマークします：モデルXが勝者です。75%の統計的検出力でサンプルサイズを計画したため、真の効果を見逃さなかった（第二種過誤）ことも確信できます。そして、アルファを0.05に設定したため、偽陽性（第一種過誤）のリスクは制御されています。\n",
    "\n",
    "### 実世界の例：HyperWriteのテストパラメータ\n",
    "\n",
    "HyperWriteは教科書的な95%信頼度と80%検出力をデフォルトにしませんでした。トラフィックは高価で、統計的確実性を最大化することは学習を遅くし、資本を消費する可能性があります。選択された85%信頼度と60%検出力は、小さな違いを過度に最適化することを避けながら、重要な低下（約30%の減少）を検出することを可能にしました。\n",
    "\n",
    "コンバージョン率は、テストが長く実行されるにつれて上昇する傾向があります。これらのテストでは、必要なサンプルサイズ（N）に達すると実行が停止されました。入ってくるトラフィックの一部のみが各テストアームに割り当てられ、大部分は実証済みのコントロール体験に残されました。\n",
    "\n",
    "### 多重性と比較の注意\n",
    "\n",
    "A/B/n（「多対一」）設計が使用されました：各候補モデル（GPT-4.1とGPT-4.1-mini）は本番コントロール（Claude 3.5 Sonnet）に対して評価されましたが、互いに直接比較されませんでした。\n",
    "\n",
    "ローンチ決定がバリアント固有だったため（「α = 0.15での独自の片側非劣性テストに合格した場合はアームを出荷、そうでなければ破棄」）、ファミリーワイズエラー率補正は適用されませんでした。これは小さなk、コントロール中心のテストでは標準的です。偽陽性リスクはローンチされた単一のアームにのみ適用され、Bonferroni型の分割を避けることで検出力が保たれます。\n",
    "\n",
    "### PythonでA/Bテストの有意性をチェックする方法\n",
    "\n",
    "A/Bテストの背後にある統計がどのように機能するかを正確に示すため、生のコンバージョン数を片側二比率Z検定（バリアントがコントロールより良い）を使用してp値に変換する10行のPythonスニペットを以下に示します。任意のPython REPL、Colab、またはノートブックに貼り付けて、実際の実験を実行する際に独自の数値に置き換えてください。\n",
    "\n",
    "```python\n",
    "# 片側二比率Z検定\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "conversions   = [30, 15]     # [バリアント, コントロール]\n",
    "sample_sizes  = [1500, 1500] # [バリアント, コントロール]\n",
    "\n",
    "z_stat, p_val = proportions_ztest(\n",
    "    conversions,\n",
    "    sample_sizes,\n",
    "    alternative=\"larger\"      # \"larger\" → バリアント > コントロール\n",
    ")\n",
    "\n",
    "print(f\"Z統計量 = {z_stat:.2f}\")\n",
    "print(f\"p値     = {p_val:.3f}\")    # → 0.012 (α = 0.05)\n",
    "```\n",
    "\n",
    "結果の読み方：\n",
    "- p値が **≤ 0.05** の場合、バリアントの高いコンバージョンは統計的に有意です—先に進んで出荷するか、より多くのデータを監視し続けてください。\n",
    "- **> 0.05** の場合、結果はランダムノイズの可能性があります—より多くのデータを収集するか、コントロールに固執してください。\n",
    "\n",
    "### 注意事項\n",
    "\n",
    "- **テール釣り / p-ハッキング：** 最初のユーザーが流入する前に片側対両側を決定してください；後で切り替えると第一種過誤（偽陽性）が膨らみます。\n",
    "- **低カウント：** どちらかのアームのコンバージョンが約10未満の場合、Z検定をFisherの正確検定またはWilson/Wald信頼区間に交換してください。\n",
    "- **早期覗き見：** α支出補正なしでデータを繰り返し見ることは偽陽性リスクを高めます。固定サンプルまたはグループ逐次設計を使用してください。\n",
    "- **ユーザー重複 / 汚染：** 同じユーザーIDが二つのアームに着地できないことを確認してください（例：ログアウト/ログインによる）。\n",
    "- **複数の挑戦者：** 多くのバリアントの中から単一の「最良」を選ぶ計画の場合、ファミリーワイズエラー（Bonferroni、Holm）を制御するか、多腕バンディットを使用してください。\n",
    "- **キャッシュとプロンプトドリフト：** 推論レイヤーが一つのモデルの応答を別のキャッシュに漏らさないことを確認し、アーム間でプロンプトを同一に保ってください。\n",
    "\n",
    "これらの落とし穴とそれらがどのように回避されるかについて詳しく学ぶには、Evan Millerの[\"How Not to Run an A/B Test\"](https://www.evanmiller.org/how-not-to-run-an-ab-test.html)をチェックしてください。\n",
    "\n",
    "### 重要なポイント\n",
    "\n",
    "A/Bテストはランディングページやボタンの色だけのものではありません—製品に適したLLMを選ぶために不可欠です。ワークフローの一部にすることで、コストのかかる間違いを避け、ユーザーが価値を置くもの：支払う価値のある製品に基づいたアップグレードを発見できます。\n",
    "\n",
    "## 実世界の例：HyperWriteのGPT-4.1によるコスト削減\n",
    "\n",
    "モデル価格は、機能が向上するにつれて増加することがよくあります。HyperWriteは数ヶ月間、現職（AnthropicのClaude 3.5 Sonnet）に匹敵し、コンバージョンやユーザー体験を害することなく、理想的にはより低コストでのモデルを探していました。いくつかのモデルがより悪いパフォーマンスを示した後、OpenAIのGPT-4.1が注目すべき結果を提供しました：より低い価格で現職のStripeコンバージョンに匹敵しました。\n",
    "\n",
    "Stripeコンバージョンでのバリアントの比較は以下の通りです：\n",
    "\n",
    "| バリアント                                    | 割り当て | コンバージョン | 率    | 必要N | %完了 | コンバージョン閾値（≤） | 悪い？ |\n",
    "|---------------------------------------------|--------:|-------------:|------:|------:|------:|----------------------:|:------:|\n",
    "| anthropic/claude-3.5-sonnet（コントロール） |    4550 |           42 | 0.92% |  3378 |  135% | —                     | —      |\n",
    "| openai/gpt-4.1（バリアント）                |    4513 |           58 | 1.29% |  3378 |  134% | 32                    | いいえ |\n",
    "| openai/gpt-4.1-mini（バリアント）           |    4557 |           45 | 0.99% |  3378 |  135% | 33                    | いいえ |\n",
    "\n",
    "- **バリアント：** モデル名（コントロールまたは挑戦者）。\n",
    "- **割り当て：** そのアームにランダムに配置されたユーザー数。\n",
    "- **コンバージョン：** アーム内でStripe経由で支払ったユーザー。\n",
    "- **率：** コンバージョンを割り当てで割った値。\n",
    "- **必要N：** 非劣性テストのために事前計算されたサンプルサイズ目標。\n",
    "- **%完了：** 割り当てを必要Nで割った値（目標への進捗）。\n",
    "- **コンバージョン閾値（≤）：** アームが「有意に悪い」とフラグされるコンバージョンの最大値。\n",
    "- **悪い？：** アームが閾値を下回った場合（つまり統計的に悪い）は「はい」；そうでなければ「いいえ」。\n",
    "\n",
    "**結果**\n",
    "\n",
    "- 両方のGPT-4.1バリアントが閾値を上回りました—つまり、どちらもコントロールより統計的に悪くありませんでした。\n",
    "- GPT-4.1（フル）は、大幅なコスト削減を提供しながら、Claude 3.5 Sonnetに対してコンバージョン率で互角でした。\n",
    "\n",
    "### コンバージョンの測定には創造性とデータが必要\n",
    "\n",
    "この分析を実行するには、ユーザー行動をStripe支払いイベントにリンクするシステムが必要です。これには汎用テンプレートはありませんが、HyperWriteで使用されているアーキテクチャは実装方法の一例を示しています。このワークフローは、ユーザーがAIと対話し、Stripe経由でアップグレードできる任意のスタートアップに適応できます。\n",
    "\n",
    "1. **ユーザー追跡：** 各新規サインアップに、ライフサイクルを通じて持続する一意の識別子を割り当てます。\n",
    "2. **モデル割り当て：** サインアップ時に各ユーザーをテストグループ（モデルバリアント）にランダムに割り当て、この割り当てをデータベースに保存します。\n",
    "3. **インタラクションログ：** 主要イベント（例：初回使用、レート制限到達）をユーザーIDとモデル割り当てと共にログします。\n",
    "4. **コンバージョンイベント捕捉：** `checkout.session.completed`イベントを聞くためのStripe webhookを設定します。トリガーされたら、Stripe顧客を内部ユーザーIDに一致させ、支払い/コンバージョンを反映するようにデータベースを更新します。\n",
    "5. **データ集約：** テストグループ割り当てとコンバージョンデータを定期的に単一のテーブルまたはダッシュボードに取り込みます。\n",
    "6. **統計テスト：** 基本的なZ検定（多くのライブラリ/Excelテンプレートが存在）を使用して、コンバージョン率の違いが意味があるかどうかを分析します。\n",
    "\n",
    "以下のシーケンス図はプロセスの概要を示しています：\n",
    "\n",
    "![プロセス図](../../images/stripe_eval_diagram.png)\n",
    "\n",
    "#### ユーザーワークフロー\n",
    "\n",
    "HyperWriteでのユーザージャーニーは以下のようになります：\n",
    "\n",
    "1. **ユーザーサインアップ：** ユーザーがアカウントを作成すると、その情報がデータベースに保存され、一意の`user_id`が割り当てられます。\n",
    "2. **初回メッセージ送信：** 新規ユーザーが初めてライティングアシスタントと対話します。\n",
    "3. **レート制限トリガー：** 設定された数のメッセージの後、レート制限に達します。これにより、アップグレードプロンプトを表示できる一貫したポイントが導入されます。\n",
    "4. **コンバージョン機会：** 一部のユーザーはこの時点でサブスクリプションを選択します—Stripeチェックアウトに誘導されます。\n",
    "\n",
    "#### Stripeワークフロー\n",
    "\n",
    "私たちは二つの主要なStripeアクションに関心があります：\n",
    "\n",
    "1. **Stripeイベントリスニング：** システムはStripeのwebhookからの`checkout.session.completed`イベントを聞きます。これは支払いが成功した時に発火します。\n",
    "2. **データベース更新：** webhookが受信されると、対応する`user_id`がデータベースでコンバージョン済みとしてマークされます。\n",
    "\n",
    "#### テストの実行\n",
    "\n",
    "テストが完了したかどうかを定期的にチェックします：\n",
    "\n",
    "1. **テストグループのクエリ：** 各モデルバリアントに割り当てられたすべてのユーザーを取得します。\n",
    "2. **Stripeデータの結合：** ユーザーデータをStripeサブスクリプションイベントとマージし、各グループのどのユーザーがコンバージョンしたかを正確に把握します。\n",
    "3. **統計実行：** 片側二比率Z検定（前のセクションを参照）を使用して、コンバージョン率の違いが統計的に意味があるかどうかをチェックします。\n",
    "\n",
    "## 結論と次のステップ\n",
    "\n",
    "このアプローチからの主要な教訓は、ビジネス指標（Stripeコンバージョンなど）に結び付けられた実世界テストが、どのモデル選択が実際に製品の結果を推進するかを明らかにできることです。オフラインベンチマークやラボテストにも場所がありますが、ユーザーが支払いを決定する瞬間に評価を結び付けることは、しばしば顧客とビジネスの両方に利益をもたらす決定につながります。\n",
    "\n",
    "### これがスタートアップにとって意味すること\n",
    "\n",
    "現職モデルを打ち負かすことが常に必要なわけではありません；より低コストで主要指標で「同程度」のパフォーマンスを示すモデルは価値があります。この場合、OpenAIのGPT-4.1は、コストを削減しながら現職のStripeコンバージョン率に匹敵しました。\n",
    "\n",
    "これは、モデル評価をStripe駆動のA/Bテストに結び付ける価値を強調しています—ベンチマークや主観的印象のみに頼るのではなく、明確で収益に結び付けられた答えを得ることができます。\n",
    "\n",
    "スタートアップは、このテストをいくつかの方向に拡張できます：\n",
    "\n",
    "- **ペルソナまたは使用例別のセグメント：** オーディエンス（例：パワーユーザー対新規ユーザー、異なる業界）を分割し、各グループに最適なモデルやプロンプトを確認します。\n",
    "- **収益-コストのスイートスポットを見つける：** トップライン収益だけでなく、各モデルを提供するコストも考慮します。最適な選択は、売上のみを最大化するのではなく、利益のバランスを取るかもしれません。\n",
    "- **長期的影響の監視：** 即座のコンバージョンを超えて見てください。持続可能な成長のために最適化するために、サブスクライバーライフタイムバリュー、チャーン、または継続率などの指標を追跡します。\n",
    "\n",
    "何を測定し、どのように実験するかについて創造的になる余地がたくさんあるので、チームにとって最も重要なことのために製品を調整できます。\n",
    "\n",
    "この種のテスト、あなたのアプローチへのフィードバック、または独自のテストの設定に関する入力についての質問があれば、お気軽にお問い合わせください：[josh@othersideai.com](mailto:josh@othersideai.com)。\n",
    "\n",
    "構築し、実験し、ユーザーとStripeダッシュボードに道を導いてもらいましょう。\n",
    "\n",
    "## 貢献者\n",
    "\n",
    "このクックブックは、AI搭載ライティングツールとリサーチアシスタントを構築する会社HyperWriteのリードエンジニアである[Josh Bickett](https://www.linkedin.com/in/josh-bickett-4219b166/)によって貢献されました。手法とケーススタディはHyperWriteの経験を反映していますが、支払いコンバージョン指標を使用してLLMを評価するスタートアップの一般的なガイドとして意図されています。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "file_extension": ".md",
   "mimetype": "text/markdown",
   "name": "python",
   "nbconvert_exporter": "markdown"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
