{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv8-mnmnj0Wp"
   },
   "source": [
    "# Deep Research Agents クックブック\n",
    "\n",
    "このクックブックでは、OpenAI Deep Research APIとOpenAI [Agents SDK](https://openai.github.io/openai-agents-python/)を使用してAgenticリサーチワークフローを構築する方法を説明します。これは[基礎クックブック](https://cookbook.openai.com/examples/deep_research_api/introduction_to_deep_research_api)の続編です。まだその内容に慣れ親しんでいない場合は、まずそちらをご覧になることをお勧めします。\n",
    "\n",
    "シングルエージェントとマルチエージェントパイプラインの調整、出力品質を最大化するためのユーザークエリの拡充、リサーチ進捗のストリーミング、ウェブ検索と[内部ファイル検索のためのMCP](https://cookbook.openai.com/examples/deep_research_api/how_to_build_a_deep_research_mcp_server/readme)の統合、そして堅牢なリサーチアプリケーションの設計について学習します。\n",
    "\n",
    "計画、統合、ツール使用、または多段階推論が必要なタスクにはDeep Research Agentsの使用を検討してください。些細な事実確認、単純なQ&A、短文チャットには Deep Research を使用しないでください。通常のopenai.responsesAPIの方が高速で安価です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6gLS5aVj0Wr"
   },
   "source": [
    "### 前提条件\n",
    "* OpenAI API キー（環境変数として OPENAI_API_KEY を設定）\n",
    "* Agents SDK と OpenAI Python SDK\n",
    "\n",
    "### セットアップ\n",
    "*依存関係のインストール*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWE9uQq4j0Ws",
    "outputId": "99c15803-0506-4464-d624-a31b5bc809a4"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade \"openai>=1.88\" \"openai-agents>=0.0.19\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9lWqn_Wj0Wt"
   },
   "source": [
    "### ライブラリのインポートとクライアントの設定\n",
    "\n",
    "**ゼロデータ保持**\n",
    "\n",
    "以下の`os.environ`設定により、データ保持を無効にします。これにより、企業はDeep Researchでゼロデータ保持環境で運用することができます。データ保持があなたにとって積極的な制約で_ない_場合は、エージェントワークフローの自動追跡機能や、評価やファインチューニングなどの他のプラットフォームツールとの深い統合を利用できるよう、有効のままにしておくことを検討してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OWnnTNZJj0Wt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from agents import Agent, Runner, WebSearchTool, RunConfig, set_default_openai_client, HostedMCPTool\n",
    "from typing import List, Dict, Optional\n",
    "from pydantic import BaseModel\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Use env var for API key and set a long timeout\n",
    "client = AsyncOpenAI(api_key=\"\", timeout=600.0)\n",
    "set_default_openai_client(client)\n",
    "os.environ[\"OPENAI_AGENTS_DISABLE_TRACING\"] = \"1\" # Disable tracing for Zero Data Retention (ZDR) Organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Omyb04nj0Wy"
   },
   "source": [
    "### 基本的なディープリサーチエージェント\n",
    "\n",
    "基本的なリサーチエージェントは、o4-mini-deep-research-alphaモデルを使用してディープリサーチを実行します。このエージェントは、パブリックインターネットへのネイティブWebSearch機能を持ち、その調査結果を直接ノートブックにストリーミングして返します。この場合、フルのo3ディープリサーチモデルよりも高速でありながら、十分な知能を持つ`o4-mini-deep-research-alpha`モデルを使用しています。\n",
    "\n",
    "**学習目標:**\n",
    "\n",
    "この後、単一エージェントのリサーチタスクを実行し、その進捗をストリーミングできるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c91rFNYWj0Wy",
    "outputId": "6f2e3bbe-f321-4a8e-b7df-6b6c5bade65a"
   },
   "outputs": [],
   "source": [
    "# Define the research agent\n",
    "research_agent = Agent(\n",
    "    name=\"Research Agent\",\n",
    "    model=\"o4-mini-deep-research-2025-06-26\",\n",
    "    tools=[WebSearchTool()],\n",
    "    instructions=\"You perform deep empirical research based on the user's question.\"\n",
    ")\n",
    "\n",
    "# Async function to run the research and print streaming progress\n",
    "async def basic_research(query):\n",
    "    print(f\"Researching: {query}\")\n",
    "    result_stream = Runner.run_streamed(\n",
    "        research_agent,\n",
    "        query\n",
    "    )\n",
    "\n",
    "    async for ev in result_stream.stream_events():\n",
    "        if ev.type == \"agent_updated_stream_event\":\n",
    "            print(f\"\\n--- switched to agent: {ev.new_agent.name} ---\")\n",
    "            print(f\"\\n--- RESEARCHING ---\")\n",
    "        elif (\n",
    "            ev.type == \"raw_response_event\"\n",
    "            and hasattr(ev.data, \"item\")\n",
    "            and hasattr(ev.data.item, \"action\")\n",
    "        ):\n",
    "            action = ev.data.item.action or {}\n",
    "            if action.get(\"type\") == \"search\":\n",
    "                print(f\"[Web search] query={action.get('query')!r}\")\n",
    "\n",
    "    # streaming is complete → final_output is now populated\n",
    "    return result_stream.final_output\n",
    "\n",
    "# Run the research and print the result\n",
    "result = await basic_research(\"Research the economic impact of semaglutide on global healthcare systems.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4wk2sulj0Wy"
   },
   "source": [
    "### 明確化を伴うマルチエージェント研究\n",
    "\n",
    "マルチエージェント深層研究\n",
    "\n",
    "「深層研究」が生成する研究品質をさらに向上させる方法について考えてみましょう。この場合、マルチエージェントアーキテクチャを活用して、深層研究エージェントに提出する前に、ユーザーのクエリと最終的な研究レポートで期待される内容について_より多くの情報_でプロンプトを充実させています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlZ6rxFn7C0d"
   },
   "source": [
    "## サブエージェントプロンプトの充実化\n",
    "\n",
    "サポートエージェントのプロンプトは、ユーザーの初期クエリに構造と厳密性を提供することで、最終的な研究成果の品質を向上させるために特別に設計されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "glkoOX6q6Ph9"
   },
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "#  Prompts\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "CLARIFYING_AGENT_PROMPT =  \"\"\"\n",
    "    If the user hasn't specifically asked for research (unlikely), ask them what research they would like you to do.\n",
    "\n",
    "        GUIDELINES:\n",
    "        1. **Be concise while gathering all necessary information** Ask 2–3 clarifying questions to gather more context for research.\n",
    "        - Make sure to gather all the information needed to carry out the research task in a concise, well-structured manner. Use bullet points or numbered lists if appropriate for clarity. Don't ask for unnecessary information, or information that the user has already provided.\n",
    "        2. **Maintain a Friendly and Non-Condescending Tone**\n",
    "        - For example, instead of saying “I need a bit more detail on Y,” say, “Could you share more detail on Y?”\n",
    "        3. **Adhere to Safety Guidelines**\n",
    "        \"\"\"\n",
    "\n",
    "RESEARCH_INSTRUCTION_AGENT_PROMPT = \"\"\"\n",
    "\n",
    "        Based on the following guidelines, take the users query, and rewrite it into detailed research instructions. OUTPUT ONLY THE RESEARCH INSTRUCTIONS, NOTHING ELSE. Transfer to the research agent.\n",
    "\n",
    "        GUIDELINES:\n",
    "        1. **Maximize Specificity and Detail**\n",
    "        - Include all known user preferences and explicitly list key attributes or dimensions to consider.\n",
    "        - It is of utmost importance that all details from the user are included in the expanded prompt.\n",
    "\n",
    "        2. **Fill in Unstated But Necessary Dimensions as Open-Ended**\n",
    "        - If certain attributes are essential for a meaningful output but the user has not provided them, explicitly state that they are open-ended or default to “no specific constraint.”\n",
    "\n",
    "        3. **Avoid Unwarranted Assumptions**\n",
    "        - If the user has not provided a particular detail, do not invent one.\n",
    "        - Instead, state the lack of specification and guide the deep research model to treat it as flexible or accept all possible options.\n",
    "\n",
    "        4. **Use the First Person**\n",
    "        - Phrase the request from the perspective of the user.\n",
    "\n",
    "        5. **Tables**\n",
    "        - If you determine that including a table will help illustrate, organize, or enhance the information in your deep research output, you must explicitly request that the deep research model provide them.\n",
    "        Examples:\n",
    "        - Product Comparison (Consumer): When comparing different smartphone models, request a table listing each model’s features, price, and consumer ratings side-by-side.\n",
    "        - Project Tracking (Work): When outlining project deliverables, create a table showing tasks, deadlines, responsible team members, and status updates.\n",
    "        - Budget Planning (Consumer): When creating a personal or household budget, request a table detailing income sources, monthly expenses, and savings goals.\n",
    "        Competitor Analysis (Work): When evaluating competitor products, request a table with key metrics—such as market share, pricing, and main differentiators.\n",
    "\n",
    "        6. **Headers and Formatting**\n",
    "        - You should include the expected output format in the prompt.\n",
    "        - If the user is asking for content that would be best returned in a structured format (e.g. a report, plan, etc.), ask the Deep Research model to “Format as a report with the appropriate headers and formatting that ensures clarity and structure.”\n",
    "\n",
    "        7. **Language**\n",
    "        - If the user input is in a language other than English, tell the model to respond in this language, unless the user query explicitly asks for the response in a different language.\n",
    "\n",
    "        8. **Sources**\n",
    "        - If specific sources should be prioritized, specify them in the prompt.\n",
    "        - Prioritize Internal Knowledge. Only retrieve a single file once.\n",
    "        - For product and travel research, prefer linking directly to official or primary websites (e.g., official brand sites, manufacturer pages, or reputable e-commerce platforms like Amazon for user reviews) rather than aggregator sites or SEO-heavy blogs.\n",
    "        - For academic or scientific queries, prefer linking directly to the original paper or official journal publication rather than survey papers or secondary summaries.\n",
    "        - If the query is in a specific language, prioritize sources published in that language.\n",
    "\n",
    "        IMPORTANT: Ensure that the complete payload to this function is valid JSON\n",
    "        IMPORTANT: SPECIFY REQUIRED OUTPUT LANGUAGE IN THE PROMPT\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1dXCfxa6sf1"
   },
   "source": [
    "# 4エージェント深層研究パイプライン\n",
    "\n",
    "1. **トリアージエージェント**  \n",
    "   - ユーザーのクエリを検査  \n",
    "   - コンテキストが不足している場合は明確化エージェントにルーティング、そうでなければ指示エージェントにルーティング  \n",
    "\n",
    "2. **明確化エージェント**  \n",
    "   - フォローアップ質問を行う  \n",
    "   - ユーザー（またはモック）の回答を待機  \n",
    "\n",
    "3. **指示構築エージェント**  \n",
    "   - 拡充された入力を正確な研究概要に変換  \n",
    "\n",
    "4. **研究エージェント** (`o3-deep-research`)  \n",
    "   - `WebSearchTool`を使用してWebスケールの実証研究を実行\n",
    "   - MCPを使用して内部ナレッジストアに対する検索を実行し、関連するドキュメントがある場合、エージェントはそれらの関連スニペットを参考資料に組み込む   \n",
    "   - 透明性のために中間イベントをストリーミング\n",
    "   - 最終的な研究成果物を出力（後でパースする）\n",
    "\n",
    "![../../images/agents_dr.png](../../../images/agent_dr.png)\n",
    "\n",
    "MCPサーバーが_どのように_構築されるかについてのより詳しい洞察については、[こちらのリソースを参照してください。](https://cookbook.openai.com/examples/deep_research_api/how_to_build_a_deep_research_mcp_server/readme )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "y-8WVGMBj0Wz"
   },
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# Structured outputs (needed only for Clarifying agent)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class Clarifications(BaseModel):\n",
    "    questions: List[str]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Agents\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "research_agent = Agent(\n",
    "    name=\"Research Agent\",\n",
    "    model=\"o3-deep-research-2025-06-26\",\n",
    "    instructions=\"Perform deep empirical research based on the user's instructions.\",\n",
    "    tools=[WebSearchTool(),\n",
    "           HostedMCPTool(\n",
    "            tool_config={\n",
    "                \"type\": \"mcp\",\n",
    "                \"server_label\": \"file_search\",\n",
    "                \"server_url\": \"https://<url>/sse\",\n",
    "                \"require_approval\": \"never\",\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "instruction_agent = Agent(\n",
    "    name=\"Research Instruction Agent\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=RESEARCH_INSTRUCTION_AGENT_PROMPT,\n",
    "    handoffs=[research_agent],\n",
    ")\n",
    "\n",
    "clarifying_agent = Agent(\n",
    "    name=\"Clarifying Questions Agent\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=CLARIFYING_AGENT_PROMPT,\n",
    "    output_type=Clarifications,\n",
    "    handoffs=[instruction_agent],\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=(\n",
    "        \"Decide whether clarifications are required.\\n\"\n",
    "        \"• If yes → call transfer_to_clarifying_questions_agent\\n\"\n",
    "        \"• If no  → call transfer_to_research_instruction_agent\\n\"\n",
    "        \"Return exactly ONE function-call.\"\n",
    "    ),\n",
    "    handoffs=[clarifying_agent, instruction_agent],\n",
    ")\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "#  Auto-clarify helper\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "async def basic_research(\n",
    "    query: str,\n",
    "    mock_answers: Optional[Dict[str, str]] = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    stream = Runner.run_streamed(\n",
    "        triage_agent,\n",
    "        query,\n",
    "        run_config=RunConfig(tracing_disabled=True),\n",
    "    )\n",
    "\n",
    "    async for ev in stream.stream_events():\n",
    "        if isinstance(getattr(ev, \"item\", None), Clarifications):\n",
    "            reply = []\n",
    "            for q in ev.item.questions:\n",
    "                ans = (mock_answers or {}).get(q, \"No preference.\")\n",
    "                reply.append(f\"**{q}**\\n{ans}\")\n",
    "            stream.send_user_message(\"\\n\\n\".join(reply))\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(ev)\n",
    "\n",
    "    #return stream.final_output\n",
    "    return stream\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "#  Example run\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "result = await basic_research(\n",
    "    \"Research the economic impact of semaglutide on global healthcare systems.\",\n",
    "    mock_answers={},   # or provide canned answers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEVDHxRzjvJM"
   },
   "source": [
    "## エージェント相互作用フロー\n",
    "\n",
    "Agent SDKトレースを通じてネイティブに提供されていますが、ツール呼び出しを含む人間が読みやすい高レベルのエージェント相互作用フローを印刷したい場合があります。`print_agent_interaction`を実行して、以下を含む簡略化された読みやすいエージェントステップのシーケンスを取得してください：エージェント名、イベントの種類（ハンドオフ、ツール呼び出し、メッセージ出力）、簡潔なツール呼び出し情報（ツール名と引数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YZ_ibZIic_u",
    "outputId": "23c97975-94f2-47e0-ea0f-7475b46c4f6c"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_agent_interaction_flow(stream):\n",
    "    print(\"=== Agent Interaction Flow ===\")\n",
    "    count = 1\n",
    "\n",
    "    for item in stream.new_items:\n",
    "        # Agent name, fallback if missing\n",
    "        agent_name = getattr(item.agent, \"name\", \"Unknown Agent\") if hasattr(item, \"agent\") else \"Unknown Agent\"\n",
    "\n",
    "        if item.type == \"handoff_call_item\":\n",
    "            func_name = getattr(item.raw_item, \"name\", \"Unknown Function\")\n",
    "            print(f\"{count}. [{agent_name}] → Handoff Call: {func_name}\")\n",
    "            count += 1\n",
    "\n",
    "        elif item.type == \"handoff_output_item\":\n",
    "            print(f\"{count}. [{agent_name}] → Handoff Output\")\n",
    "            count += 1\n",
    "\n",
    "        elif item.type == \"mcp_list_tools_item\":\n",
    "            print(f\"{count}. [{agent_name}] → mcp_list_tools_item\")\n",
    "            count += 1\n",
    "\n",
    "        elif item.type == \"reasoning_item\":\n",
    "            print(f\"{count}. [{agent_name}] → Reasoning step\")\n",
    "            count += 1\n",
    "\n",
    "        elif item.type == \"tool_call_item\":\n",
    "            tool_name = getattr(item.raw_item, \"name\", None)\n",
    "\n",
    "            # Skip tool call if tool_name is missing or empty\n",
    "            if not isinstance(tool_name, str) or not tool_name.strip():\n",
    "                continue  # skip silently\n",
    "\n",
    "            tool_name = tool_name.strip()\n",
    "\n",
    "            args = getattr(item.raw_item, \"arguments\", None)\n",
    "            args_str = \"\"\n",
    "\n",
    "            if args:\n",
    "                try:\n",
    "                    parsed_args = json.loads(args)\n",
    "                    if parsed_args:\n",
    "                        args_str = json.dumps(parsed_args)\n",
    "                except Exception:\n",
    "                    if args.strip() and args.strip() != \"{}\":\n",
    "                        args_str = args.strip()\n",
    "\n",
    "            args_display = f\" with args {args_str}\" if args_str else \"\"\n",
    "\n",
    "            print(f\"{count}. [{agent_name}] → Tool Call: {tool_name}{args_display}\")\n",
    "            count += 1\n",
    "\n",
    "        elif item.type == \"message_output_item\":\n",
    "            print(f\"{count}. [{agent_name}] → Message Output\")\n",
    "            count += 1\n",
    "\n",
    "        else:\n",
    "            print(f\"{count}. [{agent_name}] → {item.type}\")\n",
    "            count += 1\n",
    "\n",
    "# Example usage:\n",
    "parse_agent_interaction_flow(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mZW3GT5kCOw"
   },
   "source": [
    "## 引用\n",
    "\n",
    "以下は、最終出力に関連するURL引用を抽出して出力するPythonスニペットです："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCyfxC7siant",
    "outputId": "28539b23-00db-4dfa-902b-cd80c67ba765"
   },
   "outputs": [],
   "source": [
    "def print_final_output_citations(stream, preceding_chars=50):\n",
    "    # Iterate over new_items in reverse to find the last message_output_item(s)\n",
    "    for item in reversed(stream.new_items):\n",
    "        if item.type == \"message_output_item\":\n",
    "            for content in getattr(item.raw_item, 'content', []):\n",
    "                if not hasattr(content, 'annotations') or not hasattr(content, 'text'):\n",
    "                    continue\n",
    "                text = content.text\n",
    "                for ann in content.annotations:\n",
    "                    if getattr(ann, 'type', None) == 'url_citation':\n",
    "                        title = getattr(ann, 'title', '<no title>')\n",
    "                        url = getattr(ann, 'url', '<no url>')\n",
    "                        start = getattr(ann, 'start_index', None)\n",
    "                        end = getattr(ann, 'end_index', None)\n",
    "\n",
    "                        if start is not None and end is not None and isinstance(text, str):\n",
    "                            # Calculate preceding snippet start index safely\n",
    "                            pre_start = max(0, start - preceding_chars)\n",
    "                            preceding_text = text[pre_start:start].replace('\\n', ' ').strip()\n",
    "                            excerpt = text[start:end].replace('\\n', ' ').strip()\n",
    "                            print(\"# --------\")\n",
    "                            print(\"# MCP CITATION SAMPLE:\")\n",
    "                            print(f\"#   Title:       {title}\")\n",
    "                            print(f\"#   URL:         {url}\")\n",
    "                            print(f\"#   Location:    chars {start}–{end}\")\n",
    "                            print(f\"#   Preceding:   '{preceding_text}'\")\n",
    "                            print(f\"#   Excerpt:     '{excerpt}'\\n\")\n",
    "                        else:\n",
    "                            # fallback if no indices available\n",
    "                            print(f\"- {title}: {url}\")\n",
    "            break\n",
    "\n",
    "# Usage\n",
    "print_final_output_citations(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTeTcni5L-1s",
    "outputId": "eb442687-0530-4198-d778-b7d0dcf07df0"
   },
   "outputs": [],
   "source": [
    "## Deep Research Research Report\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UJcBbp9j0Wz"
   },
   "source": [
    "### 結論\n",
    "\n",
    "このノートブックのパターンを使用することで、OpenAI Deep Research Agentsを使用したスケーラブルで本番環境対応の研究ワークフローを構築するための基盤を手に入れることができます。これらの例は、マルチエージェントパイプラインの調整と研究進捗のストリーミング方法だけでなく、外部知識アクセスのためのWeb検索とMCPの統合方法も実証しています。\n",
    "\n",
    "エージェント型ワークフローを活用することで、単純なQ&Aを超えて、計画、統合、ツール使用を必要とする複雑で多段階の研究タスクに取り組むことができます。モジュラーなマルチエージェント設計（トリアージ、明確化、指示、研究エージェント）により、これらのパイプラインをヘルスケアや金融から技術的デューデリジェンス、市場分析まで、幅広いドメインとユースケースに適応させることができます。\n",
    "\n",
    "Deep Research APIとAgents SDKが進化し続ける中、これらのパターンは自動化されたデータに基づく研究の最前線に留まるのに役立ちます。内部知識ツールの構築、競合インテリジェンスの自動化、専門アナリストのサポートなど、どのような用途であっても、これらのワークフローは強力で拡張可能な出発点を提供します。\n",
    "\n",
    "**楽しい研究を！**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
