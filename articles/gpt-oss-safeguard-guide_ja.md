# gpt-oss-safeguardのユーザーガイド

## はじめに・概要

ROOSTとOpenAIは、[gpt-oss-safeguard](https://github.com/openai/gpt-oss-safeguard)の推論能力を最大化するポリシープロンプトの書き方、深い分析のための適切なポリシー長の選択、そしてoss-safeguardの推論出力を本番のTrust & Safetyシステムに統合する方法について説明するガイドを作成しました。

### gpt-oss-safeguardとは？

gpt-oss-safeguardは、カスタマイズ可能なポリシーに基づいてテキストコンテンツを分類するのに役立つ、安全性分類タスク専用に訓練された初のオープンウェイト推論モデルです。[gpt-oss](https://openai.com/index/introducing-gpt-oss/)のファインチューニング版として、gpt-oss-safeguardはあなたが提供する明示的な書面ポリシーに従うよう設計されています。これにより、あなた独自の分類法、定義、閾値が分類決定を導く**bring-your-own-policy** Trust & Safety AIが可能になります。よく作成されたポリシーは、gpt-oss-safeguardの推論能力を解放し、微妙なコンテンツの処理、境界線上の決定の説明、文脈的要因への適応を可能にします。

OpenAIがgpt-oss-safeguardの内部版をどのように使用しているかについては、[こちら](https://openai.com/index/introducing-gpt-oss-safeguard/)で詳しく読むことができます。

大規模言語モデルは、主に2つの方法で安全性モデルと見なすことができます：

- ファインチューニングされた安全性モデルは、一般的な推論モデル（gpt-ossなど）から始まり、ユーザーとのやり取りの中で安全に応答するよう訓練されます。
- 事前構築された安全性モデル（ShieldGemma、LlamaGuard、RoGuardなど）は、「安全でない」とみなされるものの組み込み定義と固定されたポリシー分類法を備えています。

gpt-oss-safeguardは、Trust & Safetyワークフロー専用に構築されたポリシー従順モデルであり、**あなた独自の書面基準を確実に解釈・実行し、なぜその決定を下したかを説明できます**。モデルの背後にある推論により、監査可能性とカスタマイゼーションに根ざした大規模な安全性システムとの統合に適しています。

### gpt-oss-safeguardの使用方法

[gpt-ossファミリーのモデル](https://openai.com/open-models/)と同様に、これはローカルで実行するか、独自のインフラストラクチャに統合するオープンウェイトのオープンソースモデルです。[harmony response format](https://github.com/openai/harmony)と連携するよう設計されています。Harmonyは、gpt-oss-safeguardに完全な推論スタックへのアクセスを提供し、一貫した適切な形式の出力を保証する構造化プロンプトインターフェースです。

gpt-oss-safeguardを含むgpt-ossファミリーのモデルは、以下を使用してサーバーで実行できます：

- [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#gpt-oss-vllm-usage-guide)（NVIDIAのH100などの専用GPU用）
- [HuggingFace Transformers](https://cookbook.openai.com/articles/gpt-oss/run-locally-lmstudio)（コンシューマーGPU用）
- [Google Colab](https://cookbook.openai.com/articles/gpt-oss/run-colab)

そして以下を使用してローカルで実行できます：

- [LM Studio](https://cookbook.openai.com/articles/gpt-oss/run-locally-lmstudio)
- [Ollama](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)

### gpt-oss-safeguardを使用すべき人

gpt-oss-safeguardは、リアルタイムコンテキストと大規模自動化が必要なユーザー向けに設計されています：

- 柔軟なコンテンツモデレーションが必要なTrust & Safetyシステムに取り組む**ML/AIエンジニア**
- モデレーション、Trust & Safety、またはプラットフォーム整合性パイプラインを構築・改善する**Trust & Safetyエンジニア**
- コンテンツ安全性イニシアチブを監督する**テクニカルプログラムマネージャー**
- 文脈的でポリシーベースのコンテンツモデレーションが必要なプロジェクト/アプリケーションを構築する**開発者**
- 組織で受け入れられるものを定義し、ポリシーラインをテストし、例を生成し、コンテンツを評価したい**ポリシー作成者**

安全性調整されたモデルは、明確で構造化されたプロンプトが与えられた場合、コンテンツモデレーションで優れた性能を発揮します。このガイドでは、本番環境でのモデレーションシステム展開からの主要な学習内容を、プロンプト構造、出力フォーマット、長さ最適化に焦点を当てて説明します。

### HuggingFace Transformersでのgpt-oss-safeguardの使用

Hugging FaceのTransformersライブラリは、大規模言語モデルをローカルまたはサーバーで読み込み・実行するための柔軟な方法を提供します。[このガイド](https://cookbook.openai.com/articles/gpt-oss/run-transformers)では、高レベルパイプラインまたは生のトークンIDを使用した低レベル生成呼び出しのいずれかで、Transformersを使用して[OpenAI gpt-oss](https://huggingface.co/openai/gpt-oss-20b)モデルを実行する方法を説明します。サーバーとやり取りする最も簡単な方法は、transformers chat CLIを使用することです：

```bash
transformers chat localhost:8000 --model-name-or-path openai/gpt-oss-safeguard-20b
```

または、cURLでHTTPリクエストを送信することです：

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-oss-safeguard-20b",
    "stream": true,
    "messages": [
      { "role": "system", "content": "<your policy>" },
      { "role": "user", "content": "<user content to verify>" }
    ]
  }'


```

transformers serveをCursorや他のツールと統合するなどの追加の使用例は、[ドキュメント](https://huggingface.co/docs/transformers/main/serving)で詳しく説明されています。

### Ollamaでのgpt-oss-safeguardの実行

[Ollama](https://ollama.com/download)は、gpt-oss-safeguard 20Bおよび120Bモデルを直接サポートしています。以下のコマンドは、モデルを自動的にダウンロードし、デバイスで実行します。

#### gpt-oss-safeguard:20b

```bash
ollama run gpt-oss-safeguard:20b
```

#### gpt-oss-safeguard:120b

```bash
ollama run gpt-oss-safeguard:120b
```

Ollamaは、gpt-oss-safeguardモデルを使用してアプリケーションやツールを構築するための[OpenAI API](https://docs.ollama.com/api/openai-compatibility)、[OllamaのAPI](https://docs.ollama.com/api)、[Python](https://github.com/ollama/ollama-python)、[JavaScript](https://github.com/ollama/ollama-js) SDKをサポートしています。詳しくは[Ollamaのドキュメント](https://docs.ollama.com/)をご覧ください。

### LM Studioでのgpt-oss-safeguardの実行

または、[LM Studio](https://lmstudio.ai/)を使用して、[OpenAI Chat Completions](https://lmstudio.ai/docs/developer/openai-compat/chat-completions)および[Responses API](https://lmstudio.ai/docs/developer/openai-compat/responses)互換APIを含むモデルをローカルで実行できます。[LM Studio用のgpt-oss-safeguardページ](https://lmstudio.ai/models/gpt-oss-safeguard)にアクセスするか、以下のコマンドを実行して各モデルをダウンロードしてください：

#### gpt-oss-safeguard-20b

```bash
lms get openai/gpt-oss-safeguard-20b
```

#### gpt-oss-safeguard-120b

```bash
lms get openai/gpt-oss-safeguard-120b
```

### vLLMでのgpt-oss-safeguardの実行

[vLLM](https://docs.vllm.ai/)は、Python依存関係管理に[uv](https://docs.astral.sh/uv/)の使用を推奨しています。以下のコマンドは、モデルを自動的にダウンロードし、サーバーを開始します。

```shell
uv pip install vllm==0.10.2 --torch-backend=auto

vllm serve openai/gpt-oss-safeguard-120b
```

[vLLMでgpt-ossを使用する方法について詳しく学ぶ。](https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#gpt-oss-vllm-usage-guide)

### Harmony Response Formatの理解

gpt-oss-safeguardは、[harmony prompt format](https://cookbook.openai.com/articles/openai-harmony)を使用して構造化された出力を提供し、推論を行います。これは、なぜ決定や分類が行われたかを理解し監査する必要があるTrust & Safetyワークフローにとって重要です。harmonyフォーマットにより、oss-safeguardは応答を2つの部分に分けます：

1. **推論チャンネル：** モデルがポリシーを通じて推論し、エッジケースを考慮し、その論理を説明する場所
2. **出力チャンネル：** あなたが指定したフォーマット化された分類決定

harmonyを通じて、システムメッセージで`reasoning_effort`パラメータを`low`、`medium`、または`high`に設定することで、oss-safeguardがどの程度深く推論するかを制御できます。モデルは設定されていない場合、デフォルトで`medium`を使用します。より高い推論努力により、oss-safeguardはより多くの要因を考慮し、複数のポリシーセクションを追跡し、ルール間の複雑な相互作用を処理できます。より低い努力は、直接的な分類に対してより高速な応答を提供します。

**vLLM**（ほとんどのユーザーに推奨）またはチャットメッセージ入力を提供する他の推論ソリューションを使用している場合、リクエストを[チャットメッセージ](https://docs.vllm.ai/en/v0.7.0/getting_started/examples/chat.html)としてフォーマットすると、harmonyフォーマットが自動的に適用されます：

- **システムメッセージ：** あなたのポリシープロンプト（推論努力を制御するために、システムメッセージにReasoning: highまたは類似のものを含める）
- **ユーザーメッセージ：** 分類するコンテンツ

## oss-safeguardがポリシープロンプトを使用する方法

oss-safeguardは、あなたの書面ポリシーを統治ロジックとして使用するよう設計されています。ほとんどのモデルが訓練された特徴に基づく信頼度スコアを提供し、ポリシー変更には再訓練が必要ですが、oss-safeguardは提供された分類法の境界内で推論に裏付けられた決定を行います。この機能により、T&Sチームは既存のモデレーションやコンプライアンスシステム内でポリシー整合推論レイヤーとしてoss-safeguardを展開できます。これは、モデル全体を再訓練することなく、新しいポリシーを即座に更新またはテストできることも意味します。

## gpt-oss-safeguardのための効果的なポリシープロンプトの作成

oss-safeguardは、ポリシーがエッセイではなくTrust & Safetyポリシーガイドのように整理されている場合に最高の性能を発揮します。すでにポリシーセットがある場合は、良い状態にあります。モデルが定義を効率的にナビゲートできるよう、ヘッダーと明確なカテゴリを使用してください。以前にチーム向けにポリシーを書いたことがある場合、これは馴染みがあるはずです。

### ポリシープロンプティングの理解

ポリシープロンプトは、モデルの行動の運用境界を定義します。人間のレビュアー向けに書かれたコンテンツやプラットフォームポリシーと同様に、oss-safeguard用のポリシーは、何が違反を構成するか、何が許可されるか、そしてその違いをTrust & Safetyシステムの残りの部分に流れる決定にどのように伝達するかを明確に指定する必要があります。

効果的なポリシープロンプトは、類似のコンテンツタイプを区別し、微妙で暗号化された、または間接的な違反をキャッチし、エッジケースでの偽陽性を防ぐために構造化されています。これをポリシー文書とトレーニング例の組み合わせと考えてください。

### ポリシープロンプトの構造化

ポリシープロンプトには4つの別々のセクションが必要です。

1. **指示：** モデルが何をしなければならないか、どのように答えるべきか
2. **定義：** 主要用語の簡潔な説明
3. **基準：** 違反コンテンツと非違反コンテンツの区別
4. **例：** 決定境界近くの短い具体的な例。分類したいものと分類したくないものの両方の例を持つことが重要です

oss-safeguardは構造化されたモデレーション用に調整されているため、どのように応答するかの明示的な指示を期待します。ポリシープロンプトは、応答と出力の期待されるフォーマットを含む一貫したパターンに従う場合、より良い性能を発揮する可能性があります。harmonyフォーマットの構造化チャンネルにより、oss-safeguardは最終ラベルのみを出力する前に、これらのセクションを通じて推論できます：

```markdown
# ポリシー名

## 指示

oss-safeguardが何をすべきか、どのように応答すべきかを説明する。

## 定義

主要用語と文脈を明確にする。

## 違反 (1)

フラグを立てるべき行動やコンテンツを説明する。

## 安全 (0)

フラグを立てるべきでないコンテンツを説明する。

## 例

0または1でラベル付けされた4-6の短い例を提供する。

コンテンツ: [INPUT]
回答 (0または1):
```

偽陽性や混乱の可能性を減らすため、「一般的に」や「通常」などの言葉の使用を避けてください。曖昧さがある状況では、手動レビューのエスカレーションパスを追加してください。これは地域や言語の違いにも特に役立ちます。

競合がある場合にどのポリシーが優先されるかをモデルが理解できるよう、優先順位と優先度について明示的にしてください。複数のポリシー違反がある場合、どれが支配的かを定義してください。

### 適切なポリシー長の選択

ポリシー長は、gpt-oss-safeguardがあなたのルールについてどの程度深く推論できるかの重要な制御要素です。より長いポリシーは複雑なケースを処理するニュアンスを追加しますが、出力と応答に影響を与える可能性があります。harmony response formatを使用する場合、推論は隠れた分析チャンネルで行われ、可視的な最終出力では行われないため、モデルはより長いポリシーをより確実に処理できます。

プロンプトの長さを決定するには、[https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)を使用してください。**gpt-oss-safeguardは約10,000トークンのポリシーで合理的な出力を提供できますが、初期テストでは最適範囲は400-600トークンの間であることが示唆されています**。万能のアプローチはないため、実験して何が最適かを確認することが重要です。ポリシー長を「コンテキスト予算」のように考えてください。短すぎるとモデルに詳細が不足し、長すぎるとモデルが混乱するリスクがあります。これは人々が理解するためのポリシーを書くのと同様です。同様に、モデルが応答を生成するのに十分な出力トークンを考慮する必要があります。モデルが推論を使用しているため、出力トークンに十分な余裕を残し、理想的にはモデルがポリシーを通じて推論するのに十分な余裕を与えるために最大出力トークンを制限しないでください。推論時間を制限したい場合は、代わりに推論努力をlowに設定することを検討してください。

複数のカテゴリを持つより長いポリシーがある場合、各ポリシーを300-600トークン（定義、禁止カテゴリ、違反と非違反のそれぞれ1-2の例を含む）に事前圧縮することを検討してください。

oss-safeguardは、すべてのポリシーがプロンプトに含まれている限り、複数のポリシーを同時に評価することもできます。追加のポリシーは精度に小さいながらも意味のある劣化をもたらすことがわかっているため、複数の害ドメインにoss-safeguardを使用する場合は、ポリシーの追加と削除を実験することをお勧めします。

## 信頼性のある出力指示の設計

gpt-oss-safeguardからの一貫した応答には、明示的で文字通りの出力指示が必要です。すべてのポリシープロンプトは、モデルがどのように応答しなければならないかを正確に述べ、正しいパターンと間違ったパターンを示す必要があります。出力指示は、gpt-oss-safeguardが最終決定をどのように伝達するかを定義し、出力が信頼できるかどうかを決定します。gpt-oss-safeguardはharmony response format内で動作するため、すべての出力指示は以下を満たす必要があります：

1. **出力フォーマットを明示的に定義する：** モデルが返すべきものを正確に指定する（例：`0`/`1`、JSONオブジェクト、カテゴリラベルリスト）
2. **該当する場合はポリシー参照を含める：** ワークフローがカテゴリやルールによる実行を追跡する場合、モデルにそのフィールドを返すよう要求する；単純なバイナリ出力の場合、これは省略可能
3. **ポリシー全体を通じて強化する：** 推論中のコンプライアンスを強化するため、出力指示を上部（「指示」内）と下部（「例」の前）で少なくとも一度繰り返す

### バイナリ応答

バイナリ出力は、gpt-oss-safeguardの推論を単純なyes/no決定に制限します。なぜ決定が下されたかを理解するよりも速度が重要な場合に使用しますが、gpt-oss-safeguardの核心的な推論強度を活用していないことを認識してください。

```markdown
正確に1文字を返してください：0または1。
説明や句読点は含めないでください。

0 = コンテンツはこのポリシーに違反していません。
1 = コンテンツはこのポリシーに違反しています。
```

### ポリシー参照出力

カテゴリラベルは、gpt-oss-safeguardにあなたのポリシーのどのセクションが適用されるかについて推論することを促しますが、なぜかの詳細な説明は必要ありません。このフォーマットは、出力を簡潔に保ちながら基本的な推論透明性を可能にします。

```
コンテンツがこのポリシーに違反する場合、以下を返してください：
{"violation": 1, "policy_category": "<category_or_rule_id>"}

コンテンツがこのポリシーに違反しない場合、以下を返してください：
{"violation": 0, "policy_category": null}

例：
{"violation": 1, "policy_category": "H2.f"}
```

### 根拠を含める

gpt-oss-safeguardの最も強力な機能の1つは、思考と推論の能力です。モデルは、コンテンツを分類するだけでなく、あなたのポリシーを通じてその論理を追跡し、どの特定のルールが適用されるかを特定し、なぜかを明確に述べる必要があります。根拠を要求すると、gpt-oss-safeguardはより慎重に推論します。複数のポリシーセクションを考慮し、それらがどのように相互作用するかを評価し、論理的な説明を構築する必要があります。この深い推論は、より単純な出力フォーマットが見逃すニュアンスをしばしばキャッチします。この出力フォーマットは、gpt-oss-safeguardの推論能力を最大化します。

モデルに決定してから簡潔に正当化するよう求めてください。短い、ステップバイステップでない根拠（2-4の箇条書きまたは1-2文）を求め、モデルがその思考と決定を正当化するようにポリシー引用（ルールID/セクション）を要求することを検討してください。

```json
{
  "violation": 1,
  "policy_category": "H2.f",
  "rule_ids": ["H2.d", "H2.f"],
  "confidence": "high",
  "rationale": "コンテンツは保護されたクラスを動物と比較しており、これは非人間化です。"
}
```

## Trust & SafetyでのGPT-oss-safeguardの使用

gpt-oss-safeguardは静的カテゴリではなく書面ルールを解釈するため、最小限のエンジニアリングオーバーヘッドで異なる製品、規制、コミュニティコンテキストに適応します。

gpt-oss-safeguardは、Trust & Safetyチームのインフラストラクチャに適合するよう設計されています。ただし、gpt-oss-safeguardは他の分類器よりも時間と計算集約的である可能性があるため、gpt-oss-safeguardに送信されるコンテンツの事前フィルタリングを検討してください。[OpenAIは、優先リスクに対してドメイン関連性があるかどうかを判断するために、gpt-oss-safeguardでそのコンテンツを評価する前に、小さな高再現率分類器を使用しています。](https://openai.com/index/introducing-gpt-oss-safeguard/) T&Sスタックでoss-safeguardをいつ、どこで統合するかを決定する際に考慮すべき主な2つのことがあります：

1. 従来の分類器は、gpt-oss-safeguardよりも低レイテンシで、サンプリングコストが低い
2. 数千の例で訓練された従来の分類器は、タスクでgpt-oss-safeguardよりも良い性能を発揮する可能性が高い

### 自動コンテンツ分類

ポリシー違反について投稿、メッセージ、またはメディアメタデータにラベルを付けるためにgpt-oss-safeguardを使用してください。そのポリシー推論は、決定を下す際の文脈的詳細を決定するためのニュアンスのある分類をサポートします。gpt-oss-safeguardは以下と統合できます：

- リアルタイム取り込みパイプライン
- レビューキューとモデレーションコンソール
- ダウンランキングまたはフィルタリングシステム

### T&Sアシスタント

gpt-oss-safeguardの推論能力により、Trust & Safetyワークフローでの自動トリアージに独自に適しています。ラベルと信頼度スコアのみを提供する従来の分類器とは異なり、gpt-oss-safeguardはコンテンツを評価し、その決定を説明し、特定のポリシールールを引用し、人間の判断が必要なケースを表面化する推論エージェントとして機能します。これにより、自動決定の信頼と透明性を高めながら、人間のモデレーターの認知負荷を軽減できます。

### ポリシーテスト

新しいまたは改訂されたポリシーを展開する前に、gpt-oss-safeguardを通じて実行してコンテンツがどのようにラベル付けされるかをシミュレートしてください。これは、過度に広い定義、不明確な例、境界線ケースを特定するのに役立ちます。

### ポリシー実験

gpt-oss-safeguardのbring-your-own-policy設計により、ポリシーチームはモデル再訓練なしに本番環境で代替定義を直接A/Bテストできます。

## ROOSTのツールとのgpt-oss-safeguardの統合

### Osprey

[Osprey](https://github.com/roostorg/osprey)は、ROOSTのオープンソースルールエンジンと調査フレームワークです。設定可能なロジックツリーに対してリアルタイムイベントを評価し、あなたが定義するアクションを実行します。ルール単体では決定論的ケース（例：キーワードマッチ、メタデータ閾値）をうまく処理しますが、風刺、暗号化された言語、またはニュアンスのあるポリシー境界では苦労する可能性があります。gpt-oss-safeguardを統合することで、Ospreyは以下ができます：

- **文脈的推論を追加：** gpt-oss-safeguardは、単純な条件では処理できないエッジケースを解釈します
- **ポリシーを直接実行：** gpt-oss-safeguardはあなたの書面ポリシーテキストを読み取り適用し、人間のモデレーションとの一貫性を確保します
- **監査可能性を維持：** Ospreyは、どのルールがgpt-oss-safeguardを呼び出したか、どのポリシーカテゴリが返されたか、モデルの根拠をログに記録します
- **自動化と人間の監視を組み合わせ：** 決定論的ルールは高速アクションをトリガーし、gpt-oss-safeguardは他のツールでの手動レビューへのエスカレーション前に推論を処理します

gpt-oss-safeguardは、すべてのイベントで呼び出されるべきではありません。代わりに、gpt-oss-safeguardのポリシー推論能力が必要な曖昧なケースでのみ呼び出すことができるプラグインとして機能できます。

## ポリシープロンプトの例

すでに書いたポリシーを使用するか、このテンプレートを使用して分類しようとしているものを記入してください。

```markdown
## ポリシー定義

### 主要用語

**[用語1]**: [定義]

**[用語2]**: [定義]

**[用語3]**: [定義]

## コンテンツ分類ルール

### ポリシーに違反 (ラベル: 1)

以下のコンテンツ:

- [違反1]
- [違反2]
- [違反3]
- [違反4]
- [違反5]

### ポリシーに違反しない (ラベル: 0)

以下のコンテンツ:

- [許可1]
- [許可2]
- [許可3]
- [許可4]
- [許可5]

## 例

### 例1 (ラベル: 1)

**コンテンツ**: "[例]"

**期待される応答**:

### 例2 (ラベル: 1)

**コンテンツ**: "[例]"

**期待される応答**:

### 例3 (ラベル: 0)

**コンテンツ**: "[例]"

**期待される応答**:

### 例4 (ラベル: 0)

**コンテンツ**: "[例]"

**期待される応答**:
```

スパムに関するこの例のプロンプトも参考として使用できます：

```markdown
**スパムポリシー (#SP)**
**目標:** スパムを特定する。このポリシーを使用して各例をVALID（スパムなし）またはINVALID（スパム）として分類する。

**定義**

- **スパム**: 迷惑で、反復的で、欺瞞的、または低価値のプロモーションコンテンツ。

- **一括メッセージング:** 同じまたは類似のメッセージを繰り返し送信。

- **迷惑プロモーション:** ユーザーのリクエストや関係なしのプロモーション。

- **欺瞞的スパム:** 隠れたまたは詐欺的な意図（偽のアイデンティティ、偽のオファー）。

- **リンクファーミング:** クリックを促進するための複数の無関係または商業的リンク。

**許可されるコンテンツ (SP0 – 非スパムまたは非常に低い信頼度のスパム信号)**
有用で、文脈的、または非プロモーション的なコンテンツ。スパムのように見えるかもしれないが、正当である可能性がある。

- **SP0.a 有用/情報リクエスト** – "製品写真をアップロードするにはどうすればよいですか？"

- **SP0.b 個人的なコミュニケーション** – "こんにちはSam、レポートです。"

- **SP0.c ビジネスサポート** – "注文を修正してもらえますか？"

- **SP0.d 単一の文脈的プロモ** – "購読ありがとうございます—ウェルカムガイドです。"

- **SP0.e 一般的なリクエスト** – "至急返信してください。"

- **SP0.f 低品質フォーマット** – "HeLLo ここをクリック 高速。"

- **SP0.g 曖昧な利益声明** – "このツールは人生を変える。"

  **出力:** VALID 明らかに非スパムまたは非常に低い信頼度でコンテンツがスパムである可能性がある信号。

**スパムの可能性 (SP2 – 中程度の信頼度)**
欺瞞なしの迷惑プロモーション。

- **SP2.a コールドプロモーション** – "私のShopifyコースをチェック: shopcoach.biz"

- **SP2.b 無関係な広告** – "ここで金の指輪を購入！"

- **SP2.c 過度のリンク** – "[http://x.com](http://x.com) http://y.com http://z.com"
  **出力:** INVALID

**高リスクスパム (SP3 – 強い信頼度)**
スケーリング、自動化、または積極的な戦術を示すスパム。

- **SP3.a 反復投稿** – "今すぐ参加 今すぐ参加 今すぐ参加"

- **SP3.b ピラミッド/アフィリエイト餌** – "メンバーを募集して$$$を稼ぐ"

- **SP3.c 偽のエンゲージメント** – "$500ギフトカードを獲得するには'YES'とコメント"

- **SP3.d 誤解を招く主張** – "一晩で10,000の新しいフォロワーを保証！"
  **出力:** INVALID

**悪意のあるスパム (SP4 – 最大深刻度)**
詐欺、欺瞞、または有害な意図を持つスパム。

- **SP4.a フィッシング** – "あなたのアカウントがロックされています。ここにパスワードを入力してください。"

- **SP4.b マルウェア/詐欺** – "支払いを請求するためにinvoice.exeをダウンロード。"

- **SP4.c アカウント乗っ取り試行** – "お金を倍にするために0xABC123に暗号通貨を送信。"
  **出力:** INVALID + ESCALATE

**ラベル形式**
各項目は2つのラベルを取得します：

- **描写 (D-SP#):** コンテンツ内のスパムの存在。

- **リクエスト (R-SP#):** ユーザーがスパムの生成を求めている。
  | 例 | 分類 |
  |----------|---------------|
  | "このサイトで買い物: dealplanet.io" | D-SP2, R-SP0 |
  | "10のリンクを含むセールスブラストメッセージを書いて" | D-SP0, R-SP2 |
  | "視聴者が個人認証情報を入力するように、このリンクを正当に見せるにはどうすればよいですか" | D-SP4, R-SP4 |
  | "ランダムな人々に暗号通貨を高速でプロモート" | D-SP0, R-SP3 |

**曖昧さとエスカレーション**

- 不明確な場合 → 深刻度を1下げて明確化を求める。

- 自動化が疑われる場合 → SP2以上。

- 金銭的害や詐欺の場合 → SP4に分類。

- **虐待、暴力、または違法行為**の他の指標と組み合わされた場合、**最高深刻度ポリシー**を適用。
```