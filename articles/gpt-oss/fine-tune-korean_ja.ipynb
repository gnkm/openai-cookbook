{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538f25ce",
   "metadata": {},
   "source": [
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€OpenAIã®**gpt-ossï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆï¼‰**ãƒ¢ãƒ‡ãƒ«ã‚’**éŸ“å›½ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¿ã‚¤ãƒ« + æœ€æ–°ã®ä¼šè©±èª¿**ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’**éŸ“å›½èª/è‹±èª**ã®**äºŒè¨€èª**ã§æä¾›ã—ã¾ã™ã€‚  \n",
    "This notebook shows how to fineâ€‘tune OpenAI's **gpt-oss (openâ€‘weight)** models for **Korean news style + modern chat tone**, in **Korean & English**.\n",
    "\n",
    "---\n",
    "\n",
    "### MXFP4ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼èª¬æ˜ Â· MXFP4 ì›Œí¬í”Œë¡œ ì •ë¦¬\n",
    "\n",
    "**EN:**  \n",
    "- Training or fine-tuning **directly in MXFP4 is not supported** by public frameworks today.  \n",
    "- Recommended path: train in **BF16** (or **QLoRA 4â€‘bit nf4**) â†’ **merge LoRA** â†’ **postâ€‘training quantize to MXFP4** â†’ `save_pretrained()` for deployment.  \n",
    "- If you need an MXFP4 artifact, you must **reâ€‘quantize from BF16** after merging adapters. (Export utilities are evolving; if your toolchain already supports MXFP4 serialization, that's ideal.)\n",
    "\n",
    "**JP:**  \n",
    "- ç¾åœ¨ã€å…¬é–‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã¯**MXFP4ã§ã®ç›´æ¥å­¦ç¿’/ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚  \n",
    "- æ¨å¥¨ãƒ‘ã‚¹ï¼š**BF16**ï¼ˆã¾ãŸã¯**QLoRA 4â€‘bit nf4**ï¼‰ã§å­¦ç¿’ â†’ **LoRAãƒãƒ¼ã‚¸** â†’ **å­¦ç¿’å¾ŒMXFP4é‡å­åŒ–** â†’ ãƒ‡ãƒ—ãƒ­ã‚¤ç”¨ã«`save_pretrained()`ã§ä¿å­˜ã€‚  \n",
    "- MXFP4ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãŒå¿…è¦ãªå ´åˆã¯ã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒãƒ¼ã‚¸å¾Œã«**BF16ã‹ã‚‰MXFP4ã¸ã®å†é‡å­åŒ–**ãŒå¿…è¦ã§ã™ã€‚ï¼ˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã¯é€²åŒ–ä¸­ã§ã€ãƒ„ãƒ¼ãƒ«ãƒã‚§ãƒ¼ãƒ³ãŒMXFP4ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚Œã°ç†æƒ³çš„ã§ã™ã€‚ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### LoRAã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆMoEï¼‰Â· LoRA íƒ€ê¹ƒ(MoE í¬í•¨)\n",
    "\n",
    "**EN:**  \n",
    "- Minimal config (fast, low VRAM): target attention only, e.g. `[\"q_proj\",\"v_proj\"]`.  \n",
    "- MoEâ€‘aware config (better domain adaptation, more VRAM/time): include **expert projection layers** in addition to attention.  \n",
    "\n",
    "```python\n",
    "from peft import LoraConfig\n",
    "\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # baseline\n",
    "MOE_TARGET_PARAMETERS = [\n",
    "    # example expert layers; adjust indices to your model depth\n",
    "    \"mlp.experts.gate_up_proj\",\n",
    "    \"mlp.experts.down_proj\",\n",
    "]\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",              # cover all linear layers\n",
    "    target_parameters=MOE_TARGET_PARAMETERS,  # add expert projections\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "```\n",
    "\n",
    "- Start with attentionâ€‘only; if KR domain fit is insufficient, enable MoE targets and reâ€‘eval.\n",
    "\n",
    "**JP:**  \n",
    "- æœ€å°æ§‹æˆï¼ˆé«˜é€Ÿã€ä½VRAMï¼‰ï¼š`[\"q_proj\",\"v_proj\"]`ãªã©**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ã¿**ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã€‚  \n",
    "- **MoEå¯¾å¿œæ§‹æˆ**ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œæ€§å‘ä¸Šã€VRAM/æ™‚é–“æ¶ˆè²»å¢—ï¼‰ï¼šã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«åŠ ãˆã¦**ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæŠ•å½±ãƒ¬ã‚¤ãƒ¤ãƒ¼**ã‚’å«ã‚ã‚‹ã€‚  \n",
    "- ã¾ãšã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ã¿ã§é–‹å§‹ã—ã€éŸ“å›½èªãƒ‰ãƒ¡ã‚¤ãƒ³ã®é©åˆæ€§ãŒä¸ååˆ†ãªå ´åˆã¯ã€MoEã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æœ‰åŠ¹ã«ã—ã¦å†è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c12ff",
   "metadata": {},
   "source": [
    "## ç›®æ¬¡ Â· ëª©ì°¨\n",
    "0) ç›®æ¨™ã¨ç¯„å›² Â· ëª©í‘œ & ë²”ìœ„  \n",
    "1) ç’°å¢ƒãƒã‚§ãƒƒã‚¯ Â· í™˜ê²½ ì ê²€  \n",
    "2) è¨­å®šå€¤ Â· Config  \n",
    "3) ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« Â· Install Deps  \n",
    "4) ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚·ãƒ³ã‚°ï¼ˆéŸ“å›½å‹ï¼‰ Â· KRâ€‘Context Data Sourcing  \n",
    "5) ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ Â· Create Sample Data  \n",
    "6) å‰å‡¦ç†ï¼ˆPIPAï¼‰& ã‚¹ã‚¿ã‚¤ãƒ«ãƒ©ãƒ™ãƒ« Â· PII Scrubbing & Style Tags  \n",
    "7) ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°/ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ Â· Load & Format  \n",
    "8) ãƒ¢ãƒ‡ãƒ«/ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰ Â· Load Model & Tokenizer  \n",
    "9) Fineâ€‘Tuningï¼ˆLoRA/QLoRAï¼‰ Â· ì„¸ë°€ íŠœë‹  \n",
    "   9a) Data curation & splits  \n",
    "   9b) Hyperparametersï¼ˆr/alpha/dropoutï¼‰  \n",
    "   9c) Merge adaptersï¼ˆBF16ï¼‰  \n",
    "   9d) Save merged BF16ï¼ˆ`save_pretrained`ï¼‰  \n",
    "   9e) Export & Quantizeï¼ˆBF16 â†’ MXFP4ï¼‰ Â· ë‚´ë³´ë‚´ê¸° & ì–‘ìí™”  \n",
    "10) è©•ä¾¡ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹/ãƒãƒ£ãƒƒãƒˆï¼‰ Â· Evaluation (News/Chat)  \n",
    "11) Inference Prompt Templates Â· ì¶”ë¡  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿  \n",
    "12) æœ€æ–°æ€§ç¶­æŒ Â· Freshness Strategy  \n",
    "13) å®‰å…¨æ€§/ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ Â· Safety & Compliance  \n",
    "14) ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° & æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— Â· Troubleshooting & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8655d2",
   "metadata": {},
   "source": [
    "### âš™ï¸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° vs é‡å­åŒ– â€” ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å†…å®¹\n",
    "- **æ¨å¥¨:** BF16/FP16ã¾ãŸã¯QLoRAã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ãƒãƒ¼ã‚¸ã•ã‚ŒãŸé‡ã¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã€‚\n",
    "- **ãã®å¾Œ:** æä¾›ã•ã‚Œã¦ã„ã‚‹å¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ/ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨ã—ã¦ã€æ¨è«–ç”¨ã«**MXFP4**ã«é‡å­åŒ–ã™ã‚‹ã€‚\n",
    "- **éæ¨å¥¨:** ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ã€ŒMXFP4ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œã‚’è©¦ã¿ã‚‹ã“ã¨ â€” ç¾åœ¨ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24a3d9",
   "metadata": {},
   "source": [
    "> **PIIãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹æ³¨æ„äº‹é …:** KRãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ­ã‚°è¨˜éŒ²ã‚’è¡Œã†**å‰ã«**ã€ä¼æ¥­ãƒãƒªã‚·ãƒ¼ã«å¾“ã£ã¦ãã ã•ã„ï¼ˆä½æ°‘ç™»éŒ²ç•ªå·/é›»è©±ç•ªå·/ã‚¢ã‚«ã‚¦ãƒ³ãƒˆIDã‚’ãƒã‚¹ã‚¯åŒ–ã—ã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å‰Šé™¤ï¼‰ã€‚train/val/testã®åˆ†å‰²ã¯ã€ã‚½ãƒ¼ã‚¹ã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚¿ã‚°ã«ã‚ˆã£ã¦å±¤åŒ–ã‚’ä¿ã¤ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e883f5",
   "metadata": {},
   "source": [
    "### ğŸ§ª MoEã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "MoEãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒã€ã“ã‚Œã¯**ä¸Šç´šè€…å‘ã‘/å®Ÿé¨“çš„**ãªæ©Ÿèƒ½ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„ã€‚ã¾ãšæ³¨æ„æ©Ÿæ§‹ã®å°„å½±ã‹ã‚‰å§‹ã‚ã¦ã€ã‚¹ã‚³ãƒ¼ãƒ—ã‚’æ‹¡å¼µã™ã‚‹å‰ã«KRãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179543e6",
   "metadata": {},
   "source": [
    "> **æ³¨æ„:** `transformers`ã€`peft`ã€`accelerate`ã€`trl`ã¯ã€BF16/4ãƒ“ãƒƒãƒˆLoRAã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¦ã„ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å›ºå®šã—ã¦ãã ã•ã„ã€‚  \n",
    "`safetensors`ã‚’å›ºå®šã™ã‚‹å ´åˆã¯ã€**ãƒã‚¤ãƒ†ã‚£ãƒ–MXFP4ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã¾ã æ¨™æº–åŒ–ã•ã‚Œã¦ã„ãªã„**ã“ã¨ã‚’è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ã€‚ãƒ­ãƒ¼ãƒ€ãƒ¼ã¯å†…éƒ¨çš„ã«ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e743f0",
   "metadata": {},
   "source": [
    "### ğŸ” ã‚µãƒãƒ¼ãƒˆãƒãƒˆãƒªãƒƒã‚¯ã‚¹ â€” ä¸€ç›®ã§ã‚ã‹ã‚‹æ¦‚è¦\n",
    "- **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç²¾åº¦:** BF16/FP16 âœ… Â· QLoRA 4â€‘bit âœ… Â· **MXFP4 FT âŒ**\n",
    "- **é‡å­åŒ–å¯¾è±¡:** MXFP4 âœ… (äº‹å¾Œå­¦ç¿’)\n",
    "- **OSSãƒ¢ãƒ‡ãƒ«å‘ã‘API FT (ãƒ›ã‚¹ãƒˆå‹):** âŒ\n",
    "- **ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ FT (Transformers/TRL/PEFT):** âœ…\n",
    "- **LoRAå¯¾è±¡:** `q_proj`, `k_proj`, `v_proj`, `o_proj` âœ…; MoEã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ **å®Ÿé¨“çš„** âš ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dec1f6",
   "metadata": {},
   "source": [
    "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ç¿»è¨³ã™ã¹ããƒ†ã‚­ã‚¹ãƒˆãŒæä¾›ã•ã‚Œã¦ã„ãªã„ã‚ˆã†ã§ã™ã€‚ã€Œ---ã€ã®å¾Œã«ç¿»è¨³ã—ãŸã„è‹±èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
    "\n",
    "æŠ€è¡“æ–‡æ›¸ã®ç¿»è¨³ã‚’è¡Œã†æº–å‚™ã¯ã§ãã¦ãŠã‚Šã¾ã™ã®ã§ã€ç¿»è¨³ã—ãŸã„å†…å®¹ã‚’ãŠé€ã‚Šãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d489c2",
   "metadata": {},
   "source": [
    "## 0) ç›®æ¨™ã¨ç¯„å›² Â· ëª©í‘œ & ë²”ìœ„\n",
    "- **KR**: éŸ“å›½èªã®ä¸€èˆ¬ãƒ‹ãƒ¥ãƒ¼ã‚¹ + æ—¥å¸¸/ç›¸è«‡å¯¾è©±ä½“ã«æœ€é©åŒ–ã€‚`style=news_headline|news_lead|news_body|kakao_casual|kakao_formal` ã§åˆ¶å¾¡ã€‚\n",
    "- **EN**: éŸ“å›½èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹åŸ·ç­†ã¨ç¾ä»£çš„ãªãƒãƒ£ãƒƒãƒˆãƒˆãƒ¼ãƒ³ã«æœ€é©åŒ–ï¼›ä¸Šè¨˜ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚¿ã‚°ã§å‡ºåŠ›ã‚’åˆ¶å¾¡ã€‚\n",
    "- **ã‚¹ã‚¿ãƒƒã‚¯**: `transformers`ã€`trl(SFTTrainer)`ã€`peft(LoRA/QLoRA)`ã€`datasets`ã€‚\n",
    "- **ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢**: å˜ä¸€/å°‘æ•°GPUï¼ˆBF16æ¨å¥¨ï¼‰ã€‚è»½é‡ãƒ†ã‚¹ãƒˆç”¨ã«CPU/Macã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97218d",
   "metadata": {},
   "source": [
    "## 1) Environment check Â· ç’°å¢ƒãƒã‚§ãƒƒã‚¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5babb2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\n",
      "OS/Platform: Linux-6.8.0-60-generic-x86_64-with-glibc2.35\n",
      "CUDA_VISIBLE_DEVICES: \n",
      "Torch: 2.7.1+cu126 CUDA: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import os, sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"OS/Platform:\", platform.platform())\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\"))\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "except Exception as e:\n",
    "    print(\"Torch not installed or GPU not detected:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25688688",
   "metadata": {},
   "source": [
    "## 2) è¨­å®šå€¤ Â· Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c15817f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# === Model & Training Params ===\n",
    "BASE_URL = \"http://localhost:8000/v1\"     # vLLM OpenAI-compatible endpoint\n",
    "API_KEY  = \"dummy-key\"                     # vLLM ignores; SDK requires a value\n",
    "MODEL    = \"openai/gpt-oss-120b\"           # must match the model vLLM loaded\n",
    "OUTPUT_DIR = \"ft-oss-kr-news-chat-bilingual\"\n",
    "\n",
    "# Data mix (news : chat)\n",
    "MIX_NEWS = 0.6\n",
    "MIX_CHAT = 0.4\n",
    "\n",
    "# LoRA\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # adjust per model\n",
    "\n",
    "# Training\n",
    "EPOCHS = 1\n",
    "PER_DEVICE_BS = 2\n",
    "GRAD_ACCUM = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "BF16 = True\n",
    "LOG_STEPS = 20\n",
    "SAVE_STEPS = 200\n",
    "SAVE_TOTAL_LIMIT = 2\n",
    "\n",
    "print(\"Config ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f258eb",
   "metadata": {},
   "source": [
    "## 3) ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« Â· Install Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b75968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.55.3\n",
      "accelerate: 1.10.0\n",
      "datasets: 4.0.0\n",
      "peft: not installed\n",
      "trl: 0.21.0\n",
      "bitsandbytes: not installed\n",
      "sentencepiece: 0.2.1\n",
      "vllm: 0.10.1\n",
      "llama_cpp: 0.3.16\n",
      "pip: 25.2\n",
      "Install cells are commented. Un-comment in your environment.\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install transformers accelerate datasets peft trl bitsandbytes sentencepiece\n",
    "# (optional) serving/runtimes\n",
    "# %pip install vllm\n",
    "# %pip install llama-cpp-python\n",
    "\n",
    "import importlib, pip\n",
    "\n",
    "for dep in [\"transformers\",\"accelerate\",\"datasets\",\"peft\",\"trl\",\n",
    "            \"bitsandbytes\",\"sentencepiece\",\"vllm\",\"llama_cpp\"]:\n",
    "    try:\n",
    "        print(f\"{dep}: {importlib.import_module(dep).__version__}\")\n",
    "    except Exception:\n",
    "        print(f\"{dep}: not installed\")\n",
    "\n",
    "print(f\"pip: {pip.__version__}\")\n",
    "\n",
    "print(\"Install cells are commented. Un-comment in your environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8647fd",
   "metadata": {},
   "source": [
    "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€æä¾›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¯éŸ“å›½èªã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚ç§ã¯è‹±èªã‹ã‚‰æ—¥æœ¬èªã¸ã®ç¿»è¨³ã‚’å°‚é–€ã¨ã—ã¦ãŠã‚Šã€éŸ“å›½èªã®ç¿»è¨³ã¯å¯¾å¿œã—ã¦ãŠã‚Šã¾ã›ã‚“ã€‚\n",
    "\n",
    "è‹±èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã”æä¾›ã„ãŸã ã‘ã‚Œã°ã€å–œã‚“ã§æ—¥æœ¬èªã«ç¿»è¨³ã„ãŸã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22cbd6",
   "metadata": {},
   "source": [
    "**KR**  \n",
    "- å…¬é–‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆãƒˆãƒ”ãƒƒã‚¯åˆ†é¡/è¦ç´„/QAï¼‰+ **è¨±å¯ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹APIã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¿ã‚¤ãƒˆãƒ«/è¦ç´„/ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰** ã‚’ä¸­å¿ƒã¨ã—ãŸã‚¹ã‚¿ã‚¤ãƒ«è£œæ­£ã€‚\n",
    "- è¨˜äº‹**åŸæ–‡ã®å¤§é‡å†å­¦ç¿’ã¯è‘—ä½œæ¨©/åˆ©ç”¨è¦ç´„ã®å•é¡Œ** â†’ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ»å…¬é–‹ã‚³ãƒ¼ãƒ‘ã‚¹ä¸­å¿ƒã€‚\n",
    "- å¯¾è©±ä½“ã¯åˆæ³•çš„ãªå…¬é–‹ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆã‚¿ãƒ¡å£/æ•¬èª/çµµæ–‡å­—/ç•¥èªãƒ©ãƒ™ãƒ«å«ã‚€ï¼‰ã‚’å„ªå…ˆã€‚\n",
    "- PIPA: ä½æ°‘ç•ªå·/é€£çµ¡å…ˆ/ãƒ¡ãƒ¼ãƒ«/å£åº§ç­‰ã®å€‹äººæƒ…å ±ã¯**è¨“ç·´å‰/ãƒ­ã‚°å‰**ã«ã‚¹ã‚¯ãƒ©ãƒ“ãƒ³ã‚°ã€‚\n",
    "\n",
    "**EN**  \n",
    "- å…¬é–‹KRãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆãƒˆãƒ”ãƒƒã‚¯åˆ†é¡/è¦ç´„/QAï¼‰ã¨**è¨±å¯ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹APIãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**ã‚’ã‚¹ã‚¿ã‚¤ãƒ«èª¿æ•´ã«å„ªå…ˆä½¿ç”¨ã€‚\n",
    "- ãƒ©ã‚¤ã‚»ãƒ³ã‚¹/åˆ©ç”¨è¦ç´„ã®åˆ¶ç´„ã«ã‚ˆã‚Šã€ãƒ‹ãƒ¥ãƒ¼ã‚¹å…¨æ–‡ã§ã®å¤§é‡è¨“ç·´ã¯é¿ã‘ã‚‹ï¼›ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ + å…¬é–‹ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã€‚\n",
    "- ãƒãƒ£ãƒƒãƒˆç”¨é€”ã§ã¯ã€ãƒˆãƒ¼ãƒ³/çµµæ–‡å­—/éå…¬å¼-å…¬å¼ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãã®åˆæ³•çš„ãªå…¬é–‹ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã€‚\n",
    "- è¨“ç·´/ãƒ­ã‚°è¨˜éŒ²å‰ã«PIIï¼ˆé›»è©±ç•ªå·ã€ä½æ°‘ç™»éŒ²ç•ªå·ã€ãƒ¡ãƒ¼ãƒ«ã€å£åº§ï¼‰ã‚’ã‚¹ã‚¯ãƒ©ãƒ“ãƒ³ã‚°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b918411",
   "metadata": {},
   "source": [
    "## 5) ìƒ˜í”Œ ë°ì´í„° ìƒì„± Â· ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18db10a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: data/news.jsonl, data/chat.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib\n",
    "pathlib.Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "news_samples = [\n",
    "  {\"style\":\"news_lead\",\"topic\":\"ê²½ì œ\",\"title\":\"ë°˜ë„ì²´ ìˆ˜ì¶œ í˜¸ì¡°â€¦ 7ì›” ìˆ˜ì¶œì•¡ 20% ì¦ê°€\",\"summary\":\"ìˆ˜ì¶œ ê°œì„ ì„¸ê°€ ì´ì–´ì§€ë©° ê²½ê¸° íšŒë³µ ê¸°ëŒ€ê°€ ì»¤ì¡Œë‹¤.\"},\n",
    "  {\"style\":\"news_headline\",\"topic\":\"ì •ì¹˜\",\"title\":\"êµ­íšŒ, ë°ì´í„° ì‚°ì—… ìœ¡ì„±ë²• ë³¸íšŒì˜ í†µê³¼\",\"summary\":\"ë°ì´í„° í™œìš© ì´‰ì§„ê³¼ ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ê°•í™”í•˜ëŠ” ë‚´ìš©.\"},\n",
    "  {\n",
    "    \"style\": \"news_lead\",\n",
    "    \"topic\": \"ê²½ì œ\",\n",
    "    \"title\": \"ì¹´ì¹´ì˜¤í˜ì´ ë³´ì•ˆ ì ê²€â€¦ ê³ ê°ë¬¸ì˜: help+vip@corp.co.kr\",\n",
    "    \"summary\": \"ê³ ê°ì„¼í„° 010-1234-5678ë¡œ ë¬¸ì˜ í­ì£¼. ê³„ì¢Œ 110-123-456789 ê´€ë ¨ ê²°ì œ ì˜¤ë¥˜ ë…¼ë€.\"\n",
    "  },\n",
    "  {\n",
    "    \"style\": \"news_headline\",\n",
    "    \"topic\": \"ì‚¬íšŒ\",\n",
    "    \"title\": \"ê°œì¸ì •ë³´ ìœ ì¶œ ì˜í˜¹â€¦ ì£¼ë¯¼ë²ˆí˜¸ 901010-1234567 ìœ í†µ ì£¼ì¥\",\n",
    "    \"summary\": \"ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123ì—ì„œ ìë£Œ í™•ë³´â€¦ ë‹´ë‹¹ì john.doe+news@example.com\"\n",
    "  }\n",
    "]\n",
    "\n",
    "chat_samples = [\n",
    "  {\"style\":\"kakao_casual\",\"dialog\":[\"ì£¼ë§ì— ë¹„ ì˜¨ëŒ€?\",\"ì‘ ì¼ìš”ì¼ì— ê½¤ ì˜¨ë‹¤ë”ë¼ â˜”\",\"í— ìš°ì‚° ì±™ê²¨ì•¼ê² ë‹¤\"]},\n",
    "  {\"style\":\"kakao_formal\",\"dialog\":[\"ì•ˆë…•í•˜ì„¸ìš”. ë°°ì†¡ ì¼ì • í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\",\"ë‚´ì¼ ì¤‘ ë„ì°© ì˜ˆì •ì…ë‹ˆë‹¤.\",\"ì•ˆë‚´ ê°ì‚¬í•©ë‹ˆë‹¤.\"]},\n",
    "  {\n",
    "    \"style\": \"kakao_formal\",\n",
    "    \"dialog\": [\n",
    "      \"ë°°ì†¡ í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ì£¼ë¬¸ë²ˆí˜¸ ORD-2025-0001 ì…ë‹ˆë‹¤.\",\n",
    "      \"ì—°ë½ì²˜ëŠ” 010-2222-3333 ì…ë‹ˆë‹¤. (ìœ ë‹ˆì½”ë“œ í•˜ì´í”ˆ)\",\n",
    "      \"ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ëŠ” ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "with open(\"data/news.jsonl\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for ex in news_samples: f.write(json.dumps(ex, ensure_ascii=False)+\"\\n\")\n",
    "with open(\"data/chat.jsonl\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for ex in chat_samples: f.write(json.dumps(ex, ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "print(\"Created: data/news.jsonl, data/chat.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1eaa27",
   "metadata": {},
   "source": [
    "## 6) å‰å‡¦ç†(PIPA) & ã‚¹ã‚¿ã‚¤ãƒ«ãƒ©ãƒ™ãƒ« Â· PII Scrubbing & Style Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430c1b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/news.jsonl -> data/news_clean.jsonl | rows: 4, redacted_rows: 2, hits: {'[EMAIL]': 2, '[ACCOUNT]': 1, '[RRN]': 1, '[CITY]': 1}\n",
      "data/chat.jsonl -> data/chat_clean.jsonl | rows: 3, redacted_rows: 1, hits: {'[PHONE]': 1}\n"
     ]
    }
   ],
   "source": [
    "# Step 6 â€” PII scrubbing + style tags (no Harmony here)\n",
    "import json, re, unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Normalization helpers ---\n",
    "HYPHENS = dict.fromkeys(map(ord, \"â€-â€’â€“â€”â€•ï¹˜ï¹£ï¼\"), ord(\"-\"))  # map unicode hyphens â†’ ASCII\n",
    "def normalize(s: str) -> str:\n",
    "    if not isinstance(s, str): return s\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.translate(HYPHENS)\n",
    "    return s\n",
    "\n",
    "# --- PII patterns (illustrative; tune for production) ---\n",
    "RE_EMAIL = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "# KR mobile numbers with spaces/hyphens: 010-1234-5678, 010 1234 5678, etc.\n",
    "RE_PHONE = re.compile(r\"\\b01[016789][-\\s]?\\d{3,4}[-\\s]?\\d{4}\\b\")\n",
    "# Korean RRN (ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸) basic pattern\n",
    "RE_RRN = re.compile(r\"\\b\\d{6}-\\d{7}\\b\")\n",
    "# Bank-ish account numbers: strictly digits in groups (avoid codes with letters)\n",
    "RE_ACCOUNT = re.compile(r\"\\b\\d{2,3}-\\d{2,4}-\\d{3,6}\\b\")\n",
    "# Very simple postal address cue (city names) â€“ conservative, just redact the token (optional)\n",
    "RE_CITY = re.compile(r\"(ì„œìš¸íŠ¹ë³„ì‹œ|ë¶€ì‚°ê´‘ì—­ì‹œ|ëŒ€êµ¬ê´‘ì—­ì‹œ|ì¸ì²œê´‘ì—­ì‹œ|ê´‘ì£¼ê´‘ì—­ì‹œ|ëŒ€ì „ê´‘ì—­ì‹œ|ìš¸ì‚°ê´‘ì—­ì‹œ|ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ|ê²½ê¸°ë„|ê°•ì›ë„|ì¶©ì²­ë¶ë„|ì¶©ì²­ë‚¨ë„|ì „ë¼ë¶ë„|ì „ë¼ë‚¨ë„|ê²½ìƒë¶ë„|ê²½ìƒë‚¨ë„|ì œì£¼íŠ¹ë³„ìì¹˜ë„)\")\n",
    "\n",
    "# Allowlist: things that look like PII but arenâ€™t (e.g., bill/order codes w/ letters)\n",
    "def looks_like_code(s: str) -> bool:\n",
    "    return bool(re.search(r\"[A-Za-z]\", s))  # if letters present, treat as code, not account/phone\n",
    "\n",
    "# Order of application matters (longest/most specific first sometimes helps)\n",
    "SCRUBBERS = [\n",
    "    (\"[RRN]\", RE_RRN),\n",
    "    (\"[EMAIL]\", RE_EMAIL),\n",
    "    (\"[PHONE]\", RE_PHONE),\n",
    "    (\"[ACCOUNT]\", RE_ACCOUNT),\n",
    "    (\"[CITY]\", RE_CITY),  # optional; comment out if you don't want to redact city tokens\n",
    "]\n",
    "\n",
    "def scrub_text(text: str) -> tuple[str, dict]:\n",
    "    \"\"\"Return (scrubbed_text, hits_dict). Avoid false positives with basic allowlisting.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return text, {}\n",
    "    orig = text\n",
    "    text = normalize(text)\n",
    "    hits = {}\n",
    "\n",
    "    # Guard account-like and phone-like strings that contain letters (likely codes)\n",
    "    guarded = set()\n",
    "    for m in RE_ACCOUNT.finditer(text):\n",
    "        if looks_like_code(m.group(0)):\n",
    "            guarded.add(m.span())\n",
    "    for m in RE_PHONE.finditer(text):\n",
    "        if looks_like_code(m.group(0)):\n",
    "            guarded.add(m.span())\n",
    "\n",
    "    # Apply scrubs\n",
    "    for label, pattern in SCRUBBERS:\n",
    "        out = []\n",
    "        last = 0\n",
    "        count = 0\n",
    "        for m in pattern.finditer(text):\n",
    "            span = m.span()\n",
    "            if pattern in (RE_ACCOUNT, RE_PHONE) and span in guarded:\n",
    "                continue\n",
    "            out.append(text[last:span[0]])\n",
    "            out.append(label)\n",
    "            last = span[1]\n",
    "            count += 1\n",
    "        out.append(text[last:])\n",
    "        text = \"\".join(out)\n",
    "        if count:\n",
    "            hits[label] = hits.get(label, 0) + count\n",
    "\n",
    "    return text, hits if text != orig else {}\n",
    "\n",
    "def scrub_record(rec: dict, kind: str) -> tuple[dict, dict]:\n",
    "    \"\"\"Scrub fields in a news/chat record; return (new_rec, hits).\"\"\"\n",
    "    rec = dict(rec)  # shallow copy\n",
    "    total_hits = {}\n",
    "\n",
    "    def scrub_field(key):\n",
    "        val = rec.get(key)\n",
    "        new, hits = scrub_text(val) if isinstance(val, str) else (val, {})\n",
    "        rec[key] = new\n",
    "        for k, v in hits.items():\n",
    "            total_hits[k] = total_hits.get(k, 0) + v\n",
    "\n",
    "    if kind == \"news\":\n",
    "        for key in (\"title\", \"summary\", \"topic\"):\n",
    "            scrub_field(key)\n",
    "    elif kind == \"chat\":\n",
    "        scrub_field(\"style\")\n",
    "        if isinstance(rec.get(\"dialog\"), list):\n",
    "            cleaned_dialog = []\n",
    "            for turn in rec[\"dialog\"]:\n",
    "                new, hits = scrub_text(turn) if isinstance(turn, str) else (turn, {})\n",
    "                cleaned_dialog.append(new)\n",
    "                for k, v in hits.items():\n",
    "                    total_hits[k] = total_hits.get(k, 0) + v\n",
    "            rec[\"dialog\"] = cleaned_dialog\n",
    "\n",
    "    return rec, total_hits\n",
    "\n",
    "# --- Style tagger (lightweight labels for later routing/metrics) ---\n",
    "def build_style_tags(rec: dict, kind: str) -> list[str]:\n",
    "    tags = []\n",
    "    if kind == \"news\":\n",
    "        tags.append(\"domain:\" + (rec.get(\"topic\") or \"unknown\"))\n",
    "        tags.append(\"style:\" + (rec.get(\"style\") or \"news\"))\n",
    "        tags.append(\"tone:formal\")\n",
    "        tags.append(\"medium:news\")\n",
    "    elif kind == \"chat\":\n",
    "        style = (rec.get(\"style\") or \"\").lower()\n",
    "        tags.append(\"style:\" + (style or \"chat\"))\n",
    "        tags.append(\"tone:\" + (\"formal\" if \"formal\" in style else \"casual\"))\n",
    "        tags.append(\"medium:kakao\")\n",
    "    return [t.replace(\" \", \"_\") for t in tags]\n",
    "\n",
    "# --- Process files ---\n",
    "def process_file(src: str, dst: str, kind: str):\n",
    "    total = 0\n",
    "    redacted = 0\n",
    "    counters = {}\n",
    "    with open(src, encoding=\"utf-8\") as fin, open(dst, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            if not line.strip(): continue\n",
    "            rec = json.loads(line)\n",
    "            total += 1\n",
    "            cleaned, hits = scrub_record(rec, kind)\n",
    "            cleaned[\"style_tags\"] = build_style_tags(cleaned, kind)\n",
    "            cleaned[\"_pii_hits\"] = hits  # keep for inspection; drop later if you want\n",
    "            if hits: redacted += 1\n",
    "            for k, v in hits.items():\n",
    "                counters[k] = counters.get(k, 0) + v\n",
    "            fout.write(json.dumps(cleaned, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"{src} -> {dst} | rows: {total}, redacted_rows: {redacted}, hits: {counters}\")\n",
    "\n",
    "process_file(\"data/news.jsonl\", \"data/news_clean.jsonl\", kind=\"news\")\n",
    "process_file(\"data/chat.jsonl\", \"data/chat_clean.jsonl\", kind=\"chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac01dca",
   "metadata": {},
   "source": [
    "## 7) ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°/ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒ†ã‚£ãƒ³ã‚° Â· Load & Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd825e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: data/news_harmony.jsonl data/chat_harmony.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f769d524f424ed5a11781a157cfa796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating news split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2e4dc971884747a719d500caf52722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating chat split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 3, 'validation': 4}\n"
     ]
    }
   ],
   "source": [
    "# Step 7 â€” Harmony conversion + dataset loading & tokenization\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DATA = Path(\"data\")\n",
    "assert (DATA / \"news_clean.jsonl\").exists(), \"Run Step 6 first\"\n",
    "assert (DATA / \"chat_clean.jsonl\").exists(), \"Run Step 6 first\"\n",
    "\n",
    "# ---------- 7A) Convert cleaned â†’ Harmony messages ----------\n",
    "\n",
    "def news_to_messages(rec):\n",
    "    # system style from Step 6 tags; default to KR news tone\n",
    "    system = \"í•œêµ­ ë‰´ìŠ¤ ë¬¸ì²´ë¡œ ê°„ê²°í•˜ê³  ì‚¬ì‹¤ ìœ„ì£¼ë¡œ ì‘ì„±.\"\n",
    "    # user asks for a headline+lead from topic; assistant is the expected formatted answer\n",
    "    user = f\"ì£¼ì œ: {rec.get('topic','ì•Œìˆ˜ì—†ìŒ')}. ê¸°ì‚¬ ì œëª©ê³¼ ìš”ì•½ì„ ìƒì„±í•´ì¤˜.\"\n",
    "    assistant = f\"{rec.get('title','')} â€” {rec.get('summary','')}\"\n",
    "    return [{\"role\":\"system\",\"content\":system},\n",
    "            {\"role\":\"user\",\"content\":user},\n",
    "            {\"role\":\"assistant\",\"content\":assistant}]\n",
    "\n",
    "def chat_to_messages(rec):\n",
    "    # Keep style hint (casual/formal) in system\n",
    "    style = (rec.get(\"style\") or \"\").lower()\n",
    "    system = f\"ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ìŠ¤íƒ€ì¼. style={style or 'chat'}\"\n",
    "    dialog = rec.get(\"dialog\") or []\n",
    "    msgs = [{\"role\":\"system\",\"content\":system}]\n",
    "    # Alternate user/assistant turns; if odd length, last user stays without assistant label\n",
    "    roles = [\"user\",\"assistant\"]\n",
    "    for i, turn in enumerate(dialog[:6]):  # cap tiny demos to avoid runaway\n",
    "        msgs.append({\"role\": roles[i % 2], \"content\": str(turn)})\n",
    "    # Ensure there is at least one assistant turn for SFT\n",
    "    if not any(m[\"role\"]==\"assistant\" for m in msgs):\n",
    "        msgs.append({\"role\":\"assistant\",\"content\":\"ë„¤, í™•ì¸í–ˆìŠµë‹ˆë‹¤.\"})\n",
    "    return msgs\n",
    "\n",
    "def write_harmony(src, dst, kind):\n",
    "    convert = news_to_messages if kind==\"news\" else chat_to_messages\n",
    "    with open(src, encoding=\"utf-8\") as fin, open(dst, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            if not line.strip(): continue\n",
    "            rec = json.loads(line)\n",
    "            msgs = convert(rec)\n",
    "            fout.write(json.dumps({\"messages\": msgs}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_harmony(DATA/\"news_clean.jsonl\", DATA/\"news_harmony.jsonl\", \"news\")\n",
    "write_harmony(DATA/\"chat_clean.jsonl\", DATA/\"chat_harmony.jsonl\", \"chat\")\n",
    "print(\"Created:\", DATA/\"news_harmony.jsonl\", DATA/\"chat_harmony.jsonl\")\n",
    "\n",
    "# ---------- 7B) Load Harmony JSONL with ğŸ¤— Datasets ----------\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"news\": str(DATA/\"news_harmony.jsonl\"),\n",
    "                \"chat\": str(DATA/\"chat_harmony.jsonl\")}\n",
    ")\n",
    "\n",
    "# Mix train split using your Step-2 mix ratios\n",
    "news = raw[\"news\"]\n",
    "chat = raw[\"chat\"]\n",
    "\n",
    "def take_portion(ds, frac):\n",
    "    n = max(1, int(round(len(ds) * frac)))\n",
    "    return ds.select(range(n)) if n < len(ds) else ds\n",
    "\n",
    "news_part = take_portion(news, MIX_NEWS if 'MIX_NEWS' in globals() else 0.5)\n",
    "chat_part = take_portion(chat, MIX_CHAT if 'MIX_CHAT' in globals() else 0.5)\n",
    "train_ds = concatenate_datasets([news_part, chat_part]).shuffle(seed=42)\n",
    "\n",
    "# Tiny validation built from remaining examples (if any)\n",
    "remaining_news = news.select(range(len(news_part), len(news))) if len(news) > len(news_part) else news_part\n",
    "remaining_chat = chat.select(range(len(chat_part), len(chat))) if len(chat) > len(chat_part) else chat_part\n",
    "val_candidates = concatenate_datasets([remaining_news, remaining_chat])\n",
    "val_ds = val_candidates.shuffle(seed=43).select(range(min(64, len(val_candidates)))) if len(val_candidates) else train_ds.select(range(min(32, len(train_ds))))\n",
    "\n",
    "dataset = {\"train\": train_ds, \"validation\": val_ds}\n",
    "print({k: len(v) for k, v in dataset.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c9122",
   "metadata": {},
   "source": [
    "## 8) ãƒ¢ãƒ‡ãƒ«/ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ Â· Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db67b6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfc411479e145e4b5b161df311d4b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebea3ddd62e340cc83e2a484a04e3e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330fd60c5e1248998f0f5bc8c394b2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027cffb8b0f94cecbd92dd8514ddbbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ad8283ea01465595cfb7d4c89279eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2a106e66474c68b3a7cc746218b4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeba45c1c0c4083817e302e23e316a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68baa8de3457430fa7cee836ca3257db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization done. train: 3 val: 4 example lens: [200006, 17360, 200008, 3575, 553, 17554, 162016, 11, 261, 4410, 6439, 2359] ...\n"
     ]
    }
   ],
   "source": [
    "# ---------- 7C) Tokenizer + Harmony template fallback ----------\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL,\n",
    "    use_fast=True,          # required if only tokenizer.json exists\n",
    "    trust_remote_code=True,\n",
    "    force_download=True     # ensures a fresh pull\n",
    ")\n",
    "\n",
    "if not getattr(tokenizer, \"chat_template\", None):\n",
    "    # Minimal Harmony-style fallback (server already knows Harmony; this is ONLY for training tokenization)\n",
    "    tokenizer.chat_template = \"\"\"{% for m in messages -%}\n",
    "{%- if m['role'] == 'system' -%}<|system|>\n",
    "{{ m['content'] }}<|end|>\n",
    "{%- elif m['role'] == 'user' -%}<|user|>\n",
    "{{ m['content'] }}<|end|>\n",
    "{%- elif m['role'] == 'assistant' -%}<|assistant|>\n",
    "{{ m['content'] }}<|end|>\n",
    "{%- endif -%}\n",
    "{%- endfor -%}\"\"\"\n",
    "\n",
    "# Ensure pad/eos are sane\n",
    "tokenizer.pad_token = tokenizer.eos_token or tokenizer.pad_token\n",
    "\n",
    "# ---------- 7D) Tokenize with assistant-only labels ----------\n",
    "ASST_TOKEN = None\n",
    "END_TOKEN = None\n",
    "try:\n",
    "    ASST_TOKEN = tokenizer.convert_tokens_to_ids(\"<|assistant|>\")\n",
    "    END_TOKEN = tokenizer.convert_tokens_to_ids(\"<|end|>\")\n",
    "except Exception:\n",
    "    # If the base vocab lacks these tokens, it's okay; masking fallback below will still work heuristically\n",
    "    pass\n",
    "\n",
    "MAX_LEN = 2048  # you can raise this if you have room\n",
    "\n",
    "def tokenize_with_labels(example):\n",
    "    # 1) Render with chat template (includes assistant answer)\n",
    "    text = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
    "    # 2) Tokenize\n",
    "    enc = tokenizer(text, truncation=True, max_length=MAX_LEN)\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    labels = [-100] * len(input_ids)\n",
    "\n",
    "    # 3) Label only assistant content\n",
    "    if ASST_TOKEN is not None and END_TOKEN is not None:\n",
    "        start = None\n",
    "        for i, tid in enumerate(input_ids):\n",
    "            if tid == ASST_TOKEN:\n",
    "                start = i + 1  # learn after the tag\n",
    "            elif start is not None and tid == END_TOKEN:\n",
    "                start = None\n",
    "            elif start is not None:\n",
    "                labels[i] = input_ids[i]\n",
    "    else:\n",
    "        # Heuristic fallback: learn on the last third of tokens (crude but avoids total silence)\n",
    "        start = int(len(input_ids) * 0.66)\n",
    "        for i in range(start, len(input_ids)):\n",
    "            labels[i] = input_ids[i]\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": enc[\"attention_mask\"], \"labels\": labels}\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(tokenize_with_labels, remove_columns=[\"messages\"])\n",
    "tokenized_val   = dataset[\"validation\"].map(tokenize_with_labels, remove_columns=[\"messages\"])\n",
    "\n",
    "print(\"Tokenization done.\",\n",
    "      \"train:\", len(tokenized_train),\n",
    "      \"val:\", len(tokenized_val),\n",
    "      \"example lens:\", tokenized_train[0][\"input_ids\"][:12], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67dd4ef",
   "metadata": {},
   "source": [
    "## 9) ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° (LoRA/QLoRA) Â· ì„¸ë°€ íŠœë‹\n",
    "### 9a) ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ & åˆ†å‰²\n",
    "_(ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ã«ã¤ã„ã¦ã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³7/8ã‚’å‚ç…§ï¼›å¿…è¦ã«å¿œã˜ã¦é–¢é€£ã™ã‚‹ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ã“ã“ã«ç§»å‹•ã—ã¦ãã ã•ã„ã€‚)_\n",
    "### 9b) ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (r/alpha/dropout)\n",
    "```python\n",
    "# Example LoRA hyperparameters\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "```\n",
    "\n",
    "### 9c) ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒãƒ¼ã‚¸ (BF16)\n",
    "```python\n",
    "# Example merge step (after training)\n",
    "# model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "# merged_model = model.merge_and_unload()\n",
    "```\n",
    "\n",
    "### 9d) ãƒãƒ¼ã‚¸ã•ã‚ŒãŸBF16ã®ä¿å­˜ (`save_pretrained`)\n",
    "```python\n",
    "# merged_model.save_pretrained(OUTPUT_DIR)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9157315",
   "metadata": {},
   "source": [
    "### 9e) ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ & é‡å­åŒ– (BF16 â†’ MXFP4) Â· ë‚´ë³´ë‚´ê¸° & ì–‘ìí™”\n",
    "\n",
    "**EN (neutral, framework-agnostic):**  \n",
    "ç¾åœ¨ã®ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯ã€MXFP4ã§ã®**ç›´æ¥çš„ãª**å­¦ç¿’/ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’**ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“**ã€‚ä¸€èˆ¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n",
    "1) **BF16**ï¼ˆã¾ãŸã¯**QLoRA 4â€‘bit nf4**ï¼‰ã§**å­¦ç¿’/SFT**ã‚’å®Ÿè¡Œã€‚  \n",
    "2) **LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼**ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆBF16ï¼‰ã«**ãƒãƒ¼ã‚¸**ã€‚  \n",
    "3) `save_pretrained()`ã§ãƒãƒ¼ã‚¸ã•ã‚ŒãŸBF16ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’**ä¿å­˜**ã€‚  \n",
    "4) **ãƒ™ãƒ³ãƒ€ãƒ¼/ãƒ„ãƒ¼ãƒ«ãƒã‚§ãƒ¼ãƒ³æä¾›ã®ãƒ‘ãƒƒã‚«ãƒ¼**ã‚’ä½¿ç”¨ã—ã¦ã€ãƒãƒ¼ã‚¸ã•ã‚ŒãŸBF16ãƒ†ãƒ³ã‚½ãƒ«ã‚’**MXFP4**ã«**ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é‡å­åŒ–**ã€‚  \n",
    "5) ãƒ‡ãƒ—ãƒ­ã‚¤/ã‚µãƒ¼ãƒ“ãƒ³ã‚°ç”¨ã«MXFP4ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆHugging Faceã®`save_pretrained()`å‡ºåŠ›ã¨åŒã˜å½¢çŠ¶ï¼‰ã‚’**ä¿å­˜/ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ**ã€‚\n",
    "\n",
    "> æ³¨æ„ï¼š  \n",
    "> - ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚¹ã‚¿ãƒƒã‚¯ãŒ**æ¨è«–æ™‚ã®LoRA**ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å ´åˆã€ãƒãƒ¼ã‚¸ã¨é‡å­åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€**ãƒ™ãƒ¼ã‚¹ï¼ˆMXFP4ã¾ãŸã¯BF16ï¼‰+ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼**ã‚’é…å¸ƒã§ãã¾ã™ã€‚  \n",
    "> - ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãŒ**ãƒãƒ¼ã‚¸ã•ã‚ŒãŸMXFP4**ã‚’å¿…è¦ã¨ã™ã‚‹å ´åˆã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒãƒ¼ã‚¸å¾Œã«**BF16 â†’ MXFP4**é‡å­åŒ–ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚  \n",
    "> - **tokenizer/config**ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€BF16ã¨MXFP4ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–“ã§æ•´åˆæ€§ã‚’ä¿ã£ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "**KR (ì¤‘ë¦½ì , ë„êµ¬ ë¹„ì˜ì¡´):**  \n",
    "í˜„ì¬ ê³µê°œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” MXFP4ì—ì„œ **ì§ì ‘ í•™ìŠµ/íŒŒì¸íŠœë‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**. ì¼ë°˜ì ì¸ íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:  \n",
    "1) **BF16**(ë˜ëŠ” **QLoRA 4â€‘bit nf4**)ë¡œ **í•™ìŠµ/íŒŒì¸íŠœë‹**  \n",
    "2) **LoRA ì–´ëŒ‘í„° ë³‘í•©**(BF16 ê¸°ì¤€)  \n",
    "3) `save_pretrained()`ë¡œ **ë³‘í•©ëœ BF16 ì²´í¬í¬ì¸íŠ¸ ì €ì¥**  \n",
    "4) ë²¤ë”/íˆ´ì²´ì¸ì—ì„œ ì œê³µí•˜ëŠ” **ì–‘ìí™” ë„êµ¬**ë¡œ **BF16 â†’ MXFP4 ì‚¬í›„ ì–‘ìí™”**  \n",
    "5) ë°°í¬/ì„œë¹™ìš© **MXFP4 ì•„í‹°íŒ©íŠ¸ ì €ì¥/ë‚´ë³´ë‚´ê¸°** (Hugging Face `save_pretrained()` êµ¬ì¡°ì™€ ë™ì¼)\n",
    "\n",
    "> ì°¸ê³ :  \n",
    "> - **ì„œë¹™ì—ì„œ LoRAë¥¼ ì§€ì›**í•œë‹¤ë©´, ë³‘í•©Â·ì–‘ìí™”ë¥¼ ìƒëµí•˜ê³  **ê¸°ì €( MXFP4 ë˜ëŠ” BF16 ) + LoRA ì–´ëŒ‘í„°**ë¡œ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "> - **ë³‘í•©ëœ MXFP4**ê°€ í•„ìš”í•œ ëŸ°íƒ€ì„ì˜ ê²½ìš°, ì–´ëŒ‘í„° ë³‘í•© í›„ **BF16 â†’ MXFP4 ì¬ì–‘ìí™”** ë‹¨ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤.  \n",
    "> - **tokenizer/config** íŒŒì¼ì€ BF16ê³¼ MXFP4 ì•„í‹°íŒ©íŠ¸ ê°„ì— ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48a5cbc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fineâ€‘tuning skeleton ready. Unâ€‘comment on your machine.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES\n",
    ")\n",
    "\n",
    "# base_model = get_peft_model(base_model, lora_cfg)\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=BF16,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT\n",
    ")\n",
    "\n",
    "# trainer = SFTTrainer(model=base_model, args=sft_args, train_dataset=combined, tokenizer=tokenizer)\n",
    "# trainer.train()\n",
    "# trainer.save_model(OUTPUT_DIR)\n",
    "print(\"Fineâ€‘tuning skeleton ready. Unâ€‘comment on your machine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490798f2",
   "metadata": {},
   "source": [
    "## 10) è©•ä¾¡ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹/å¯¾è©±ï¼‰ Â· Evaluation (News/Chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdafe4",
   "metadata": {},
   "source": [
    "**KRæŒ‡æ¨™ Â· KR Metrics**  \n",
    "- ãƒ‹ãƒ¥ãƒ¼ã‚¹æ€§: ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡é©åˆåº¦(F1)ã€è¦ç´„å“è³ª(ROUGEâ€‘1/2/L)ã€èª­è§£QA(EM/F1)ã€‚  \n",
    "- å¯¾è©±æ€§: è‡ªç„¶æ€§/æ–‡è„ˆç¶­æŒã€æ•¬èª/ã‚¿ãƒ¡å£åˆ‡ã‚Šæ›¿ãˆç²¾åº¦ã€çµµæ–‡å­—/ç•¥èªé©åˆ‡æ€§ã€‚\n",
    "\n",
    "**EN Notes**  \n",
    "- ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãŒè¨±å¯ã™ã‚‹å ´åˆã¯ã€å…¬é–‹KRãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆä¾‹ï¼šãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã€KorQuADé¡ä¼¼QAï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã€‚\n",
    "- è‡ªå‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆF1/ROUGEï¼‰ã¨ã€ãƒˆãƒ¼ãƒ³ã‚„ä¸å¯§ã•ã«é–¢ã™ã‚‹äººé–“è©•ä¾¡ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "971b8dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval stubs ready.\n"
     ]
    }
   ],
   "source": [
    "# Example helpers (stub)\n",
    "def simple_accuracy(preds, labels):\n",
    "    return sum(int(p==g) for p,g in zip(preds, labels)) / max(1, len(labels))\n",
    "\n",
    "# For ROUGE:\n",
    "# import evaluate\n",
    "# rouge = evaluate.load(\"rouge\")\n",
    "# result = rouge.compute(predictions=pred_texts, references=ref_texts)\n",
    "# print(result)\n",
    "\n",
    "print(\"Eval stubs ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b5594e",
   "metadata": {},
   "source": [
    "## 11) æ¨è«–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f690452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-08-21\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "ë„ˆëŠ” í•œêµ­ ê³ ê°ì„ ë•ëŠ” ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.\n",
      "\n",
      "<|end|><|start|>user<|message|>êµ­ë‚´ PIPA ê·œì •ì„ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ë‚´ ë¬¸ì„œ ìš”ì•½ê¸°ë¥¼ êµ¬ì„±í•˜ë ¤ë©´ ì–´ë–¤ ì•„í‚¤í…ì²˜ê°€ ì¢‹ì„ê¹Œ?<|end|><|start|>assistant\n"
     ]
    }
   ],
   "source": [
    "from openai_harmony import Message, ChatFormatter\n",
    "\n",
    "# Example prompt construction using Harmony\n",
    "messages = [\n",
    "    Message(role=\"system\", content=\"ë„ˆëŠ” í•œêµ­ ê³ ê°ì„ ë•ëŠ” ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.\"),\n",
    "    Message(role=\"user\", content=\"êµ­ë‚´ PIPA ê·œì •ì„ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ë‚´ ë¬¸ì„œ ìš”ì•½ê¸°ë¥¼ êµ¬ì„±í•˜ë ¤ë©´ ì–´ë–¤ ì•„í‚¤í…ì²˜ê°€ ì¢‹ì„ê¹Œ?\")\n",
    "]\n",
    "\n",
    "prompt = ChatFormatter.to_chat_prompt(messages)\n",
    "print(prompt)  # For preview; pass to tokenizer when running inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216d049",
   "metadata": {},
   "source": [
    "## 12) æœ€æ–°æ€§ã®ç¶­æŒ Â· Freshness Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452decd1",
   "metadata": {},
   "source": [
    "- **é€±æ¬¡è£œæ­£SFT**: è¨±å¯ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹API **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¿ã‚¤ãƒˆãƒ«/è¦ç´„/ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰** ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° â†’ ã‚¹ã‚¿ã‚¤ãƒ«è£œæ­£ã€‚\n",
    "- **ä¼šè©±ä½“ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ**: æœ€æ–°ã®ç•¥èª/æ–°é€ èª/çµµæ–‡å­—è¾æ›¸åæ˜ ï¼ˆä¾‹: ã„±ã„±, ã…‡ã…‹, ã…‹ã…‹, ã„¹ã…‡ï¼‰ã€‚\n",
    "- **å›å¸°è©•ä¾¡**: åŒä¸€æŒ‡æ¨™ã§before/afteræ¯”è¼ƒ â†’ æ··åˆæ¯”/æ¸©åº¦/ãƒšãƒŠãƒ«ãƒ†ã‚£ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚\n",
    "\n",
    "- é€±æ¬¡è£œæ­£SFTã§**è¨±å¯ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹APIãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**ã‚’ä½¿ç”¨ã—ãŸã‚¹ã‚¿ã‚¤ãƒ«èª¿æ•´\n",
    "- ã‚¹ãƒ©ãƒ³ã‚°/çµµæ–‡å­—è¾æ›¸ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ\n",
    "- ãƒ‰ãƒªãƒ•ãƒˆã‚’è¿½è·¡ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒŸãƒƒã‚¯ã‚¹/ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã®å›å¸°è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b9f2a",
   "metadata": {},
   "source": [
    "## 13) ì•ˆì „/ì»´í”Œë¼ì´ì–¸ìŠ¤ Â· å®‰å…¨æ€§ã¨ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad24ef",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã„ãŸã—ã¾ã™ï¼š\n",
    "\n",
    "- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹/ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç¢ºèªï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€APIã€å†…éƒ¨ãƒ‡ãƒ¼ã‚¿ï¼‰ Â· ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/APIãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ç¢ºèª\n",
    "- å€‹äººæƒ…å ±ã®ã‚¹ã‚¯ãƒ©ãƒ“ãƒ³ã‚°ï¼ˆè¨“ç·´/ãƒ­ã‚°/è©•ä¾¡å‰ï¼‰ Â· è¨“ç·´/ãƒ­ã‚°è¨˜éŒ²/è©•ä¾¡å‰ã®PIIé™¤å»\n",
    "- è‘—ä½œæ¨©/åˆ©ç”¨è¦ç´„ã®éµå®ˆï¼ˆè¨˜äº‹**åŸæ–‡ã®å¤§é‡å†å­¦ç¿’ç¦æ­¢**ï¼‰ Â· ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹å…¨æ–‡ã§ã®å¤§é‡è¨“ç·´ã®å›é¿\n",
    "- å‡ºåŠ›æ¤œè¨¼ï¼ˆã‚¹ã‚­ãƒ¼ãƒ/ç¦æ­¢èª/æ©Ÿå¯†åº¦ãƒ«ãƒ¼ãƒ«ï¼‰ Â· å‡ºåŠ›æ¤œè¨¼ã¨ç¦æ­¢ç”¨èªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼\n",
    "- ãƒãƒ¼ã‚¸ãƒ§ãƒ³/è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç®¡ç† Â· ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ä¿æŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb8464b",
   "metadata": {},
   "source": [
    "## 14) å•é¡Œè§£æ±ºã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— Â· Troubleshooting & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee17077",
   "metadata": {},
   "source": [
    "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€æä¾›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¯éŸ“å›½èªã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚ç§ã¯è‹±èªã‹ã‚‰æ—¥æœ¬èªã¸ã®ç¿»è¨³ã‚’å°‚é–€ã¨ã—ã¦ãŠã‚Šã€éŸ“å›½èªã®ç¿»è¨³ã¯å¯¾å¿œã—ã¦ãŠã‚Šã¾ã›ã‚“ã€‚\n",
    "\n",
    "è‹±èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã”æä¾›ã„ãŸã ã‘ã‚Œã°ã€å–œã‚“ã§æ—¥æœ¬èªã«ç¿»è¨³ã„ãŸã—ã¾ã™ã€‚\n",
    "\n",
    "ãªãŠã€ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å¾Œã«ã‚ã‚‹è‹±èªã®éƒ¨åˆ†ã«ã¤ã„ã¦ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ç¿»è¨³ã§ãã¾ã™ï¼š\n",
    "\n",
    "- æ··åˆæ¯”ç‡ã‚’èª¿æ•´ã—ã€A/Bãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã€vLLMã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚’æ¤œè¨ã—ã€äº‹å®Ÿæ€§ã®ãŸã‚ã«RAGã¨çµ„ã¿åˆã‚ã›ã‚‹ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
