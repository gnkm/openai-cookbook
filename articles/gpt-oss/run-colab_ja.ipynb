{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGfI8meEHXfM"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openai/openai-cookbook/blob/main/articles/gpt-oss/run-colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gj6KvThm8Jjn"
   },
   "source": [
    "# 無料のGoogle ColabでOpenAI gpt-oss 20Bを実行する\n",
    "\n",
    "OpenAIは`gpt-oss` [120B](https://hf.co/openai/gpt-oss-120b)と[20B](https://hf.co/openai/gpt-oss-20b)をリリースしました。両方のモデルはApache 2.0ライセンスです。\n",
    "\n",
    "特に、`gpt-oss-20b`は低レイテンシとローカルまたは特殊な用途向けに作られました（210億パラメータで36億のアクティブパラメータ）。\n",
    "\n",
    "モデルはネイティブMXFP4量子化で訓練されているため、Google Colabのようなリソース制約のある環境でも20Bを簡単に実行できます。\n",
    "\n",
    "著者：[Pedro](https://huggingface.co/pcuenq)と[VB](https://huggingface.co/reach-vb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kv2foJJa9Xkc"
   },
   "source": [
    "## 環境のセットアップ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMRXDOpY1Q3Q"
   },
   "source": [
    "transformersでのmxfp4サポートは最先端の機能であるため、`mxfp4` tritonカーネルをインストールできるように、PyTorchとCUDAの最新バージョンが必要です。\n",
    "\n",
    "また、transformersをソースからインストールする必要があり、依存関係の競合を解決するために`torchvision`と`torchaudio`をアンインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gUEKrLEvJmf"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N00UT7gtpkp"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers triton==3.4 kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GW0knW2w3ND"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -q torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxU0WKwtH19m"
   },
   "source": [
    "上記のパッケージをインストールした後、Colabランタイムセッションを再起動してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3xCxY159frD"
   },
   "source": [
    "## Google ColabでHugging Faceからモデルを読み込む\n",
    "\n",
    "こちらからモデルを読み込みます：[openai/gpt-oss-20b](https://hf.co/openai/gpt-oss-20b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2HFwdkXu2R1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jbeq6kN79ql0"
   },
   "source": [
    "## メッセージ/チャットの設定\n",
    "\n",
    "オプションのシステムプロンプトを提供するか、直接入力を行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5dJV3xsu_89"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Always respond in riddles\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather like in Madrid?\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    "\n",
    "generated = model.generate(**inputs, max_new_tokens=500)\n",
    "print(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksxo7bjR_-th"
   },
   "source": [
    "## 推論の努力レベルを指定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcv6QdcQKLr0"
   },
   "source": [
    "`apply_chat_template()`の追加引数として渡すだけです。サポートされている値は`\"low\"`、`\"medium\"`（デフォルト）、または`\"high\"`です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmnkAle608Hl"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Always respond in riddles\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain why the meaning of life is 42\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    reasoning_effort=\"high\",\n",
    ").to(model.device)\n",
    "\n",
    "generated = model.generate(**inputs, max_new_tokens=500)\n",
    "print(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tf2-ocGqEC_r"
   },
   "source": [
    "## 他のプロンプトやアイデアを試してみましょう！\n",
    "\n",
    "他のアイデアについては、私たちのブログ投稿をご覧ください：[https://hf.co/blog/welcome-openai-gpt-oss](https://hf.co/blog/welcome-openai-gpt-oss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QrnTpcCKd_n"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
